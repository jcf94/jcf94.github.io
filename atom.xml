<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chenfan Blog</title>
  
  <subtitle>Do cool things that matter.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jcf94.com/"/>
  <updated>2020-08-29T10:14:44.785Z</updated>
  <id>https://jcf94.com/</id>
  
  <author>
    <name>Jcf94</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NVIDIA GPU 架构演进</title>
    <link href="https://jcf94.com/2020/05/24/2020-05-24-nvidia-arch/"/>
    <id>https://jcf94.com/2020/05/24/2020-05-24-nvidia-arch/</id>
    <published>2020-05-24T08:11:19.000Z</published>
    <updated>2020-08-29T10:14:44.785Z</updated>
    
    <content type="html"><![CDATA[<p>最近 NV 刚发布了新架构的 GPU，自之前理过 <a href="/2018/02/13/2018-02-13-intel/">Intel CPU 的架构变化</a> 后再来理一下 GPU 的。</p><p><a href="/2018/02/12/2018-02-12-cod3/#6-6-Introduction-to-Graphics-Processing-Units">硬/软件接口</a>那篇有介绍过 GPU 的结构，当时也是以 Fermi 架构为例的，NV 很有意思的是会用一些历史上杰出的科学家的名字来命名自己的硬件架构。</p><p>总体上，NV GPU 用到的 SIMT 基本编程模型都是一致的，每一代相对前代基本都会在 SM 数量、SM 内部各个处理单元的流水线结构等等方面有一些升级和改动。这篇暂时不涉及到渲染管线相关的部分，其他诸如多少 nm 工艺、内存频率提升等等也都先略过，只关注计算相关的硬件架构演进。</p><a id="more"></a><hr><h1 id="Tesla"><a href="#Tesla" class="headerlink" title="Tesla"></a>Tesla</h1><p>关于初代 GPU 的架构，找到的资料不太多，基本上都是从 Fermi 开始的。</p><h1 id="Fermi"><a href="#Fermi" class="headerlink" title="Fermi"></a>Fermi</h1><p>Compute Capability: 2.0, 2.1</p><p><img data-src="https://jcf94.com/download/2018-02-06-cod2-Fermi.svg" alt="Fermi 架构 SM"></p><p>每个 SM 中包含：</p><ul><li>2 个 Warp Scheduler/Dispatch Unit</li><li>32 个 CUDA Core（分在两条 lane 上，每条分别是 16 个）<ul><li>每个 CUDA Core 里面是 1 个单精浮点单元（FPU）和 1 个整数单元（ALU），可以直接做 FMA 的乘累加</li><li>每个 cycle 可以跑 16 个双精的 FMA</li></ul></li><li>16 个 LD/ST Unit</li><li>4 个 SFU</li></ul><blockquote><p>我的理解是做一个双精 FMA 需要用到两个 CUDA Core？所以是 32 / 2 = 16</p></blockquote><h1 id="Kepler"><a href="#Kepler" class="headerlink" title="Kepler"></a>Kepler</h1><p>Compute Capability: 3.0, 3.2, 3.5, 3.7</p><p>这一代 SM 整体结构上跟之前是一致的，只不过升级完了以后又往里面塞进去了更多的运算单元，其他部分也没有做太大的改动。</p><p><img data-src="https://jcf94.com/download/2020-05-24-nvidia-arch-kepler.png" alt="Kepler 架构 SM"></p><p>每个 SM（这里叫 SMX 了）中包含：</p><ul><li>4 个 Warp Scheduler，8 个 Dispatch Unit</li><li>CUDA Core 增加到 192 个（4 * 3 * 16，每条 lane 上还是 16 个）</li><li>单独分出来 64 个（4 * 16，每条 lane 上 16 个）双精运算单元。</li><li>SFU 和 LD/ST Unit 分别也都增加到 32 个</li></ul><p>Kepler 是附近几代在硬件上直接有双精运算单元的架构，不用通过单精单元去做双精运算了，所以对比前后几代的双精浮点的性能话会发现 Kepler 要高出一截。</p><h1 id="Maxwell"><a href="#Maxwell" class="headerlink" title="Maxwell"></a>Maxwell</h1><p>Compute Capability: 5.0, 5.2, 5.3</p><p><img data-src="https://jcf94.com/download/2020-05-24-nvidia-arch-maxwell.png" alt="Maxwell 架构 SM"></p><p>可能是觉得 Kepler 往一个 SM 里面塞了太多东西，其实最终效率也并没有那么高，这一代的 SM 开始做减法了，每个 SM（SMM）中包含：</p><ul><li>4 个 Warp Scheduler，8 个 Dispatch Unit</li><li>128 个 CUDA Core（4 * 32）</li><li>32 个 SFU 和 LD/ST Unit（4 * 8）</li></ul><p>Kepler 里面 192 这个数字也被诟病了（不是 2 的倍数）。</p><p>这些硬件单元的流水线分布也不再是像 Kepler 那样大锅炖了，而是有点像是把 4 个差不多像是 Fermi 的 SM 拼在一起组成一个 SM：<br>每个 Process Block 里面是：</p><ul><li>1 个 Warp Scheduler 和 2 个 Dispatch Unit</li><li>32 个 CUDA Core</li><li>8 个 SFU 和 LD/ST Unit</li></ul><p>图上没有看到之前 lane 的标记，不过我猜应该也还是 4 条，两条 CUDA Core 的 lane，1 条 SFU，1 条 LD/ST Unit。</p><p>应该是工艺和频率的提升，Maxwell 每个 CUDA Core 的性能相比 Kepler 提升了 1.4 倍，每瓦性能提升了 2 倍。对 CUDA Core 的详细结构没有再介绍，姑且认为从 Fermi 开始一直到以后 CUDA Core 内部的结构都没有什么改变。</p><p>另外一点是，前面说到的双精单元在这一代上也移除了。</p><blockquote><p>也许是觉得认为只有少数 HPC 科学计算才用的上的双精单元在这代上不太有必要吧。</p></blockquote><h1 id="Pascal"><a href="#Pascal" class="headerlink" title="Pascal"></a>Pascal</h1><p>Compute Capability: 6.0, 6.1, 6.2</p><p>这一代可以说是有了质的飞跃，还是先从 SM 开始：</p><p><img data-src="https://jcf94.com/download/2020-05-24-nvidia-arch-pascal.png" alt="Pascal 架构 SM"></p><p>可以看到一个 SM 内的部分作了进一步的精简，整体思路是 SM 内部包含的东西越来越少，但是总体的片上 SM 数量每一代都在不断增加，每个 SM 中包含：</p><ul><li>2 个 Warp Scheduler，4 个 Dispatch Unit</li><li>64 个 CUDA Core（2 * 32）</li><li>32 个双精浮点单元（2 * 16，双精回来了！）</li><li>16 个 SFU 和 LD/ST Unit（2 * 8）</li></ul><p>一个 SM 里面包含的 Process Block 数量减少到了 2 个，每个 Process Block 内部的结构倒是 Maxwell 差不多：</p><ul><li>1 个 Warp Scheduler 和 2 个 Dispatch Unit</li><li>32 个 CUDA Core</li><li>多了 16 个 DP Unit</li><li>8 个 SFU 和 LD/ST Unit</li></ul><blockquote><p>单个 Process Block 的流水线增加到 6 条 lane 了？</p></blockquote><p>其他质变的升级包括：</p><ul><li>面向 Deep Learning 做了一些专门的定制（CuDNN 等等）</li><li>除了 PCIE 以外，P100 还有 NVLink 版，单机卡间通信带宽逆天了，多机之间也能通过 Infiniband 进一步扩展 NVLink（GPUDirect）<blockquote><p>然后 NV 现在已经把 Infiniband 行业的龙头 Mellanox 给收购了……说不定那时候就已经有这个想法了呢</p></blockquote></li><li>P100 上把 GDDR5 换成了 HBM2，Global Memory 的带宽涨了一个数量级</li><li>16nm FinFET 工艺，性能提升一大截，功耗还能控制住不怎么增加</li><li>Unified Memory，支持把 GPU 的显存和 CPU 的内存统一到一个相同的地址空间，驱动层自己会做好 DtoH 和 HtoD 的内存拷贝，编程模型上更加友好了</li></ul><p>CUDA Core 在这一代也终于有了升级，现在硬件上直接支持 FP16 的半精计算了，半精性能是单精的 2 倍，猜测应该是一个单精单元用来算两个半精的计算。</p><h1 id="Volta"><a href="#Volta" class="headerlink" title="Volta"></a>Volta</h1><p>Compute Capability: 7.0, 7.2</p><p>又一个针对深度学习的质变 Feature，Tensor Core！</p><p><img data-src="https://jcf94.com/download/2020-05-24-nvidia-arch-volta.png" alt="Volta 架构 SM"></p><p>看到 SM 的时候我们会发现这一代除了多出了一个额外的 Tensor Core 的单元以外，怎么 SM 的体积看起来好像又加回去了，每个 SM 中包含：</p><ul><li>4 个 Warp Scheduler，4 个 Dispatch Unit（发现不需要配 2 个 Dispatch 给每个 Scheduler 了？白皮书里面倒是没有对这个的解释）</li><li>64 个 FP32 Core（4 * 16）</li><li>64 个 INT32 Core（4 * 16）</li><li>32 个 FP64 Core（4 * 8）</li><li>8 个 Tensor Core （4 * 2）</li><li>32 个 LD/ST Unit（4 * 8）</li><li>4 个 SFU（发现对特殊计算的需求减少了？）</li></ul><p>事实上相比 Pascal 而言，单个 SM 中的单精运算单元数量是一致的，相当于把 Pascal 中的每个 Process Block 进一步地又拆成了 2 个，每个 Process Block 中包含：</p><ul><li>1 个 Warp Scheduler，1 个 Dispatch Unit</li><li>16 个 FP32 Core</li><li>16 个 INT32 Core</li><li>8 个 FP64 Core</li><li>2 个 Tensor Core</li><li>8 个 LD/ST Unit</li><li>1 个 SFU</li></ul><p>这里把原本的 CUDA Core 给拆开了，FP32 和 INT32 的两组运算单元现在是独立出现在流水线 lane 里面了，这一设计的好处是在前几代架构中 CUDA Core 同时只能处理一种类型的运算，而现在每个 cycle 都可以同时有 FP32 和 INT32 的指令在一起跑了。Pascal 中需要 6 个 cycles 来做一组 FMA，现在在 Volta 中只需要 4 个 cycles。</p><p>另外每个 Warp Scheduler 还有了自己的 L0 指令 cache。</p><p>最重大的改动不用说也知道是 Tensor Core 了，不过这里暂时先不展开 … 后面可能会回来补充完整。</p><h2 id="Turing"><a href="#Turing" class="headerlink" title="Turing"></a>Turing</h2><p>Compute Capability: 7.5</p><h1 id="Ampere"><a href="#Ampere" class="headerlink" title="Ampere"></a>Ampere</h1><p>Compute Capability: 8.0</p><hr><p>To be continued.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近 NV 刚发布了新架构的 GPU，自之前理过 &lt;a href=&quot;/2018/02/13/2018-02-13-intel/&quot;&gt;Intel CPU 的架构变化&lt;/a&gt; 后再来理一下 GPU 的。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/2018/02/12/2018-02-12-cod3/#6-6-Introduction-to-Graphics-Processing-Units&quot;&gt;硬/软件接口&lt;/a&gt;那篇有介绍过 GPU 的结构，当时也是以 Fermi 架构为例的，NV 很有意思的是会用一些历史上杰出的科学家的名字来命名自己的硬件架构。&lt;/p&gt;
&lt;p&gt;总体上，NV GPU 用到的 SIMT 基本编程模型都是一致的，每一代相对前代基本都会在 SM 数量、SM 内部各个处理单元的流水线结构等等方面有一些升级和改动。这篇暂时不涉及到渲染管线相关的部分，其他诸如多少 nm 工艺、内存频率提升等等也都先略过，只关注计算相关的硬件架构演进。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Computer Architecture" scheme="https://jcf94.com/categories/Computer-Architecture/"/>
    
    
      <category term="Microarchitecture" scheme="https://jcf94.com/tags/Microarchitecture/"/>
    
      <category term="NVIDIA" scheme="https://jcf94.com/tags/NVIDIA/"/>
    
  </entry>
  
  <entry>
    <title>TVM 拆包（二）：IR</title>
    <link href="https://jcf94.com/2020/03/08/2020-03-08-tvm2/"/>
    <id>https://jcf94.com/2020/03/08/2020-03-08-tvm2/</id>
    <published>2020-03-08T04:43:57.000Z</published>
    <updated>2020-03-08T08:30:15.168Z</updated>
    
    <content type="html"><![CDATA[<p>这一篇来记一下 TVM 跟语法树 IR 相关的一些内容。</p><p>TVM 是在 Halide 的基础上发展而来的一套编译架构。早一点版本的 TVM 其实还能看到 Halide 是作为一个 git 的 submodule 放在 TVM 目录里的，当时 TVM 自身的代码中还有一个 <code>HalideIR</code> 的 namespace，也有不少的结构是直接继承了 Halide IR 里的内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">using</span> HalideIR::Type;</span><br><span class="line"><span class="keyword">using</span> HalideIR::Float;</span><br><span class="line"><span class="keyword">using</span> HalideIR::Bool;</span><br><span class="line"><span class="keyword">using</span> HalideIR::Int;</span><br><span class="line"><span class="keyword">using</span> HalideIR::UInt;</span><br><span class="line"><span class="keyword">using</span> HalideIR::Handle;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><a id="more"></a><p><a href="https://github.com/apache/incubator-tvm/pull/3533" target="_blank" rel="noopener">[INFA][IR] Build and Evolve Low-level IR. Remove HalideIR dep. #3533</a> 这个 PR 之后，Halide 相关的部分逐渐从 TVM 中删掉了，之后可以说从代码层面已经跟 Halide 没有关系了。</p><blockquote><p>目前 TVM IR 的结构基本上还是从 Halide IR 一脉相承，可能以后 TVM IR 进一步演化以后跟 Halide 的差别就更大了。</p></blockquote><h1 id="IR"><a href="#IR" class="headerlink" title="IR"></a>IR</h1><p><code>expr.h</code> 里面定义了 TVM IR 的两个基础结构：Expr 和 Stmt，分别是语法表达式和语法树节点的基类。</p><p>Expr 的派生类有加减乘除、IntImm、FloatImm 等等，从文法上可以做一些 symbolic 的处理。</p><p>Stmt 的派生类有 AttrStmt（语法树属性节点）、Store（数据存储节点）、Allocate（数据 Buffer 分配节点）等等。</p><p>每个 Stmt 结构本身表示一个独立的语法树节点，但是语法树节点之间相互嵌套，通过 Stmt 的 body（Stmt 的通常结构）等成员继续向下查看就能够看到一颗完整的抽象语法树（AST）了。</p><p>例如 IfThenElse 这个 Stmt 的结构：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IfThenElse</span> :</span> <span class="keyword">public</span> StmtNode &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/*! \brief The condition. */</span></span><br><span class="line">  Expr condition;</span><br><span class="line">  <span class="comment">/*! \brief The branch to be executed when condition is true. */</span></span><br><span class="line">  Stmt then_case;</span><br><span class="line">  <span class="comment">/*! \brief The branch to be executed when condition is false, can be null. */</span></span><br><span class="line">  Stmt else_case;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">VisitAttrs</span><span class="params">(AttrVisitor* v)</span> final </span>&#123;</span><br><span class="line">    v-&gt;Visit(<span class="string">"condition"</span>, &amp;condition);</span><br><span class="line">    v-&gt;Visit(<span class="string">"then_case"</span>, &amp;then_case);</span><br><span class="line">    v-&gt;Visit(<span class="string">"else_case"</span>, &amp;else_case);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">TVM_DLL <span class="keyword">static</span> Stmt <span class="title">make</span><span class="params">(Expr condition, Stmt then_case, Stmt else_case = Stmt())</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">constexpr</span> <span class="keyword">const</span> <span class="keyword">char</span>* _type_key = <span class="string">"IfThenElse"</span>;</span><br><span class="line">  TVM_DECLARE_NODE_TYPE_INFO(IfThenElse, StmtNode);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>判断条件是个 Expr，then_case 和 else_case 都是另外两个 Stmt，再展开又是两棵子树。</p><h2 id="IRVisitor-amp-IRMutator"><a href="#IRVisitor-amp-IRMutator" class="headerlink" title="IRVisitor &amp; IRMutator"></a>IRVisitor &amp; IRMutator</h2><p>TVM 中定义了一些 ir_pass 来处理语法树，通过对语法树的修改和调整来完成编译优化的过程。</p><p>各种 ir_pass 的核心结构是 IRVisitor 和 IRMutator，从名称上也可以很容易看出来，IRVisitor 的功能是遍历语法树收集信息，本身对语法树的访问是只读的，然后通过 IRMutator 完成 ir_pass 需要的语法树修改需求。</p><p>看下 IRVisitor 的结构：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TVM_DLL</span> <span class="title">IRVisitor</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief recursively visit an IR node</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Visit</span><span class="params">(<span class="keyword">const</span> NodeRef&amp; node)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">const</span> FVisit&amp; f = vtable();</span><br><span class="line">    <span class="keyword">if</span> (node.defined()) f(node, <span class="keyword">this</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/*! \brief destructor */</span></span><br><span class="line">  <span class="keyword">virtual</span> ~IRVisitor() &#123;&#125;</span><br><span class="line">  <span class="comment">/*! \brief functor type of visitor */</span></span><br><span class="line">  <span class="keyword">using</span> FVisit = IRFunctor&lt;<span class="keyword">void</span>(<span class="keyword">const</span> NodeRef&amp;, IRVisitor*)&gt;;</span><br><span class="line">  <span class="comment">/*! \return internal vtable*/</span></span><br><span class="line">  <span class="function"><span class="keyword">static</span> FVisit&amp; <span class="title">vtable</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="comment">// overloadable visit function.</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Visit_</span><span class="params">(<span class="keyword">const</span> Variable* op)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Visit_</span><span class="params">(<span class="keyword">const</span> LetStmt* op)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Visit_</span><span class="params">(<span class="keyword">const</span> AttrStmt* op)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Visit_</span><span class="params">(<span class="keyword">const</span> IfThenElse* op)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Visit_</span><span class="params">(<span class="keyword">const</span> For* op)</span></span>;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里面最重要的实现其实是自己构造了一个虚函数表：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IRVisitor::FVisit&amp; IRVisitor::vtable() &#123;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  <span class="keyword">static</span> FVisit inst; <span class="keyword">return</span> inst;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之后 IRVisitor 的派生类只需要去重载针对不同 Stmt 类型的 <code>Visit_()</code> 函数就好了。</p><h2 id="Dump-Ast"><a href="#Dump-Ast" class="headerlink" title="Dump Ast"></a>Dump Ast</h2><p>看 TVM 代码包括 debug 的时候一开始会觉得两眼一抹黑连语法树长啥样都没有个概念，也不知道 build 过程中每个 ir_pass 具体做了什么事情，就很想要有个工具能把语法树打成图看下。</p><p>最开始 TVM 里面是找不到这种现成的工具的，后来在论坛里有看到别人提的这方面相关 RFC，不过我后续也没有去关注最后到底有没有收进 repo 里了。其实了解完 IRVisitor 的实现之后，自己写一个语法树的 Dumper 还是挺简单的。</p><p><code>ir_visitor.cc</code> 里面有一个 PostOrderVisit 的示例：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IRApplyVisit</span> :</span> <span class="keyword">public</span> IRVisitor &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  explicit IRApplyVisit(std::function&lt;void(const NodeRef&amp;)&gt; f) : f_(f) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">Visit</span><span class="params">(<span class="keyword">const</span> NodeRef&amp; node)</span> final </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (visited_.count(node.get()) != <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">    visited_.insert(node.get());</span><br><span class="line">    IRVisitor::Visit(node);</span><br><span class="line">    f_(node);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">const</span> NodeRef&amp;)&gt; f_;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unordered_set</span>&lt;<span class="keyword">const</span> Node*&gt; visited_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PostOrderVisit</span><span class="params">(<span class="keyword">const</span> NodeRef&amp; node, <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">const</span> NodeRef&amp;)&gt; fvisit)</span> </span>&#123;</span><br><span class="line">  IRApplyVisit(fvisit).Visit(node);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>按照对语法树后续遍历的顺序对每个语法树节点应用 <code>f_()</code> 函数，不过要实现语法树输出的目标，光靠这个还不够，需要再稍微进行一点点的扩充。</p><p>我们通过一个栈来记录下语法树上每一个 stmt 的从属关系，首先扩展一下上面那个 Visitor 来做到在访问 stmt 节点的前后分别调一个外部函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IRPrePostOrderVisitor</span> :</span> <span class="keyword">public</span> IRVisitor &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">IRPrePostOrderVisitor</span><span class="params">(<span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">const</span> NodeRef&amp;)&gt; f,</span></span></span><br><span class="line">                                 std::function&lt;void(const NodeRef&amp;)&gt; e) : f_(f), e_(e) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">Visit</span><span class="params">(<span class="keyword">const</span> NodeRef&amp; node)</span> final </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (visited_.count(node.get()) != <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">    visited_.insert(node.get());</span><br><span class="line">    f_(node);</span><br><span class="line">    IRVisitor::Visit(node);</span><br><span class="line">    e_(node);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">const</span> NodeRef&amp;)&gt; f_, e_;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unordered_set</span>&lt;<span class="keyword">const</span> Node*&gt; visited_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PrePostOrderVisit</span><span class="params">(<span class="keyword">const</span> NodeRef&amp; node,</span></span></span><br><span class="line"><span class="function"><span class="params">                       <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">const</span> NodeRef&amp;)&gt; fvisit,</span></span></span><br><span class="line"><span class="function"><span class="params">                       <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">const</span> NodeRef&amp;)&gt; evisit)</span> </span>&#123;</span><br><span class="line">  IRPrePostOrderVisitor(fvisit, evisit).Visit(node);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> <code>PrePostOrderVisit()</code> 相应的在 <code>ir_visitor.h</code> 里也要添加一下。</p><p>接下来再往 <code>api_pass.cc</code> 里添加一下<code>_PrePostOrderVisit</code>的函数注册：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TVM_REGISTER_API(<span class="string">"_PrePostOrderVisit"</span>)</span><br><span class="line">.set_body([](TVMArgs args, TVMRetValue *ret) &#123;</span><br><span class="line">    Stmt stmt = args[<span class="number">0</span>];</span><br><span class="line">    PackedFunc f = args[<span class="number">1</span>];</span><br><span class="line">    PackedFunc e = args[<span class="number">2</span>];</span><br><span class="line">    ir::PrePostOrderVisit(stmt, f, e);</span><br><span class="line">  &#125;);</span><br></pre></td></tr></table></figure><p>这样在 C++ 部分的工作就完成了，之后是 Python 这一层，我们的 dump 目标是语法树，所以找个能直接拿到 stmt 结构的地方，<code>build_module.py</code> 就很不错。</p><p><code>lower()</code> 是从 TVM IR 往能够运行的代码编译的第一步，涉及到多种 ir_pass 的使用，不同 ir_pass 的前后插入 dump ast 的代码可以帮助我们快速搞清楚每个 ir_pass 到底实际做了什么事情。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower</span><span class="params">(...)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> lower_phase0:</span><br><span class="line">        stmt = f(stmt)</span><br><span class="line">    <span class="comment"># Phase 1</span></span><br><span class="line">    stmt = ir_pass.StorageFlatten(stmt, binds, <span class="number">64</span>, cfg.instrument_bound_checkers)</span><br><span class="line">    stmt = ir_pass.CanonicalSimplify(stmt)</span><br><span class="line"></span><br><span class="line">    dump_ast(stmt)<span class="comment"># &lt;-------------- here</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> lower_phase1:</span><br><span class="line">        stmt = f(stmt)</span><br><span class="line">    <span class="comment"># Phase 2</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> simple_mode:</span><br><span class="line">        stmt = ir_pass.LoopPartition(stmt, cfg.partition_const_loop)</span><br><span class="line">    <span class="keyword">if</span> cfg.disable_vectorize:</span><br><span class="line">        stmt = ir_pass.SkipVectorize(stmt)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        stmt = ir_pass.VectorizeLoop(stmt)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>接下来看一下我们需要在 <code>dump_ast()</code> 里面写上什么：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_ast</span><span class="params">(stmt)</span>:</span></span><br><span class="line">    stack = []</span><br><span class="line">    ast_node = []</span><br><span class="line">    ast_edge = []</span><br><span class="line">    count = [<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_func</span><span class="params">(stmt)</span>:</span></span><br><span class="line">        node_idx = count[<span class="number">0</span>]</span><br><span class="line">        count[<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        ast_node.append([node_idx, stmt])</span><br><span class="line">        <span class="keyword">if</span> len(stack):</span><br><span class="line">            ast_edge.append([stack[<span class="number">-1</span>], node_idx])</span><br><span class="line"></span><br><span class="line">        stack.append(node_idx)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">post_func</span><span class="params">(stmt)</span>:</span></span><br><span class="line">        <span class="keyword">del</span> stack[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    _api_internal._PrePostOrderVisit(stmt, pre_func, post_func)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"graph.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(<span class="string">"digraph &#123;\n"</span>)</span><br><span class="line">        f.write(<span class="string">"    node [shape=matrix]\n"</span>)</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> ast_node:</span><br><span class="line">            ast_type = type(node[<span class="number">1</span>])</span><br><span class="line">            ast_str = str(node[<span class="number">1</span>]).replace(<span class="string">"\n"</span>, <span class="string">"\\l"</span>).replace(<span class="string">"\\n"</span>, <span class="string">"\\l"</span>)</span><br><span class="line">            f.write(<span class="string">"    node%d"</span> % (node[<span class="number">0</span>]))</span><br><span class="line">            f.write(<span class="string">"[label=\"%s\n%s\"]"</span> % (ast_type, ast_str))</span><br><span class="line">            f.write(<span class="string">";\n"</span>)</span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> ast_edge:</span><br><span class="line">            f.write(<span class="string">"    node%d -&gt; node%d;\n"</span> % (edge[<span class="number">0</span>], edge[<span class="number">1</span>]))</span><br><span class="line">        f.write(<span class="string">"&#125;\n"</span>)</span><br></pre></td></tr></table></figure><p>思路也是很简单的，我们只要在 PrePostOrderVisit 访问 stmt 节点前将节点入栈，然后访问节点结束后将节点退栈就完事了。</p><p>这个地方也体现出上一篇中 TVM 特别搞出来的这套跨 Python 和 C++ 混合运行的 PackedFunc 机制的便利性，这里事实上我们是从 Python 层开始，调了一个 C++ 的函数，然后在这个 C++ 的函数里面又回调了两个 Python 的函数，并且这个过程中数据还是存在我们在 Python 层创建的结构上的。</p><hr><p>试一下下面这段示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n = tvm.var(<span class="string">"n"</span>)</span><br><span class="line">A = tvm.placeholder((n,), name=<span class="string">'A'</span>)</span><br><span class="line">B = tvm.placeholder((n,), name=<span class="string">'B'</span>)</span><br><span class="line">C = tvm.compute(A.shape, <span class="keyword">lambda</span> i: A[i] + B[i], name=<span class="string">"C"</span>)</span><br><span class="line"></span><br><span class="line">s = tvm.create_schedule([C.op])</span><br><span class="line">bx, tx = s[C].split(C.op.axis[<span class="number">0</span>], factor=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">res = tvm.lower(s, [A, B, C], simple_mode=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>通过 dump_ast 打出来的 dot 代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">digraph &#123;</span><br><span class="line">    node [shape&#x3D;matrix]</span><br><span class="line">    node0[label&#x3D;&quot;&lt;class &#39;tvm.stmt.ProducerConsumer&#39;&gt;</span><br><span class="line">produce C &#123;\l  for (i.outer, 0, ((n + 63)&#x2F;64)) &#123;\l    for (i.inner, 0, 64) &#123;\l      if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l        if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l          C[((i.outer*64) + i.inner)] &#x3D; (A[((i.outer*64) + i.inner)] + B[((i.outer*64) + i.inner)])\l        &#125;\l      &#125;\l    &#125;\l  &#125;\l&#125;\l&quot;];</span><br><span class="line">    node1[label&#x3D;&quot;&lt;class &#39;tvm.stmt.For&#39;&gt;</span><br><span class="line">for (i.outer, 0, ((n + 63)&#x2F;64)) &#123;\l  for (i.inner, 0, 64) &#123;\l    if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l      if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l        C[((i.outer*64) + i.inner)] &#x3D; (A[((i.outer*64) + i.inner)] + B[((i.outer*64) + i.inner)])\l      &#125;\l    &#125;\l  &#125;\l&#125;\l&quot;];</span><br><span class="line">    node2[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">0&quot;];</span><br><span class="line">    node3[label&#x3D;&quot;&lt;class &#39;tvm.expr.Div&#39;&gt;</span><br><span class="line">((n + 63)&#x2F;64)&quot;];</span><br><span class="line">    node4[label&#x3D;&quot;&lt;class &#39;tvm.expr.Add&#39;&gt;</span><br><span class="line">(n + 63)&quot;];</span><br><span class="line">    node5[label&#x3D;&quot;&lt;class &#39;tvm.expr.Var&#39;&gt;</span><br><span class="line">n&quot;];</span><br><span class="line">    node6[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">63&quot;];</span><br><span class="line">    node7[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">64&quot;];</span><br><span class="line">    node8[label&#x3D;&quot;&lt;class &#39;tvm.stmt.For&#39;&gt;</span><br><span class="line">for (i.inner, 0, 64) &#123;\l  if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l    if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l      C[((i.outer*64) + i.inner)] &#x3D; (A[((i.outer*64) + i.inner)] + B[((i.outer*64) + i.inner)])\l    &#125;\l  &#125;\l&#125;\l&quot;];</span><br><span class="line">    node9[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">0&quot;];</span><br><span class="line">    node10[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">64&quot;];</span><br><span class="line">    node11[label&#x3D;&quot;&lt;class &#39;tvm.stmt.IfThenElse&#39;&gt;</span><br><span class="line">if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l  if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l    C[((i.outer*64) + i.inner)] &#x3D; (A[((i.outer*64) + i.inner)] + B[((i.outer*64) + i.inner)])\l  &#125;\l&#125;\l&quot;];</span><br><span class="line">    node12[label&#x3D;&quot;&lt;class &#39;tvm.expr.Call&#39;&gt;</span><br><span class="line">likely((((i.outer*64) + i.inner) &lt; n))&quot;];</span><br><span class="line">    node13[label&#x3D;&quot;&lt;class &#39;tvm.expr.LT&#39;&gt;</span><br><span class="line">(((i.outer*64) + i.inner) &lt; n)&quot;];</span><br><span class="line">    node14[label&#x3D;&quot;&lt;class &#39;tvm.expr.Add&#39;&gt;</span><br><span class="line">((i.outer*64) + i.inner)&quot;];</span><br><span class="line">    node15[label&#x3D;&quot;&lt;class &#39;tvm.expr.Mul&#39;&gt;</span><br><span class="line">(i.outer*64)&quot;];</span><br><span class="line">    node16[label&#x3D;&quot;&lt;class &#39;tvm.expr.Var&#39;&gt;</span><br><span class="line">i.outer&quot;];</span><br><span class="line">    node17[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">64&quot;];</span><br><span class="line">    node18[label&#x3D;&quot;&lt;class &#39;tvm.expr.Var&#39;&gt;</span><br><span class="line">i.inner&quot;];</span><br><span class="line">    node19[label&#x3D;&quot;&lt;class &#39;tvm.stmt.IfThenElse&#39;&gt;</span><br><span class="line">if (likely((((i.outer*64) + i.inner) &lt; n))) &#123;\l  C[((i.outer*64) + i.inner)] &#x3D; (A[((i.outer*64) + i.inner)] + B[((i.outer*64) + i.inner)])\l&#125;\l&quot;];</span><br><span class="line">    node20[label&#x3D;&quot;&lt;class &#39;tvm.expr.Call&#39;&gt;</span><br><span class="line">likely((((i.outer*64) + i.inner) &lt; n))&quot;];</span><br><span class="line">    node21[label&#x3D;&quot;&lt;class &#39;tvm.expr.LT&#39;&gt;</span><br><span class="line">(((i.outer*64) + i.inner) &lt; n)&quot;];</span><br><span class="line">    node22[label&#x3D;&quot;&lt;class &#39;tvm.expr.Add&#39;&gt;</span><br><span class="line">((i.outer*64) + i.inner)&quot;];</span><br><span class="line">    node23[label&#x3D;&quot;&lt;class &#39;tvm.expr.Mul&#39;&gt;</span><br><span class="line">(i.outer*64)&quot;];</span><br><span class="line">    node24[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">64&quot;];</span><br><span class="line">    node25[label&#x3D;&quot;&lt;class &#39;tvm.stmt.Store&#39;&gt;</span><br><span class="line">C[((i.outer*64) + i.inner)] &#x3D; (A[((i.outer*64) + i.inner)] + B[((i.outer*64) + i.inner)])\l&quot;];</span><br><span class="line">    node26[label&#x3D;&quot;&lt;class &#39;tvm.expr.Add&#39;&gt;</span><br><span class="line">(A[((i.outer*64) + i.inner)] + B[((i.outer*64) + i.inner)])&quot;];</span><br><span class="line">    node27[label&#x3D;&quot;&lt;class &#39;tvm.expr.Load&#39;&gt;</span><br><span class="line">A[((i.outer*64) + i.inner)]&quot;];</span><br><span class="line">    node28[label&#x3D;&quot;&lt;class &#39;tvm.expr.Add&#39;&gt;</span><br><span class="line">((i.outer*64) + i.inner)&quot;];</span><br><span class="line">    node29[label&#x3D;&quot;&lt;class &#39;tvm.expr.Mul&#39;&gt;</span><br><span class="line">(i.outer*64)&quot;];</span><br><span class="line">    node30[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">64&quot;];</span><br><span class="line">    node31[label&#x3D;&quot;&lt;class &#39;tvm.expr.UIntImm&#39;&gt;</span><br><span class="line">(bool)1&quot;];</span><br><span class="line">    node32[label&#x3D;&quot;&lt;class &#39;tvm.expr.Load&#39;&gt;</span><br><span class="line">B[((i.outer*64) + i.inner)]&quot;];</span><br><span class="line">    node33[label&#x3D;&quot;&lt;class &#39;tvm.expr.Add&#39;&gt;</span><br><span class="line">((i.outer*64) + i.inner)&quot;];</span><br><span class="line">    node34[label&#x3D;&quot;&lt;class &#39;tvm.expr.Mul&#39;&gt;</span><br><span class="line">(i.outer*64)&quot;];</span><br><span class="line">    node35[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">64&quot;];</span><br><span class="line">    node36[label&#x3D;&quot;&lt;class &#39;tvm.expr.UIntImm&#39;&gt;</span><br><span class="line">(bool)1&quot;];</span><br><span class="line">    node37[label&#x3D;&quot;&lt;class &#39;tvm.expr.Add&#39;&gt;</span><br><span class="line">((i.outer*64) + i.inner)&quot;];</span><br><span class="line">    node38[label&#x3D;&quot;&lt;class &#39;tvm.expr.Mul&#39;&gt;</span><br><span class="line">(i.outer*64)&quot;];</span><br><span class="line">    node39[label&#x3D;&quot;&lt;class &#39;tvm.expr.IntImm&#39;&gt;</span><br><span class="line">64&quot;];</span><br><span class="line">    node40[label&#x3D;&quot;&lt;class &#39;tvm.expr.UIntImm&#39;&gt;</span><br><span class="line">(bool)1&quot;];</span><br><span class="line">    node0 -&gt; node1;</span><br><span class="line">    node1 -&gt; node2;</span><br><span class="line">    node1 -&gt; node3;</span><br><span class="line">    node3 -&gt; node4;</span><br><span class="line">    node4 -&gt; node5;</span><br><span class="line">    node4 -&gt; node6;</span><br><span class="line">    node3 -&gt; node7;</span><br><span class="line">    node1 -&gt; node8;</span><br><span class="line">    node8 -&gt; node9;</span><br><span class="line">    node8 -&gt; node10;</span><br><span class="line">    node8 -&gt; node11;</span><br><span class="line">    node11 -&gt; node12;</span><br><span class="line">    node12 -&gt; node13;</span><br><span class="line">    node13 -&gt; node14;</span><br><span class="line">    node14 -&gt; node15;</span><br><span class="line">    node15 -&gt; node16;</span><br><span class="line">    node15 -&gt; node17;</span><br><span class="line">    node14 -&gt; node18;</span><br><span class="line">    node11 -&gt; node19;</span><br><span class="line">    node19 -&gt; node20;</span><br><span class="line">    node20 -&gt; node21;</span><br><span class="line">    node21 -&gt; node22;</span><br><span class="line">    node22 -&gt; node23;</span><br><span class="line">    node23 -&gt; node24;</span><br><span class="line">    node19 -&gt; node25;</span><br><span class="line">    node25 -&gt; node26;</span><br><span class="line">    node26 -&gt; node27;</span><br><span class="line">    node27 -&gt; node28;</span><br><span class="line">    node28 -&gt; node29;</span><br><span class="line">    node29 -&gt; node30;</span><br><span class="line">    node27 -&gt; node31;</span><br><span class="line">    node26 -&gt; node32;</span><br><span class="line">    node32 -&gt; node33;</span><br><span class="line">    node33 -&gt; node34;</span><br><span class="line">    node34 -&gt; node35;</span><br><span class="line">    node32 -&gt; node36;</span><br><span class="line">    node25 -&gt; node37;</span><br><span class="line">    node37 -&gt; node38;</span><br><span class="line">    node38 -&gt; node39;</span><br><span class="line">    node25 -&gt; node40;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过 GraphViz 这种 dot 可视化工具处理一下：</p><p><img data-src="http://jcf94.com/download/2020-03-08-tvm2-graph_exp.png" alt="Vis Result"></p><p>当然这个实现还是相当简单了，根据每个节点的类型等等还可以再加一些更复杂的判断逻辑，控制一下输出的内容量等等，以及其实可以看到里面还有很多重复的节点也都可以被筛掉。</p><p>Relay IR 的整体处理结构跟 TVM IR 一致，用类似的方法也可以把 Relay 那一层的 AST 打出来。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一篇来记一下 TVM 跟语法树 IR 相关的一些内容。&lt;/p&gt;
&lt;p&gt;TVM 是在 Halide 的基础上发展而来的一套编译架构。早一点版本的 TVM 其实还能看到 Halide 是作为一个 git 的 submodule 放在 TVM 目录里的，当时 TVM 自身的代码中还有一个 &lt;code&gt;HalideIR&lt;/code&gt; 的 namespace，也有不少的结构是直接继承了 Halide IR 里的内容：&lt;/p&gt;
&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; HalideIR::Type;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; HalideIR::Float;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; HalideIR::Bool;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; HalideIR::Int;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; HalideIR::UInt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; HalideIR::Handle;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;...&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TVM" scheme="https://jcf94.com/tags/TVM/"/>
    
      <category term="Compile" scheme="https://jcf94.com/tags/Compile/"/>
    
  </entry>
  
  <entry>
    <title>TVM 拆包（一）：Runtime basics</title>
    <link href="https://jcf94.com/2019/09/14/2019-09-14-tvm/"/>
    <id>https://jcf94.com/2019/09/14/2019-09-14-tvm/</id>
    <published>2019-09-14T01:53:09.000Z</published>
    <updated>2020-03-08T05:01:26.020Z</updated>
    
    <content type="html"><![CDATA[<p>还是应该多写东西，感觉离开学校以后写的越来越少了，真是惭愧。</p><p>之后的工作有很多是要跟神经网络的编译执行相关，准备继 TF 拆包之后再来记一下我对 TVM 的探索过程。这个系列的坑会开多久也不一定，毕竟之前 TF 的其实也只做了一点点微小的工作，还有很多方面的内容没有看，另外 TF 要更到 2.0 之后可能有不少地方已经有一些变动了。</p><p>（也许有空也会看一下 XLA 吧，不过那个应该是要合到 TF 的拆包系列里面去了）</p><p>一些基础的背景可以见前面一篇对 TVM 作者一门课的课程记录：<a href="/2018/10/04/2018-10-04-cse559w/">【CSE 599W： Systems for ML】</a></p><a id="more"></a><hr><p>第一篇先从代码中一些基础的结构开始，最早读代码的时候没看明白一些东西是怎么实现的，为了看懂细节吃了不少的苦头。</p><p>参考文档：<a href="https://docs.tvm.ai/dev/runtime.html" target="_blank" rel="noopener">【TVM Runtime System】</a></p><h1 id="Node-amp-NodeRef"><a href="#Node-amp-NodeRef" class="headerlink" title="Node &amp; NodeRef"></a>Node &amp; NodeRef</h1><p>Node 和 NodeRef 这两个类在 TVM 中几乎是所有对象的基类了，Node 是功能本体，NodeRef 可以看成是对 Node 的一个指针引用。举例来说 TVM IR 语法树中的两个结构 Statement 和 Expression 的实际存储对象是 <code>StmtNode</code> 和 <code>ExprNode</code>，从 Node 继承而来，但是在被其他结构用到的时候用的却是 <code>Stmt</code> 和 <code>Expr</code> 两个结构，从 NodeRef 继承而来。</p><p>Node 这个结构本身没什么好看的，看一下 NodeRef 的实现：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*! \brief Base class of all node reference object */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NodeRef</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/*! \brief type indicate the container type */</span></span><br><span class="line">  <span class="keyword">using</span> ContainerType = Node;</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief Comparator</span></span><br><span class="line"><span class="comment">   * \param other Another node ref.</span></span><br><span class="line"><span class="comment">   * \return the compare result.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> NodeRef&amp; other) <span class="keyword">const</span>;</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief Comparator</span></span><br><span class="line"><span class="comment">   * \param other Another node ref.</span></span><br><span class="line"><span class="comment">   * \return the compare result.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">same_as</span><span class="params">(<span class="keyword">const</span> NodeRef&amp; other)</span> <span class="keyword">const</span></span>;</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief Comparator</span></span><br><span class="line"><span class="comment">   * \param other Another node ref.</span></span><br><span class="line"><span class="comment">   * \return the compare result.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span>&lt;(<span class="keyword">const</span> NodeRef&amp; other) <span class="keyword">const</span>;</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief Comparator</span></span><br><span class="line"><span class="comment">   * \param other Another node ref.</span></span><br><span class="line"><span class="comment">   * \return the compare result.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span>!=(<span class="keyword">const</span> NodeRef&amp; other) <span class="keyword">const</span>;</span><br><span class="line">  <span class="comment">/*! \return the hash function for NodeRef */</span></span><br><span class="line">  <span class="function"><span class="keyword">inline</span> size_t <span class="title">hash</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">  <span class="comment">/*! \return whether the expression is null */</span></span><br><span class="line">  <span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">defined</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">  <span class="comment">/*! \return the internal type index of IRNode */</span></span><br><span class="line">  <span class="function"><span class="keyword">inline</span> uint32_t <span class="title">type_index</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">  <span class="comment">/*! \return the internal node pointer */</span></span><br><span class="line">  <span class="function"><span class="keyword">inline</span> <span class="keyword">const</span> Node* <span class="title">get</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">  <span class="comment">/*! \return the internal node pointer */</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> Node* <span class="keyword">operator</span>-&gt;() <span class="keyword">const</span>;</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief Downcast this ir node to its actual type (e.g. Add, or</span></span><br><span class="line"><span class="comment">   * Select). This returns nullptr if the node is not of the requested</span></span><br><span class="line"><span class="comment">   * type. Example usage:</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * if (const Add *add = node-&gt;as&lt;Add&gt;()) &#123;</span></span><br><span class="line"><span class="comment">   *   // This is an add node</span></span><br><span class="line"><span class="comment">   * &#125;</span></span><br><span class="line"><span class="comment">   * \tparam T the target type, must be subtype of IRNode</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">inline</span> <span class="keyword">const</span> T *<span class="title">as</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">  <span class="comment">/*!</span></span><br><span class="line"><span class="comment">   * \brief A more powerful version of as that also works with</span></span><br><span class="line"><span class="comment">   *  intermediate base types.</span></span><br><span class="line"><span class="comment">   * \tparam T the target type, must be subtype of IRNode</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">inline</span> <span class="keyword">const</span> T *<span class="title">as_derived</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">  <span class="comment">/*! \brief default constructor */</span></span><br><span class="line">  NodeRef() = <span class="keyword">default</span>;</span><br><span class="line">  explicit NodeRef(NodePtr&lt;Node&gt; node) : node_(node) &#123;&#125;</span><br><span class="line">  <span class="comment">/*! \brief the internal node object, do not touch  */</span></span><br><span class="line">  NodePtr&lt;Node&gt; node_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">const</span> Node* NodeRef::<span class="keyword">operator</span>-&gt;() <span class="keyword">const</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> node_.get();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最关键的点在于 NodeRef 对它的 <code>-&gt;</code>运算符做了一下重载，即返回自己实际代表的实体对象，所以 NodeRef 以及其派生结构虽然实际是个对象，但是在代码里面可以当成指针来用，<code>-&gt;</code>运算符之后直接跟的就是本体 Node 的成员。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*! \brief Container of all statements */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stmt</span> :</span> <span class="keyword">public</span> NodeRef &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  TVM_DEFINE_NODE_REF_METHODS(Stmt, NodeRef, StmtNode);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief Macro to define common node ref methods.</span></span><br><span class="line"><span class="comment"> * \param TypeName The name of the NodeRef.</span></span><br><span class="line"><span class="comment"> * \param BaseTypeName The Base type.</span></span><br><span class="line"><span class="comment"> * \param NodeName The node container type.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TVM_DEFINE_NODE_REF_METHODS(TypeName, BaseTypeName, NodeName)   \</span></span><br><span class="line">  TypeName() &#123;&#125;                                                         \</span><br><span class="line">  explicit TypeName(::tvm::NodePtr&lt;::tvm::Node&gt; n) : BaseTypeName(n) &#123;&#125; \</span><br><span class="line">  <span class="keyword">const</span> NodeName* <span class="keyword">operator</span>-&gt;() <span class="keyword">const</span> &#123;                                  \</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> NodeName*&gt;(node_.get());                   \</span><br><span class="line">  &#125;                                                                     \</span><br><span class="line">  <span class="function"><span class="keyword">operator</span> <span class="title">bool</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;defined(); &#125;                     \</span><br><span class="line">  <span class="keyword">using</span> ContainerType = NodeName;</span><br></pre></td></tr></table></figure><p>以上面的 <code>Stmt</code> 类为例，其他很多 NodeRef 的派生类结构也会用一个 <code>TVM_DEFINE_NODE_REF</code> 的宏来扩展出这部分的关键代码。</p><hr><p>那么问题来了，为什么要设计成这个样子呢。</p><blockquote><p>这里谈一下我自己的理解，如果有问题的话也欢迎看到的同学指正一下。</p></blockquote><p>一方面可能是为了方便下面 C++ runtime 和 Python 部分所有结构的无缝衔接；另一方面从上面的代码也可以看到，NodeRef 重载了 <code>-&gt;</code> 运算符之后返回的是个 const 的指针对象，所以通过 <code>-&gt;</code> 访问到的实际的成员结构都是只读的了，这就保证了 TVM 这套复杂系统里面各种数据结构的安全性。</p><p>当然严格的只读访问在某些情况下是不够的，所以 TVM 提供了 <code>CopyOnWrite()</code> 的机制，如果某个 NodeRef 类的定义中包含了 <code>TVM_DEFINE_NODE_REF_COW</code> 这个宏的话，可以通过 <code>NodeRef.CopyOnWrite()</code> 获得一个可修改的 Node 指针，之后对成员内容的修改均通过这个指针来做就可以了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief Macro to define CopyOnWrite function in a NodeRef.</span></span><br><span class="line"><span class="comment"> * \param NodeName The Type of the Node.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  CopyOnWrite will generate a unique copy of the internal node.</span></span><br><span class="line"><span class="comment"> *  The node will be copied if it is referenced by multiple places.</span></span><br><span class="line"><span class="comment"> *  The function returns the raw pointer to the node to allow modification</span></span><br><span class="line"><span class="comment"> *  of the content.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * \code</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  MyCOWNodeRef ref, ref2;</span></span><br><span class="line"><span class="comment"> *  ref2 = ref;</span></span><br><span class="line"><span class="comment"> *  ref.CopyOnWrite()-&gt;value = new_value;</span></span><br><span class="line"><span class="comment"> *  assert(ref2-&gt;value == old_value);</span></span><br><span class="line"><span class="comment"> *  assert(ref-&gt;value == new_value);</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * \endcode</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TVM_DEFINE_NODE_REF_COW(NodeName)                               \</span></span><br><span class="line">  <span class="function">NodeName* <span class="title">CopyOnWrite</span><span class="params">()</span> </span>&#123;                                             \</span><br><span class="line">      CHECK(node_ != <span class="literal">nullptr</span>);                                          \</span><br><span class="line">      <span class="keyword">if</span> (!node_.unique())  &#123;                                           \</span><br><span class="line">        NodePtr&lt;NodeName&gt; n = make_node&lt;NodeName&gt;(*(<span class="keyword">operator</span>-&gt;()));     \</span><br><span class="line">        NodePtr&lt;Node&gt;(<span class="built_in">std</span>::move(n)).swap(node_);                        \</span><br><span class="line">      &#125;                                                                 \</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;NodeName*&gt;(node_.get());                       \</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>话说 <code>CopyOnWrite()</code> 这个函数名称我感觉可能不是特别确切，也许改成 <code>GetMutablePtr()</code> 之类的会更好点？因为这个实际上并不 Copy，直接调用这个函数返回的是对这个 NodeRef 自己所指代对象的指针，之后的改动也都是对这个 Node 自身做的。</p><p>如果确切希望实现 Copy 的语义，则需要像前面注释里面示例的那样，先用另一个 <code>ref2</code> 复制一份 <code>ref</code>，之后再在 <code>ref2</code> 上进行修改。</p><h1 id="PackedFunc"><a href="#PackedFunc" class="headerlink" title="PackedFunc"></a>PackedFunc</h1><p>TVM 的整个软件栈涉及到很多高层脚本语言（Python、JavaScript）和 C++ 运行时的交互，因此这里提供了一套 PackedFunc 的基础用来把整个过程方便地串接起来。</p><p>第一次看到这种实现时真的是被惊到了，感觉非常神奇。</p><p>在 C++ 层面创建一个函数，可以直接进行本地调用：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;tvm/runtime/packed_func.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MyAdd</span><span class="params">(TVMArgs args, TVMRetValue* rv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// automatically convert arguments to desired type.</span></span><br><span class="line">  <span class="keyword">int</span> a = args[<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">int</span> b = args[<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// automatically assign value return to rv</span></span><br><span class="line">  *rv = a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">CallPacked</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  PackedFunc myadd = PackedFunc(MyAdd);</span><br><span class="line">  <span class="comment">// get back 3</span></span><br><span class="line">  <span class="keyword">int</span> c = myadd(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以通过 API 注册之后（要注册到 TVM 的 C++ 运行时库里面去），从 Python 层进行调用：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// register a global packed function in c++</span></span><br><span class="line">TVM_REGISTER_GLOBAL(<span class="string">"myadd"</span>)</span><br><span class="line">.set_body(MyAdd);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"></span><br><span class="line">myadd = tvm.get_global_func(<span class="string">"myadd"</span>)</span><br><span class="line"><span class="comment"># prints 3</span></span><br><span class="line">print(myadd(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>反过来 Python 层写好的函数也可以直接从 C++ 层调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TVM_REGISTER_GLOBAL(<span class="string">"callhello"</span>)</span><br><span class="line">.set_body([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  f(<span class="string">"hello world"</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span><span class="params">(msg)</span>:</span></span><br><span class="line">  print(msg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to PackedFunc</span></span><br><span class="line">f = tvm.convert(callback)</span><br><span class="line">callhello = tvm.get_global_func(<span class="string">"callhello"</span>)</span><br><span class="line"><span class="comment"># prints hello world</span></span><br><span class="line">callhello(f)</span><br></pre></td></tr></table></figure><p>Python 层定义的 <code>callback(msg)</code> 通过 <code>callhello</code> 传递给 C++ 层，C++ 层执行时直接从输入参数中得到了 Python 的函数对象并调用执行。</p><hr><p>实现方面，C++ 层的 PackedFunc 是一个对 std::function 对象的封装结构，而 Python 层面<code>tvm.convert</code> 实际上是把 Python 的函数用 ctype 做了一下封装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TVMPackedCFunc = ctypes.CFUNCTYPE(</span><br><span class="line">    ctypes.c_int,</span><br><span class="line">    ctypes.POINTER(TVMValue),</span><br><span class="line">    ctypes.POINTER(ctypes.c_int),</span><br><span class="line">    ctypes.c_int,</span><br><span class="line">    ctypes.c_void_p,</span><br><span class="line">    ctypes.c_void_p)</span><br></pre></td></tr></table></figure><p>PackedFunc 对函数的输入参数、返回值的解析处理做了比较精巧的处理，最终达到了从 API 层面看上去非常好的使用体验。</p><blockquote><p>Amazing!</p><p>有空的话可以试一下把 TVM 里面的这部分内容单独扒出来，这个实现思路真的非常有意思。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;还是应该多写东西，感觉离开学校以后写的越来越少了，真是惭愧。&lt;/p&gt;
&lt;p&gt;之后的工作有很多是要跟神经网络的编译执行相关，准备继 TF 拆包之后再来记一下我对 TVM 的探索过程。这个系列的坑会开多久也不一定，毕竟之前 TF 的其实也只做了一点点微小的工作，还有很多方面的内容没有看，另外 TF 要更到 2.0 之后可能有不少地方已经有一些变动了。&lt;/p&gt;
&lt;p&gt;（也许有空也会看一下 XLA 吧，不过那个应该是要合到 TF 的拆包系列里面去了）&lt;/p&gt;
&lt;p&gt;一些基础的背景可以见前面一篇对 TVM 作者一门课的课程记录：&lt;a href=&quot;/2018/10/04/2018-10-04-cse559w/&quot;&gt;【CSE 599W： Systems for ML】&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TVM" scheme="https://jcf94.com/tags/TVM/"/>
    
      <category term="Compile" scheme="https://jcf94.com/tags/Compile/"/>
    
  </entry>
  
  <entry>
    <title>大半年的流水账</title>
    <link href="https://jcf94.com/2019/06/18/2019-06-18-note/"/>
    <id>https://jcf94.com/2019/06/18/2019-06-18-note/</id>
    <published>2019-06-18T07:03:11.000Z</published>
    <updated>2020-01-29T03:08:09.402Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center">            <p>生活总是不知道会在什么时候给你点“惊喜”。</p>          </blockquote><p>过去的一年间发生了挺多事情的，对我自己来说有好事也有坏事，上上下下大起大落，最开始的计划被一次次地打乱，每一次都算是艰难的抉择吧。</p><p>看了下博客也有半年没更了，更让我觉得惊讶的是上一次挂了“随笔” tag 的居然已经是2年前了（可见这两年读研生涯中我是多么正经…），准备来理一理过去这大半年的流水账。</p><a id="more"></a><hr><h1 id="校招的大潮"><a href="#校招的大潮" class="headerlink" title="校招的大潮"></a>校招的大潮</h1><p>大概是去年6月多投出去的第一份简历，到10月中旬签下 offer 为止大概经历过 NVIDIA、大疆、华为、网易游戏、网易云音乐、腾讯、阿里的面试，然后聊过几家独角兽（后来有一天接到厦门航空招聘组的电话真的是吓到我了！招副机长？还专业不限？入职后再做飞行培训？……也许我应该应下来跟他们多聊聊的）。</p><p>NV 的笔试题很简单，然后6月份就迎来了我人生中的第一次电话面试。虽然当时完全没怎么准备，却也只能硬着头皮上了。1 小时的电面里面大概问了我快40道题，幸而对 GPU 和 C++ 这块还算熟悉，有惊无险。之后是 4 位面试官一共 4 小时的车轮面（一人一小时，连着一个一个上的那种）。</p><p>虽然面对 NV 时很匆忙，但还是很感谢这段面试给我的经验，经历过车轮面之后后面再来啥都不怕啦。NV的校招流程整体给我的体验真的非常好，谈薪资和 offer 之前都不管你未来是不是有可能拒，一律先免费给做一次入职体检！以至于后来抉择完选了别家的时候真的是觉得很不好意思，觉得有点愧对一直帮我处理了整个校招过程的HR小姐姐。我投的岗位是 Deep Learning Performance Architect，其实工作内容都是我很想做的方面，只可惜后来有了另外的考虑。</p><p>网易游戏是线下笔试，当时运气不好的是笔试的日子刚好遇上实验室有另外的事情，只好放弃。网易云去了杭州现场，还找在网易的上届师兄蹭了一顿他们的食堂（不得不说！猪厂的食堂是真的赞啊！什么都有，而且员工吃饭免费！要啥自行车！）。可惜投的岗不太对，面我的小哥说他们部门是做推荐算法的，要的是纯算法方向，看完我简历说来他们这里不太合适，可以帮我推荐到百度的部门。额，知道是这个情况，那后来就干脆开始跟小哥聊天了（原来小哥自己就是从熊厂跳过来的）。最后还是婉拒了他的推荐，真的都是很好的人啊。</p><p>华为没啥好说的，聊得还行，奇怪的是我投的明明是杭州的岗，最后说录的却是深圳。体验就没前两个好了。</p><p>阿里的 PAI 算是我之前很早就看上的部门了，一年前就在知乎上看到了他们发的研究分享以及招聘公告，然后在他们校招开放的第一天开始了内推的过程。一路下来，3面和交叉面都给了我很大的压力，完全是被按在地上摩擦的那种。虽然觉得自己当时没有表现好，但是经历过面试就能感觉到他们是真的对我感兴趣的方向非常了解，我也相信如果我去了这里也一定能够学到我真正想要的东西。他们家跟 NV 岗位的区别可能是工作重心会更偏系统一点，NV应该是跟偏向加速卡这块。纠结了很久。</p><blockquote><p>P.S.可能最后杭州这个工作地点对我来说也比其他城市有更高的吸引力吧。</p></blockquote><p>然后跳出来半路截胡的是腾讯和大疆，之前做梦都没想到过能够拿到这么高的 package。遥想当年本科毕业的时候，想着拿个十几二十W就顶天了（当时没定下来要不要保研的时候身为一个魅粉还差点就去了珠海…），面对好几倍的数字我是真的犹豫了。腾讯的方向是安全平台，要处理巨量的流量数据所以需要高性能高并发的支持，总的来说还算是系统方面的工作。大疆的岗位是个新部门，目标倒是跟我想做的比较契合，只是经历过阿里的面试之后，我觉得自己在这方面缺的东西不是一点半点，本来研究生阶段的工作很大程度上都已经是我们自己在一个新方向上摸着石头过河了，工作后再去到一个需要一边学一边慢慢建设的部门，我确实没有太大的信心了。</p><p>后来跟很多人商量了我自己的想法：在一开始工作的时候还是更想去一个成熟一点有足够积累的部门，我希望在离开学校的一两年间能尽可能地先提升自己。过来人都是劝我说没必要跟钱过不去（捂脸）……当然，薪资的吸引力还是很大啊，只是也许真的是我太年轻？</p><p>至少我到现在还是怀有这样的想法，也希望未来真的开始工作之后自己不会为当初做的这个选择而后悔吧。</p><p><img data-src="http://jcf94.com/download/2019-06-18-note-2018.jpeg" alt=""></p><h1 id="Apple-遇上西雅图"><a href="#Apple-遇上西雅图" class="headerlink" title="Apple 遇上西雅图"></a>Apple 遇上西雅图</h1><p>本以为已经结束了所有校招日程，可以老老实实忙手头上最后的一些事情准备毕业了，结果10月底收到的一封邮件又改变了我原来的计划。</p><p>Apple？做 iPhone 和 Mac 的那个 Apple？又是一件做梦都没有想到过的事情发生了。</p><p>很难形容当时刚看到邮件那一刻是什么样的心情，二十多年来我跟苹果唯一的交集就是用过一个 iPod Nano，结果突然有一天天上掉下来一扇门，打开以后就能去到地球的另一边，去到一个拥有全球最高市值的公司。</p><p>还有人会犹豫吗？</p><p>于是得到导师的同意之后开始了又一轮的面试，之后是等HC、背调、准备签证材料等等等等一堆麻烦的事情。</p><p><img data-src="http://jcf94.com/download/2019-06-18-note-visa.jpeg" alt=""></p><p>等到真正弄完所有的手续，等签证材料寄到已经是1月底了，能够约到的最近一次面签时间是1号的北京使馆。于是31号跑去上海FedEx的仓库人肉取了签证要用的表，连夜赶到北京。</p><p>然后，果然还是被 check 了。我记得这时候贸易战还没开打，但是也许计算机和 AI 相关的内容早就已经成了敏感词了吧，签证官根本就没问几个问题，要了个人简历扫了一眼之后直接就给了条子。</p><p>可是我原本的打算是2月开始实习的，这样可以赶在学校毕业答辩之前回来。从大使馆出来之后也想过要不就这样算了吧。</p><blockquote><p>现在想来如果当初没有继续坚持下去，也许就不会有再后来的这些麻烦事了吧。嗯…当然我人生中也会少掉一段珍贵的经历。</p><p>这都是后话了。</p></blockquote><p>接下来这一等就是2个月。</p><p>真的非常感谢我在 Apple 的主管 Jay（要说这也是一种缘分呐，我喜欢的第一偶像周董叫 Jay，之前也经常用这个作为自己的英文名），最初是他给了我这次实习的机会，在我签证被 check 之后还愿意帮我推迟实习的日期，并且在以后的日子中给了我很多帮助。</p><hr><p><img data-src="http://jcf94.com/download/2019-06-18-note-flight.jpeg" alt=""></p><p>4月6号，终于登上了飞往西雅图的航班。</p><p>更幸运的是，去 Cupertino 做入职培训时正好赶上了公司内部的一个学术交流会，让我有机会走进 Apple Park 以及 Steve Jobs Theater（应该是果粉圣地吧）！</p><p><img data-src="http://jcf94.com/download/2019-06-18-note-applepark.jpeg" alt=""></p><p>最惊讶的发现大概是原来国际巨头里面也会有重复造轮子这样的事情了，以前一直以为可能只有国内的公司才会有不同部门“很巧地”刚好都做了差不多的东西的情况……</p><p><img data-src="http://jcf94.com/download/2019-06-18-note-rain.jpeg" alt=""></p><p>西雅图从纬度上来看应该算很北方了，却觉得一点也不像国内北方的气候。也许是因为靠海？这边下雨很多。总体上对一个来自江南地区的人来说，这里还算挺舒服的。</p><p>神奇的是虽然说下雨多，但是即使雨天也很少看到有带伞的人，有时候走在路上发现只有我一个人打伞的时候就很尴尬。所以后来我也不带了。</p><p>在 Apple 每周都是 955 的生活，做的是自己很感兴趣的方向，每周大大小小的会也能听到学到很多新鲜的东西。这样的日子真的很赞啊。</p><p>Office 里的同事们都很友善。偶尔下午开完会之后，大家也会约着下楼，走过几条街，然后买杯咖啡带回来继续干活。</p><p>但是某天突然意识到从过道前走过的人中可能 CMU 的一抓一大把，坐在隔壁的小哥是 StandFord 毕业的高材生，前几天跟我聊过天的另外一个实习生小姑娘是 MIT 来的。于是会觉得应该要更脚踏实地地对待每一项任务，我差的还很远啊，我要学的东西还很多啊！</p><p><img data-src="http://jcf94.com/download/2019-06-18-note-seattle.jpeg" alt=""></p><p>一开始的日子现在想起来都觉得好笑。因为不熟悉美国超市的情况，想买双拖鞋都花了我一两个星期才找到……</p><p>然后一个人去了 Pike Market、Space Needle、附近的一些博物馆。某个周日去 Pioneer Square 参加他们的 Underground Tour 时导游都笑话我这么好的日子跑来听历史课真是可惜了。</p><p>收到的对我英语的最高评价是 Underground Tour 中一对老夫妇惊讶地问我：“Where did you pick up your English?” 额……惭愧惭愧，我觉得我听力是挺不错的，不过口语是真的不太行哈哈。</p><hr><p>主管说问我愿不愿意留下来做 Full-time？</p><p>以前是真的从没想过还有出国工作这个选项。但是如果能拿着美元的工资，做的还是自己最感兴趣的方向。Why not？</p><p>遗憾的是我过来实习用的 J-1 签证会有一个回国工作的限制，而豁免手续似乎也不是我当时的情况下能顺利办下来的。</p><p>更麻烦的是学校没有远程答辩的选项。由于最早的签证被 check，实习开始的晚了，一系列连锁因素导致中间还卡着一个关口必须回国参加毕业答辩。而我拿到的 J-1 签证只给了单次入境的机会，这意味着回国后需要重新再办一次签证，还存在没有办法回来做完后半程实习的危险。</p><p><img data-src="http://jcf94.com/download/2019-06-18-note-back.jpeg" alt=""></p><p>嗯……真的居然又被 check 了！🙄</p><p>大概是要凉了。</p><h1 id="也许是至暗时刻"><a href="#也许是至暗时刻" class="headerlink" title="也许是至暗时刻"></a>也许是至暗时刻</h1><blockquote class="blockquote-center">            <p>卡塞尔学院对你来说是一扇门，打开这扇门你就会进入新的世界，但那样你就再也回不去了……你每做出一个新的选择，其他选项就消失了。自始至终，你都只有一条路走。</p><p>——《龙族IV》</p>          </blockquote><p>对于一个毕业生来说，临走前夕最惨的事莫过于告诉你还不能走了。</p><p>没错，匆忙从大洋彼岸飞回来，然后签证被卡，毕业也遇到了麻烦。还能更惨一点吗？</p><blockquote><p>嗯，可以的，人一旦开始倒霉，好像所有不好的事情都会接踵而来。当然另外发生的一些事情也是后话了，不想说了。</p></blockquote><hr><p>得知答辩日期的时候已经只剩下不到一周时间了，然后赶紧买了回国的机票，预约新的面签，在路上修改大论文以及准备答辩的材料。</p><p>Anyway，中间的事情一言难尽，最后的结果就是按学校的流程，一旦错过了6月的毕业窗口就只能等11月才能拿到双证了。学院说一切按流程办事，而且当初请假出去实习时“发生任何事情后果自负”的字是我自己签了的，好吧，确实没毛病。</p><p>只能说这也是我自己的原因吧，一切都处理的太着急了，没有把该准备的东西都提前准备好。不作死就不会死，自己作死的苦果只有自己吃了。</p><hr><p>惊完之后的几天后开始后悔：</p><ol><li>还不如当初就干脆把毕业时间推到11月呢，这样至少还能做完实习再回来；</li><li>其实虽然没确定时间，但是大概在5月中旬需要回来这是早就知道的，如果早在5月初就提前回来准备好，是不是就不会遇到这种麻烦的情况了呢；</li><li>再或者如果当初签证被 check 的时候就放弃，老老实实在学校里面待着，也许也不会有后面这些坑了；</li><li>不，等签证材料从去年一直等到 1 月底这事本身就很不顺利了，也许早就该放手了？</li><li>……</li></ol><p>会想如果有时光机，那要回到哪一刻更好呢？回国前、5月初、或者更早到2月……不，这样一想过去后悔的事情多了去了，有太多“如果当时不这么做会怎么样”了。</p><p>世界上有好多事情是可以重来的，但更多的只会有一次机会。</p><p>时间就是这么公正。</p><hr><p>最后开始接受现实并且思考解决办法，然后理了理这段日子我收获了什么，付出的代价是不是真的有这么大。</p><blockquote><p>理性思维战胜了感性思维。</p><p>理工头脑：画个表格列一下？</p><p>不不不，那还是算了吧。</p></blockquote><p>一年前可能不会有这种感触：除了签证以外，出个国真的是很平常的一件事情。之前有时实验室的一些项目中也有要去别的省出差的时候，也会面对完全陌生的环境，会有很多新鲜的事物，感受到不一样的文化和氛围。可能区别就是路上时间会更久，当地文化的差异更大吧。</p><p>这 1 个月的海外实习经历对我即将起步的工作生涯来说是真的很珍贵了。体会到了大公司内部轻松而又非常有序的节奏，长了很多见识，也体验到了他们是怎么看待 AI 领域的未来的。</p><p>另外……至少从生产力工具层面，软粉表示真香！Mac 对程序员来说是真的很友好啊。Pad + Pencil 的体验感觉也要比 Surface 更好一点。</p><h1 id="Keep-Going！"><a href="#Keep-Going！" class="headerlink" title="Keep Going！"></a>Keep Going！</h1><p>几天前看到以前的一篇博文收到了评论：<a href="http://jcf94.com/2016/10/16/2016-10-16-october/">【October】</a>。还是有不少感慨的。</p><p>可能是性格原因吧，我常常会自己反思自己以前做过的各种事情。更重要的是，如果我写的东西能给到看到这些文字的人任何帮助或者鼓励的话，对我来说也会是很开心的事情。</p><p>一直觉得自己是个对自己没什么信心的人。以至于有时候发生了一些好事的时候，第一反应不是高兴，而是会迟疑自己是不是真的 afford（想了半天没想到怎么表达比较好，还是这个单词比较顺）？然后小心翼翼地珍惜。相反遇到麻烦的时候却能比较快地接受现实，可能习惯了对自己没有那么高的预期吧，然后能很快恢复过来：既然事情已经这样了，有空自怨自艾不如想想怎么解决，生活还是要继续啊。</p><p>做过很多未来规划，但是也经常被一些未知的因素打断。幸而这大半年来，大多数的“意外”都是向着更好的方向改变，嗯，最后这当头一棒挺重的，不过也还算受得了吧。</p><p>也想感谢一路上所有帮助过我和给我肯定的人们。</p><p>我不知道自己之前做的这些选择是不是都是对的，可能也没有人能说自己的评判是绝对正确的。唯有希望未来做出每个抉择的时候也都能有自己清晰的想法，不要让自己后悔！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;
            &lt;p&gt;生活总是不知道会在什么时候给你点“惊喜”。&lt;/p&gt;

          &lt;/blockquote&gt;

&lt;p&gt;过去的一年间发生了挺多事情的，对我自己来说有好事也有坏事，上上下下大起大落，最开始的计划被一次次地打乱，每一次都算是艰难的抉择吧。&lt;/p&gt;
&lt;p&gt;看了下博客也有半年没更了，更让我觉得惊讶的是上一次挂了“随笔” tag 的居然已经是2年前了（可见这两年读研生涯中我是多么正经…），准备来理一理过去这大半年的流水账。&lt;/p&gt;
    
    </summary>
    
    
      <category term="日常" scheme="https://jcf94.com/categories/%E6%97%A5%E5%B8%B8/"/>
    
    
      <category term="随笔" scheme="https://jcf94.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Release 了一个新的 VizGraph</title>
    <link href="https://jcf94.com/2019/01/01/2019-01-01-vizgraph/"/>
    <id>https://jcf94.com/2019/01/01/2019-01-01-vizgraph/</id>
    <published>2019-01-01T10:58:34.000Z</published>
    <updated>2019-01-01T12:03:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center">            <p>2019 第一篇！</p>          </blockquote><p>也是去年年底的时候心血来潮写的这个小东西：</p><ul><li><a href="/2018/01/18/2018-01-18-vizgraph/">【写了一个画框图的轮子】</a></li></ul><p>中间断断续续往里面补了一些功能，修修 bug 什么的，然后就很久没有再管过了。</p><p>虽说一开始想要的本来不是个能写 GraphViz 的文本编辑器，结果写到最后还是写成了个文本编辑器。</p><blockquote><p>话说 vscode 里面的 GraphViz 插件其实也挺不错的，</p><p>结果我自己平时还是觉得用这个更顺手哈哈</p></blockquote><a id="more"></a><hr><p>有天忘了在找什么的时候路过发现这个叫 <a href="https://csacademy.com/about/" target="_blank" rel="noopener">CS Academy</a> 的 OJ 上提供了一个简易的图编辑工具：</p><p><img data-src="http://jcf94.com/download/2019-01-01-vizgraph-grapheditor.png" alt=""></p><p>左边是文字形式记录的图关系，右边呢可以通过点击和拖动等等直观对图进行编辑，然后修改一边另外一边也会跟着同步更新。</p><p>刚看到的时候觉得惊了！这不就是我最早想要的玩意嘛！</p><p>然后果断 F12 开始研究起来。</p><hr><p>嗯，这个网站本身应该是用某种框架做的，所有的动作都被包含在一个大概 10 万行左右的超大 js 文件里面。虽然从这里面直接扒出来基本上应该是不太可能的，不过看了下几个位置的 click、鼠标拖动事件等等差不多也可以知道实现的原理了，又在 d3.js 里找到个类似的东西，照着改改很快也山寨了一个功能差不多的出来。</p><p>然后我就发现更纠结了。</p><p>最初的想法是通过右边的图形界面操作生成左边的中间结果，然后手动细调之后再进一步生成 GraphViz 的 dot 代码，最后通过 GraphViz 生成 svg 的图。</p><p>结果等到这个 Graph Editor 写出来之后，却发现这样瞎搞还不如从一开始就老老实实直接写 dot 代码来的顺畅。</p><p>这大概就是花了好大的精力实现了最初的想法之后发现自己走了歪路的感觉了。</p><hr><p>说起来这算是我第一个真正走到了实用阶段还一直在继续改的脑洞了，歪过一次之后现在决定这玩意还是老老实实就写成个编辑器吧。</p><p><img data-src="http://jcf94.com/download/2019-01-01-vizgraph-07.png" alt=""></p><p>目前是加了个多模版的启动页，然后用 PPT 花几分钟做了个图标。</p><blockquote><p>对！你没看错，PPT！</p></blockquote><p>一直没有真正好好学过前端相关的东西，所有这些都是有想法之后现场在网上找的教程和资料。</p><p>等以后有空了再改大概得想办法照顾一下这个界面的美观性（目前负分……），然后把逻辑部分的代码也好好重构一下。然后虽说现在把 Ace Editor 的自动补全功能打开了，但是这里面似乎不自带补全规则，之后还得看看怎么加。再然后还想把 GraphViz 语法的文档功能加进去，我自己经常用的时候还得开 GraphViz 的网站查有的东西怎么写，生产力啊生产力！</p><hr><p>Keep coding！</p><p>2019 大家继续加油呀！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;
            &lt;p&gt;2019 第一篇！&lt;/p&gt;

          &lt;/blockquote&gt;

&lt;p&gt;也是去年年底的时候心血来潮写的这个小东西：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/18/2018-01-18-vizgraph/&quot;&gt;【写了一个画框图的轮子】&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;中间断断续续往里面补了一些功能，修修 bug 什么的，然后就很久没有再管过了。&lt;/p&gt;
&lt;p&gt;虽说一开始想要的本来不是个能写 GraphViz 的文本编辑器，结果写到最后还是写成了个文本编辑器。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;话说 vscode 里面的 GraphViz 插件其实也挺不错的，&lt;/p&gt;
&lt;p&gt;结果我自己平时还是觉得用这个更顺手哈哈&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="Node.js" scheme="https://jcf94.com/tags/Node-js/"/>
    
      <category term="GraphViz" scheme="https://jcf94.com/tags/GraphViz/"/>
    
      <category term="VizGraph" scheme="https://jcf94.com/tags/VizGraph/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 拆包（十）：Allreduce</title>
    <link href="https://jcf94.com/2018/12/18/2018-12-18-tfunpacking10/"/>
    <id>https://jcf94.com/2018/12/18/2018-12-18-tfunpacking10/</id>
    <published>2018-12-18T08:31:44.000Z</published>
    <updated>2018-12-21T08:55:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>前篇：</p><ul><li><a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a></li><li><a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a></li><li><a href="/2018/02/28/2018-02-28-tfunpacking3/">TensorFlow 拆包（三）：Graph 和 Node</a></li><li><a href="/2018/03/07/2018-03-07-tfunpacking4/">TensorFlow 拆包（四）：Device</a></li><li><a href="/2018/03/09/2018-03-09-tfunpacking5/">TensorFlow 拆包（五）：Distributed</a></li><li><a href="/2018/03/12/2018-03-12-tfunpacking6/">TensorFlow 拆包（六）：RDMA</a></li><li><a href="/2018/04/10/2018-04-10-tfunpacking7/">TensorFlow 拆包（七）：Profiling 踩坑 &amp; Benchmark</a></li><li><a href="/2018/06/11/2018-06-11-tfunpacking8/">TensorFlow 拆包（八）：Dynamic Control Flow in Large-Scale Machine Learning</a></li><li><a href="/2018/10/21/2018-10-21-tfunpacking9/">TensorFlow 拆包（九）：High Level APIs</a></li></ul><p>不知不觉居然写了十篇了……写这个的初衷是觉得自己是个容易忘事的人，不找个地方做点笔记可能回过头就忘了自己看过什么了。</p><p>这篇要分析的是 TensorFlow 自带的 Allreduce 实现。</p><a id="more"></a><hr><h1 id="APIs-outside-of-tf-wrapper"><a href="#APIs-outside-of-tf-wrapper" class="headerlink" title="APIs outside of tf wrapper"></a>APIs outside of tf wrapper</h1><p>TensorFlow 中常规的使用操作是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><p>但是实际上 TensorFlow 目录下的 <code>__init__.py</code> 里面并没有把全部的内容都包含进去，另外的内容需要通过：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.xxx.xxx <span class="keyword">import</span> xxx</span><br></pre></td></tr></table></figure><p>这样的方式直接导入单独的包。</p><p>然后更奇怪的是有的 API 明明不在 <code>contrib</code> 中，按理说应该算是官方库中正式内容了，但是在官网的文档中却找不到。也可能是正要从 <code>contrib</code> 往官方库中移？</p><p>举例来说前面提过的 StagingArea 就是这样，然后这里的 Allreduce 操作的 API 也是，这就让人很好奇是不是还有别的什么不常用到的 API 可能在某些场景下会有奇效？</p><p>扯远了，</p><h1 id="collective-op"><a href="#collective-op" class="headerlink" title="collective op"></a>collective op</h1><p>首先是 <code>tensorflow.python.ops</code> 下的 <code>collective_ops.py</code> 这个文件，一共包含了 3 个 API：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_reduce</span><span class="params">(t, group_size, group_key, instance_key, merge_op, final_op,</span></span></span><br><span class="line"><span class="function"><span class="params">               subdiv_offsets=<span class="params">(<span class="number">0</span>,)</span>)</span></span></span><br><span class="line">  """Reduces tensors collectively, across devices.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    t: the tensor to be reduced.</span><br><span class="line">    group_size: the total number of tensors to be collectively reduced.</span><br><span class="line">      Each must reside on a different device.</span><br><span class="line">    group_key: an integer identifying the group of devices.</span><br><span class="line">    instance_key: an integer identifying the participating group of Ops.</span><br><span class="line">    merge_op: string naming the binary Op to be applied to compute each</span><br><span class="line">      partial reduction.</span><br><span class="line">    final_op: string naming the unary Op to be applied to each fully</span><br><span class="line">      reduced value.  Can be <span class="string">'Id'</span> <span class="keyword">for</span> no operation.</span><br><span class="line">    subdiv_offsets: a list of integer offsets into the tensor at which each</span><br><span class="line">      independent subdivision should begin.  Use [<span class="number">0</span>] <span class="keyword">if</span> no subdivision should</span><br><span class="line">      be done.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    An Op implementing the distributed reduction.</span><br><span class="line"></span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: <span class="keyword">if</span> any of the input parameter constraints are <span class="keyword">not</span> met.</span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def broadcast_send(t, shape, dtype, group_size, group_key, instance_key)</span></span><br><span class="line"><span class="string">  """</span>Broadcasts one tensor to a group of others, across devices.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    t: the tensor to be sent.</span><br><span class="line">    shape: the shape of the tensor being sent, which must agree <span class="keyword">with</span> t.</span><br><span class="line">    dtype: the type of the tensor being sent, which must agree <span class="keyword">with</span> t.</span><br><span class="line">    group_size: one plus the number of receiving tensors, i.e. the total</span><br><span class="line">      number of devices participating.  Each tensor must reside on a</span><br><span class="line">      different device.</span><br><span class="line">    group_key: an integer identifying the group of devices.</span><br><span class="line">    instance_key: an integer identifying the participating group of Ops.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    An Op implementing the distributed broadcast send.</span><br><span class="line"></span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: <span class="keyword">if</span> any of the input parameter constraints are <span class="keyword">not</span> met.</span><br><span class="line"></span><br><span class="line">  Note that the shape <span class="keyword">and</span> dtype arguments appear redundant since they</span><br><span class="line">  should be obtainable <span class="keyword">from</span> t.  The are two reasons <span class="keyword">for</span> including</span><br><span class="line">  them.  First, the shape <span class="keyword">and</span> type of tensors passed via broadcast must</span><br><span class="line">  be known ahead of time <span class="keyword">in</span> their most specific form so that the receive</span><br><span class="line">  side can allocate memory <span class="keyword">for</span> the operation <span class="keyword">and</span> shape/type inference can</span><br><span class="line">  carry forward <span class="keyword">from</span> there.  Including the same declarations on the</span><br><span class="line">  send side clarifies a commitment already made.  Secondly, having nearly</span><br><span class="line">  identical use syntax <span class="keyword">for</span> send <span class="keyword">and</span> receive sides may simplify tool-driven</span><br><span class="line">  generation of broadcast.</span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def broadcast_recv(shape, dtype, group_size, group_key, instance_key)</span></span><br><span class="line"><span class="string">  """</span>Receives a broadcasts tensor, across devices.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    shape: Shape of the tensor to be received.</span><br><span class="line">    dtype: Type of the tensor to be received.</span><br><span class="line">    group_size: one plus the number of receiving tensors, i.e. the total</span><br><span class="line">      number of devices participating.  Each tensor must reside on a</span><br><span class="line">      different device.</span><br><span class="line">    group_key: an integer identifying the group of devices.</span><br><span class="line">    instance_key: an integer identifying the participating group of Ops.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    An Op implementing the broadcast receive.</span><br><span class="line"></span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: <span class="keyword">if</span> any of the input parameter constraints are <span class="keyword">not</span> met.</span><br><span class="line">  <span class="string">"""</span></span><br></pre></td></tr></table></figure><p>基本的用法示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'cpu:0'</span>):</span><br><span class="line">    v0 = tf.Variable([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu:0'</span>):</span><br><span class="line">    v1 = tf.Variable([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu:1'</span>):</span><br><span class="line">    v2 = tf.Variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">sess = tf.Session(config=CONFIG)</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">print(sess.run([v0, v1, v2]))</span><br><span class="line"></span><br><span class="line">sum_reduce = []</span><br><span class="line"><span class="comment"># with tf.device('cpu:0'):</span></span><br><span class="line"><span class="comment">#     out.append(collective_ops.all_reduce(v0, 3, 1, 1, 'Add', 'Id'))</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu:0'</span>):</span><br><span class="line">    sum_reduce.append(collective_ops.all_reduce(v1, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="string">'Add'</span>, <span class="string">'Id'</span>))</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu:1'</span>):</span><br><span class="line">    sum_reduce.append(collective_ops.all_reduce(v2, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="string">'Add'</span>, <span class="string">'Id'</span>))</span><br><span class="line">print(sess.run(sum_reduce))</span><br><span class="line"></span><br><span class="line">average_reduce = []</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu:0'</span>):</span><br><span class="line">    average_reduce.append(collective_ops.all_reduce(v1, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="string">'Add'</span>, <span class="string">'Div'</span>))</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu:1'</span>):</span><br><span class="line">    average_reduce.append(collective_ops.all_reduce(v2, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="string">'Add'</span>, <span class="string">'Div'</span>))</span><br><span class="line">print(sess.run(average_reduce))</span><br><span class="line"></span><br><span class="line">print(sess.run([v0, v1, v2]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'=========================='</span>)</span><br><span class="line"></span><br><span class="line">bcast = []</span><br><span class="line"><span class="comment"># with tf.device('cpu:0'):</span></span><br><span class="line"><span class="comment">#     bcast.append(collective_ops.broadcast_send(v0, v0.shape, v0.dtype, 2, 3, 1))</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu:0'</span>):</span><br><span class="line">    bcast.append(collective_ops.broadcast_send(v0, v0.shape, v0.dtype, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu:1'</span>):</span><br><span class="line">    bcast.append(collective_ops.broadcast_recv(v0.shape, v0.dtype, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(sess.run(bcast))</span><br></pre></td></tr></table></figure><p>参数中的 t 是每个 device 上要进行 reduce 的本地 Tensor，group_size 是一共需要参与的 Tensor 数量，merge_op 和 final_op 分别是 reduce 聚合时和聚合结束之后要做的事情。</p><p>然后比较坑的是 group_key 和 instance_key 这两个参数，注释写的太不明确了（还是我理解能力有问题？）。instance_key 应该是用于标识完整的一次 reduce 操作，而 group_key 具体是怎么来的还不是很清楚。测试的时候整体换成另外的数字都是可以正常工作的。</p><p>另外 all_reduce 和 broadcast 都似乎不能在 GPU 和 CPU 间进行工作？选择两块 GPU 卡之前是没有问题的，而再加上一个 CPU 就报错了。</p><hr><p>其中的具体实现来自于 <code>gen_collective_ops</code> 这个包，那我们就知道这又是个用 C++ 写的然后封装到 python 下面用的操作了。</p><p>接口定义在 <code>tensorflow/core/ops/collective_ops.cc</code> 中，C++ 部分的实际实现在 <code>tensorflow/core/kernels/collective_ops.cc</code> 中，但是这里的 <code>CollectiveReduceOpKernel</code>、<code>CollectiveBcastSendOpKernel</code>、<code>CollectiveBcastRecvOpKernel</code> 三个类也主要还是用于设置输入输出的参数信息、检查结果等等，更进一步的实现要到 <code>tensorflow/core/common_runtime/base_collective_executor.cc</code> 中。</p><p>继续深挖下去之后，在 <code>collective_param_resolver_local.cc</code> 中看到了这样一段：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp-&gt;instance.impl_details.collective_name =</span><br><span class="line">    (cp-&gt;instance.type == BROADCAST_COLLECTIVE) ? <span class="string">"HierarchicalTreeBroadcast"</span></span><br><span class="line">                                                : <span class="string">"RingReduce"</span>;</span><br></pre></td></tr></table></figure><blockquote><p>……</p></blockquote><p>嗯，所以这里的实现又是注册、又是各种封装的，结果到最后只有两种算法。</p><p>目前这里的 all_reduce 只有环状通信的算法实现，broadcast 则只有二叉树广播方式的算法实现。</p><h2 id="group-key"><a href="#group-key" class="headerlink" title="group_key"></a>group_key</h2><p>要想搞清楚这个 group_key 是怎么回事，还得从 <code>tensorflow/core/kernels/collective_ops.cc</code> 这个接口开始。</p><p><code>CollectiveReduceOpKernel</code>、<code>CollectiveBcastSendOpKernel</code>、<code>CollectiveBcastRecvOpKernel</code>  三个类的 <code>ComputeAsync()</code> 方法的基本结构都差不多：</p><ul><li>准备输出用的 Tensor，由于 Reduce 操作本身带有输入，这里也会尝试是否可以重用输入的 Tensor</li><li>调用 <code>CanProceedWithCompute()</code> 方法检查各项参数，对 group_key 的检查也在这里完成</li><li>调用对应的实现算法完成计算</li></ul><p>单机环境下的 <code>CanProceedWithCompute()</code> 最后会调用 <code>CollectiveParamResolverLocal::CompleteGroupLocal()</code> ，group_key 在这里只是完全作为一个 map 的关键字来使用：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="function">mutex_lock <span class="title">l</span><span class="params">(group_mu_)</span></span>;</span><br><span class="line">  <span class="keyword">auto</span> it = group_table_.find(cp-&gt;group.group_key);</span><br><span class="line">  <span class="keyword">if</span> (it == group_table_.end()) &#123;</span><br><span class="line">    gr = <span class="keyword">new</span> GroupRec;</span><br><span class="line">    gr-&gt;group.group_key = cp-&gt;group.group_key;</span><br><span class="line">    gr-&gt;group.group_size = cp-&gt;group.group_size;</span><br><span class="line">    gr-&gt;group.device_type = cp-&gt;group.device_type;</span><br><span class="line">    group_table_[gr-&gt;group.group_key].reset(gr);</span><br><span class="line">    VLOG(<span class="number">2</span>) &lt;&lt; <span class="string">"New group_key="</span> &lt;&lt; gr-&gt;group.group_key</span><br><span class="line">            &lt;&lt; <span class="string">" group_size="</span> &lt;&lt; gr-&gt;group.group_size;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    gr = it-&gt;second.get();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>若某个 group_key 是第一次被使用，则与之关联的 GroupRec 会用当前创建该 op 的默认设备类型来作为整个 GroupRec 的设备类型。</p><p>当下一个 collective_op 创建时，再对目标的设备类型和根据 group_key 找出来的 GroupRec 作对比，不一致则报错，因此这里确实是限制了参与 reduce 或者 broadcast 的所有 op 都要是同一种设备类型的。</p><p>另外需要注意的是，同一个 group 中参与 all_reduce 和 broadcast 的 op 必须要和设备独立一一对应，所以也不可以在一块卡上同时发起 broadcast 的 send 和 recv，或者在同一块卡上的两个变量间进行 allreduce。</p><blockquote><p>这个设备限制我觉得还是比较奇怪的，从 CPU 向当前节点下的所有 GPU 设备广播我觉得是很常规的一种逻辑啊……</p></blockquote><p>instance_key 的作用也与 group_key 类似，InstanceRec 在这里主要是用来维护几个 mutex，主要负责多个 op 之间的同步，同时发生的多个 collective_op 操作只要 instance 不一样相互之间是不影响的。</p><hr><p>To be continued.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前篇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/13/2018-01-13-tfunpacking/&quot;&gt;TensorFlow 拆包（一）：Session.Run()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/23/2018-01-23-tfunpacking2/&quot;&gt;TensorFlow 拆包（二）：TF 的数据流模型实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/02/28/2018-02-28-tfunpacking3/&quot;&gt;TensorFlow 拆包（三）：Graph 和 Node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/07/2018-03-07-tfunpacking4/&quot;&gt;TensorFlow 拆包（四）：Device&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/09/2018-03-09-tfunpacking5/&quot;&gt;TensorFlow 拆包（五）：Distributed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/12/2018-03-12-tfunpacking6/&quot;&gt;TensorFlow 拆包（六）：RDMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/04/10/2018-04-10-tfunpacking7/&quot;&gt;TensorFlow 拆包（七）：Profiling 踩坑 &amp;amp; Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/06/11/2018-06-11-tfunpacking8/&quot;&gt;TensorFlow 拆包（八）：Dynamic Control Flow in Large-Scale Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/10/21/2018-10-21-tfunpacking9/&quot;&gt;TensorFlow 拆包（九）：High Level APIs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不知不觉居然写了十篇了……写这个的初衷是觉得自己是个容易忘事的人，不找个地方做点笔记可能回过头就忘了自己看过什么了。&lt;/p&gt;
&lt;p&gt;这篇要分析的是 TensorFlow 自带的 Allreduce 实现。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TensorFlow" scheme="https://jcf94.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Faster and Faster -- ImageNet</title>
    <link href="https://jcf94.com/2018/11/26/2018-11-26-fasterimagenet/"/>
    <id>https://jcf94.com/2018/11/26/2018-11-26-fasterimagenet/</id>
    <published>2018-11-26T11:51:50.000Z</published>
    <updated>2018-12-02T08:34:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>这个月十几号的时候索尼发了篇论文，224 秒跑完了 ImageNet……想想大概去年的这个时候还是 Facebook 和 UCBerkeley 在 ImageNet 的大规模方面你争我赶，不得不感叹时间过得真快。</p><p>去年毕业开题大概开的也是这个方向，然后前段时间面试的时候：</p><blockquote><p>面试官：你最近做了什么？</p><p>我：我 follow 了 FB 和 UCB 他们在 ImageNet 上做训练的工作，大概试了下他们论文里面的想法。</p><p>面试官：哦，那你最多用到多少卡？</p><p>我：e……最多跑到 16 块 V100。</p></blockquote><p>说完这句自己突然感到一阵尴尬，人家工作多少块卡我多少块卡？真好意思说复现……（捂脸）</p><p>然而我也没办法啊，实验室资源已经算不少了，我还想拿一块卡假装模拟两块卡用呢。0.0</p><p>Anyway，趁着正好要读大法的论文，把之前其他人的东西都理一理。</p><a id="more"></a><hr><p>前面这几段笔记还是<a href="/2017/12/20/2017-12-20-distributeddl/">去年的时候写的</a>，看了下过了这么长时间，有的已经正式发了论文了，有的是 arXiv 上有了新版，那就下新的重新看一遍吧。</p><p>还是留下 OneDrive 的链接：</p><ul><li><a href="https://1drv.ms/f/s!AuxK9QzqA6LQkMJrIjc5oZ6-xZxTyA" target="_blank" rel="noopener">【Large Scale Deep Learning】</a></li></ul><p>要说其他更多的材料的话，其实这几年在 ImageNet 上面冲分的有很多，Stanford 做过一个 DAWNBench，上面也记录了好多研究者跑 ImageNet 的最短时间、最小开销等等：</p><ul><li><a href="https://dawn.cs.stanford.edu/benchmark/index.html" target="_blank" rel="noopener">【DAWNBench - An End-to-End Deep Learning Benchmark and Competition】</a></li></ul><h1 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h1><h2 id="2017-Facebook-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-1-Hour"><a href="#2017-Facebook-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-1-Hour" class="headerlink" title="2017 Facebook - Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"></a>2017 Facebook - Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</h2><p>Facebook 做的大规模 GPU 集群训练，顺便推一波 Caffe2。</p><p>256 块 GPU，1 小时内跑完整个 ImageNet 的数据集训练，想想都可怕。</p><p>这篇文章主要是从一个“炼丹师”的角度来写的，主要关注在学习率、minibatch 等等之间的关系，以及它们对训练带来的影响。文章的主要工作是提出了一种根据 minibatch 大小调整学习率的扩放规则，以及一种在训练时预跑的机制。</p><p>这些可能大多都是训练中总结出来的一些 tricks 吧。</p><hr><p>随着网络模型和数据规模的不断增长，训练时间也跟着增长了，为了保证训练时间能够保持在一个有意义的限度之内，需要一些额外的工作。</p><p>文章的立足点是通过增大训练时的 batch size 来加快训练速度。例如 ResNet-50 这个网络通常大家采用的 batch 大小是 256，在 8 块 P100 上大约需要 29 个小时才能跑完整个 ImageNet 数据集（应该指的是原始的 ResNet 论文）。</p><p>但是如果加大 batch size 呢？<strong>改变 batch size 会影响到最终的收敛和训练结果的正确率</strong>。</p><p><img data-src="http://jcf94.com/download/2017-12-20-distributeddl-minibatch.png" alt=""></p><p>文章接下来的工作就是通过调学习率等等其他的一些方法，把加大 batch size 带来的影响控制在一个可接受的范围内，最终做到 8192 个 batch 训出来的结果跟原本的效果持平。当然最重要的，batch size 增大了 32 倍，总的训练时间也能够降下来，<strong>用 256 块 GPU 在 1 小时内训完了整个 ImageNet！</strong></p><hr><p>为了维持训练结果的准确率，他们在过往积累的大量经验上得出了一个令人难以置信地简单却又有效的规则：</p><ul><li>Linear Scaling Rule：minibatch 扩大 k 倍，学习率也扩大 k 倍。</li></ul><p>其他超参数维持不变就好。</p><p>多次测试之后，可以发现应用了上面这个规则时，不同 minibatch 的 SGD 不仅最终准确率非常接近，而且训练的收敛曲线都是非常相似的。</p><p>但是在两种情况下，前面讨论的一些假定会有问题：</p><ol><li><p>训练初始的几个 epoch 可能会引起网络参数巨变</p><p>在某些网络中，如果一开始设的学习率太大，初始的几个 epoch 会产生过大的梯度，上面这条规则就可能不太适用了。为了解决这个问题，文章于是提出了<strong>预热训练</strong>的阶段。</p></li><li><p>minibatch 的大小不能无限增长</p><p>就拿前面 ResNet-50 为例，这个规则只够保证 minibatch 在增长到 8192 的时候能维持训练效果，再大就保证不了了。</p></li></ol><p>接下来是训练预热的机制：</p><ul><li>Gradual warmup：预训练时先用一个比较小的学习率，然后爬坡增长，逐渐增加到目标需要的 k 倍</li></ul><p>经过实践调整之后，他们的实现里面是经过 5 个 epoch，学习率逐渐从$\eta$增长到$k\eta$。</p><hr><p>分布式 SGD 在具体实现上还有很多细节的地方，可能稍微变动一下就会影响到整个模型的超参数，这里提了几个注意点吧：</p><ol><li>权重衰减：交叉熵 loss 的扩放并不等价于学习率的扩放（？）</li><li>动量修正：如果用了像 Momentum SGD 这种方法，需要在改变学习率之后再应用动量修正</li><li>梯度聚合：用整个 minibatch 的大小 kn 来均一化每个 worker 的 loss，而不是每个 worker 负责的 n</li><li>数据清洗：先对训练数据做 shuffle 再把他们放到不同的 worker 上</li></ol><p>关于我最关心的梯度聚合的实现，他们没有采用 Parameter Server 的形式，而是所有 <strong>worker 直接做 allreduce</strong>。</p><p>Allreduce 分成 3 个阶段：</p><ol><li>每个节点的 8 块 GPU 的数据合并到同一个 buffer 上</li><li>所有节点对数据进行汇总做 allreduce</li><li>每个节点的梯度更新完之后再广播到各自的 8 块 GPU 上</li></ol><p>1、3 两个阶段的操作，如果数据量大于 256 KB 则用 NVIDIA 的 NCCL 库来完成，否则就简单地各自把数据传回 CPU 再做汇总。</p><p>阶段 2 用到了<strong>递归二分倍增算法</strong>以及<strong>令牌环算法</strong>……嗯，基本上也没有什么特别的，就是想办法高效实现 all_reduce 和 all_gather。</p><p>软件栈用的是 Facebook 自家开源的 <a href="https://github.com/facebookincubator/gloo" target="_blank" rel="noopener">Gloo</a>，具体的通信方面都是靠这个通信库来完成，就不涉及到具体的实现了（……）。以及 Caffe2。</p><p>硬件是 Facebook 的 Big Basin GPU 集群。每个节点是 <strong>8 块 NVLINK 的 P100（DGX-1），50 Gbit 的 InfiniBand</strong>。</p><p>预估一下 ResNet-50 的数据量：大约参数量是 100MB 左右，单 P100 跑完一次大约 120ms，也就是峰值带宽大概要在 100MB * 2 / 0.125s = 12.8 Gbit/s 的程度，加上一些额外的 overhead 应该不会超过 15 Gbit/s，对这套 IB 来说完全不是问题。</p><hr><p>后面的训练设置描述得很详细，包括很多超参数要怎么取值，卷积层和全连接层参数的初始化分别采用不同的方法等等。</p><p>单节点的 baseline 是 8 卡（k=8），每块卡的 BatchSize 是 32（n=32），相当于单机一轮的 BatchSize 是 256，90 个 epoch 之后得到了 23.6% 的错误率，表现可以说是非常不错了。</p><p>多机训练的时候上了 32 个节点，一共 256 块卡，总的 BatchSize 达到了 8k，相较单机扩大了 32 倍。如前面所描述的，单机训的时候学习率选在 0.1，这里则是从 0.1 开始经过 5 个 epoch 逐渐增长到 3.2，即原来的 32 倍。用了这套 Gradual warmup 的预热机制之后，最终网络收敛在了 23.74%，几乎与单机一致了，并且从测试结果中可以看到后期甚至是连 error 的下降曲线都是基本一致的。</p><p>效率方面，他们使用的这套 Allreduce 算法得到了非常好的效果，基本上已经可以看到线性的性能表现。</p><blockquote><p>做大规模训练的话，这篇论文基本上可以说是教科书级别的示范了。</p></blockquote><h2 id="2017-Extremely-Large-MinibatchSGD-Training-ResNet-50-on-ImageNet-in-15-Minutes"><a href="#2017-Extremely-Large-MinibatchSGD-Training-ResNet-50-on-ImageNet-in-15-Minutes" class="headerlink" title="2017 - Extremely Large MinibatchSGD Training ResNet-50 on ImageNet in 15 Minutes"></a>2017 - Extremely Large MinibatchSGD Training ResNet-50 on ImageNet in 15 Minutes</h2><p>上面 Facebook 用 256 块 P100 做了大 minibatch 的 ImageNet 训练之后，感觉军备竞赛就开始了。</p><p>这是日本的一个研究机构，用 1024 块 P100，更大的 BatchSize！！15分钟跑完了 ImageNet。</p><hr><p>完成这个任务主要有两方面的挑战：</p><ol><li>算法层面，使用更大的 BatchSize 之后，要想办法防止精度的损失；</li><li>系统层面，需要设计一套方案来有效地利用上可用的硬件和软件资源。</li></ol><p>这里给了个表，对比了一下目前用 ResNet 跑 ImageNet 的几个工作：</p><table><thead><tr><th>Team</th><th>Hardware</th><th>Software</th><th>Minibatch size</th><th>Time</th><th>Accuracy</th></tr></thead><tbody><tr><td>MSRA（ResNet作者）</td><td>Tesla P100 * 8</td><td>Caffe</td><td>256</td><td>29 h</td><td>75.3%</td></tr><tr><td>Facebook</td><td>Tesla P100 * 256</td><td>Caffe2</td><td>8192</td><td>1 h</td><td>76.3%</td></tr><tr><td><a href="https://blog.surf.nl/en/imagenet-1k-training-on-intel-xeon-phi-in-less-than-40-minutes/" target="_blank" rel="noopener">SURFsara</a></td><td>KNL 7250 * 720</td><td>Intel Caffe</td><td>11520</td><td>62 m</td><td>75.0%</td></tr><tr><td>UC Berkeley</td><td>Xeon 8160 * 1600</td><td>Intel Caffe</td><td>16000</td><td>31 m</td><td>75.3%</td></tr><tr><td>This work</td><td>Tesla P100 * 1024</td><td>Chainer</td><td>32768</td><td>15 m</td><td>74.9%</td></tr></tbody></table><p>嗯…话说，本来以为 UCB 最发的文章是在 KNL 上跑的，刚刚才发现他们当时最新的工作是用居然是 8160（这可是通用处理器啊？？？害怕，下面那篇真得好好看看了）</p><p>具体的实现，这里说是参照 Facebook 的工作做的，比较额外的就是加大了 BatchSize，然后软件方面换了他们自己写的 Chainer 框架，因此这块写的比较简略。可能还有个差别是这里用了 RMSprop 吧，另外也提供了他们所使用的 RMSprop 的 Warm-up 方案。</p><p>NCCL 和 OpenMPI 意味着他们写的这个框架是基于 MPI 做的通信，而且还做了卡间数据传输。主要还是用的同步模型，然后做 Allreduce。</p><p>训练用的单精，然后划重点，为了减少通信的数据量，数据传输的时候用的<strong><u>半精浮点</u></strong>！！测试之后发现对结果影响不大（666666….）。</p><p>硬件方面，双路 E5-2677，8 块 P100 的配置，应该是没有 NVLINK 的，IB 用的 FDR（56 Gbps）。</p><h2 id="2017-UC-Berkeley-2018-ICPP-ImageNet-Training-in-Minutes"><a href="#2017-UC-Berkeley-2018-ICPP-ImageNet-Training-in-Minutes" class="headerlink" title="2017 UC Berkeley - 2018 ICPP - ImageNet Training in Minutes"></a>2017 UC Berkeley - 2018 ICPP - ImageNet Training in Minutes</h2><blockquote><p>2018 年更新，这篇已经发在了 ICPP 上，也增加了很多内容，基本上已经是一篇新的文章了</p></blockquote><p>接下来上台的是 UC Berkeley。</p><p>最早看到他们 arXiv 上的论文版本写的是 24 分钟跑完，可能被同期其他同行的工作刺激了一下，后来更新把标题里面的 24 去掉了，最终实现里面确实也是把这个时间缩短到了 10 分钟的量级。</p><p>他们家应该是跟 Intel 合作，用的是 Intel Caffe 和 Intel 的计算设备。</p><p>17 年的论文中使用了两种硬件方案：</p><ol><li>Xeon Platinum 8160（Skylake 的 Xeon 通用 CPU）</li><li>Intel Xeon Phi 7250（KNL）</li></ol><p>18 年的正式版本还额外增加了 P100 的测试内容。</p><p>基本的结果是：2048 颗 8160 用 11 分钟跑完 100 个 epoch 的 AlexNet，准确率收敛在 58.6%；2048 块 KNL 用 20 分钟跑完 90 个 epoch 的 ResNet-50，然后 14 分钟跑到 64 个 epoch 的时候就已经收敛到 74.9% 了。</p><p>他们不出意外地也用了<strong>同步</strong>的更新策略，这里给的理由是同步可以保证整个计算过程的<strong>确定性</strong>，并行实现与串行实现的行为是一致的，这一点在设计和调试以及优化网络的时候尤其重要。有一些前人的工作也证明了异步的方式是不稳定的（例如 Revisiting Distributed Synchronous SGD 以及前面 Facebook 的工作等等）。</p><blockquote><p>异步策略会引入更多的影响因素，而且由于各个 worker 上的参数存在延迟，对整体的算法收敛也会有影响。话说前面几篇论文里面提出的优化方案也是，在异步训练的时候完全是不同的表现，不能直接适用过去。</p></blockquote><p>Introduction 最后点了一下这篇论文的一些要点：</p><ul><li>本文证明了大规模的 CPU 集群也能跑的很好，面对 GPU 并不是没有一战之力（Skylake 这一代的 Xeon 是真的强啊）。这篇文章应该也是当时使用 Intel 的硬件达到的最好结果。</li><li>本文使用的名为 LARS 的优化算法在 AlexNet 和 ResNet-50 上都表现得非常好，目前大部分效果比较好的新网络都是类似 ResNet 这种特征（言下之意 LARS 同样能在那些网络上跑的很好）。然后轻点一下同行 Facebook 的工作，说 LARS 比他们的要好！</li></ul><p>嗯……Related Work 里面把前面两篇文章都提了一下，还是很有趣的。</p><hr><p>第三章分析了一下大 Batch 训练的好处。理论上改变 BatchSize 是不会影响到一个 epoch 的总浮点计算量的，但是随着单个 step 的 BatchSize 的增长，一个 epoch 需要的迭代次数就减少了，整体通信的次数也减少了。</p><p>总之整体的运行时间可以减少，最理想情况当然是可以做到线性。表中给的总时间应该是以 GPU 数量取 log 来作为一次通信的 overhead 影响因子，因为这里考虑的是 Allreduce 同步的通信方式，log 级的估算基本上还是可以接受的。</p><blockquote><p>话说我觉得这个问题还得换个角度更深入考虑一下，一个 epoch 中通信的次数减少了，但是如果增加节点数就意味着单次通信的总数据量要增加，最后还是反映在带宽上。</p><p>另一方面 BatchSize 变大之后单个 step 的计算时间也增大了，说起来通信这部分的 overhead 可能也更容易被 overlap 在计算时间之后。这里还只是一些比较模糊的定性讨论，数据并行能不能达到线性增长还是得看通信带来的 overhead 有多大和实际跑的结果。</p></blockquote><p>另外一个原因是说大 Batch 可以提高 GPU 的利用率。</p><blockquote><p>我觉得本来跑满一块 GPU 应该是基本吧。</p><p>其实像前面 Facebook 的工作，单块 P100 能够支持的 ResNet-50 的 BatchSize 应该远不止 32，不知道他们最后训的时候 P100 的使用率大概是个什么情况。当然他们也有实际收敛效果方面的考虑，以他们当时的优化策略还并不能够保证 BatchSize 再大以后的收敛性能。</p></blockquote><p>在模型的选择上，他们提出了一个扩放率的概念，即计算和通信的比率，对比 AlexNet 和 ResNet-50 这两个网络：</p><table><thead><tr><th>Model</th><th>Communication<br># parameters</th><th>Computiation<br># flops per image</th><th>Comp/Comm<br>Scaling Ratio</th></tr></thead><tbody><tr><td>AlexNet</td><td>61 Million</td><td>1.5 Billion</td><td>24.6</td></tr><tr><td>ResNet-50</td><td>25 Million</td><td>7.7 Billion</td><td>308</td></tr></tbody></table><p>ResNet-50 的扩放率大概是 AlexNet 的 12.5 倍，所以 ResNet-50 的可扩放性更好，能达到更高的弱扩放效率。</p><p>但同时大 Batch 也会带来一定的问题，比如 BatchSize 不能无限往上加，大到一定程度之后，准确率就降下来了。所以后面的重点就在于怎么调整学习率这些超参数来保证增大 BatchSize 之后的训练效果（LARS 登场预告）。</p><p>基本的原则跟 Facebook 提出的观点一致：</p><ul><li>Linear Scaling：BatchSize 增大多少倍，学习率也要增大到相应的倍率；</li><li>Warmup Scheme：初始的学习率不能太大，要在开始的几个 epoch 里逐渐增长到目标的学习率为好。</li></ul><p>但是比 Facebook 的工作有了更深入的研究：</p><p>他们发现不同层参数和梯度的二范数比值（$\frac{||w||_2}{||\nabla w||_2}$）差距可能会很大，例如 AlexNet 中第一个卷积层和第一个全连接层的比值可能会差上千倍，猜测这个会成为影响大 BatchSize 训练精度的一个重要原因。</p><p>于是他们提出了 LARS（Layer-wise Adaptive Rate Scaling）算法来<strong>分别控制每一层</strong>的学习率。</p><blockquote><p>666666</p></blockquote><p>整体算法如下：</p><ol><li>分别计算每一个参与训练的参数的学习率：$\alpha = l * \frac{||w||_2}{||\nabla w||_2+\beta||\nabla w||_2}$，这里 $l$ 是一个缩放系数，$\beta$ 用于控制权重衰减；</li><li>分别计算每一层的学习率：$\eta = \gamma * \alpha$，$\gamma$ 是一个用户控制的超参数，按照线性缩放设置就好了；</li><li>对梯度应用权值衰减：$\nabla w = \nabla w + \beta w$；</li><li>最后的更新量：$a = \mu a + \eta\nabla w$，$\mu$ 是扰动量</li><li>更新参数：$w = w - a$</li></ol><hr><p>把一个算法扩大规模到更多的处理器上时，通常通信都会成为最大的 overhead。</p><p>所以这里从两个角度分析了通信比计算慢这一点，首先是硬件运算力（每个 flop 需要花费的时间，例如 P100 的峰值性能是 11TFlops 左右，$\frac{1}{11TFlops}$大约是$0.9*10^{-13}s$左右）会远小于理论传输速度（这里用的是$\frac{1}{Bandwidth}$，例如 56Gb/s 的 FDR，大约是$0.2*10^{-9}s$的水平），远小于延迟：</p><p>$$Time-Per-Flop &lt;&lt; \frac{1}{Bandwidth}&lt;&lt;Latency$$</p><p>去年的论文中还分析了在 45nm 工艺的 CMOS 处理器上，通信消耗的能量也会比计算更大：</p><table><thead><tr><th>Operation</th><th>Type</th><th>Energy(pJ)</th></tr></thead><tbody><tr><td>32-bit Int Add</td><td>Computation</td><td>0.1</td></tr><tr><td>32-bit Float Add</td><td>Computation</td><td>0.9</td></tr><tr><td>32-bit Int Multiply</td><td>Computation</td><td>3.1</td></tr><tr><td>32-bit Float Multiply</td><td>Computation</td><td>3.7</td></tr><tr><td>32-bit Register Access</td><td>Communication</td><td>1.0</td></tr><tr><td>32-bit SRAM Access</td><td>Communication</td><td>5.0</td></tr><tr><td>32-bit DRAM Access</td><td>Communication</td><td>640</td></tr></tbody></table><p>能量这个角度比较神奇，感觉应该体现的不是很明确，正式论文中已经删掉了这一段。</p><hr><p>设总的 epoch 数为 E，图片数为 n，BatchSize 为 B，网络参数为 |W|。</p><p>当 E 和 n 保持不变的时候，总的计算量是固定的，总的迭代次数是 $E* \frac{n}{B}$，通信量是$|W|*E*\frac{n}{B}$。当 BatchSize 增大之后，总迭代次数减少，那么通信次数也减少了，通信量减少。</p><blockquote><p>感觉这里再加一个 worker 数量等等这样的参数会更好一点。</p></blockquote><p>更大的 BatchSize 有利于增大计算通信比，因此更大的 BatchSize 有利于网络向大规模进行扩展。</p><p>后面的测试都得到了非常好的结果。</p><h2 id="2018-Highly-Scalable-Deep-Learning-Training-System-with-Mixed-Precision-Training-ImageNet-in-Four-Minutes"><a href="#2018-Highly-Scalable-Deep-Learning-Training-System-with-Mixed-Precision-Training-ImageNet-in-Four-Minutes" class="headerlink" title="2018 - Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes"></a>2018 - Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes</h2><p>这篇是腾讯和香港浸会大学合作的成果，话说 HKBU 的这几个作者之前还发过一个叫 DLBench 的各个深度学习框架性能测试的文章，没想到又看到了他们的名字。</p><p>这篇文章主要做了三件事：</p><ol><li>他们在训练的时候采用了一种混合精度的方法，增大吞吐量的同时不影响最终的收敛效果；</li><li>比前人的工作用了更大的 BatchSize，达到了 64k，保证最终收敛精度；</li><li>提出了一套高效的 Allreduce 算法。</li></ol><p>最好的成果是 2048 块 P40 在 6.6 分钟把 ResNet-50 跑到了 75.8% 的准确率，以及 4 分钟跑完 95 个 epoch 的 AlexNet。</p><p>他们搭了一套名叫 Jizhi 的训练系统（机智？）：</p><p><img data-src="http://jcf94.com/download/2018-11-26-fasterimagenet-jizhi.png" alt=""></p><ul><li>Input Pipeline 负责准备下一个 step 的训练数据，在上一个 step 的运算阶段准备完成以尽可能地减少 CPU 和 GPU 之间的数据传输延迟；</li><li>训练模块包括了模型的搭建和参数管理等等，他们对这部分增加了 fp16/fp32 混合精度训练以及 LARS 的支持；</li><li>通信模块包含了 Tensor 聚合和他们设计的混合 Allreduce 算法来提高整体的数据传输效率。</li></ul><hr><p>他们整体采用 fp16 来完成整个训练以提高整体运算的吞吐量，在额外应用 LARS 进行优化时遇到了半精范围溢出的问题。于是最终做了这么一件事：</p><ul><li>fp16 做完前向</li><li>把参数和梯度转换成 fp32 做 LARS</li><li>用梯度更新参数</li><li>把参数和梯度重新转换成 fp16</li><li>用 fp16 做完后向</li></ul><blockquote><p>这……</p></blockquote><p>使用这套方案之后，最终在 64k 的超大 BatchSize 下 ResNet-50 能够收敛到 76.2% 的准确率。</p><p>通信方面，首先是 Tensor 聚合，由于某些卷积层的参数量特别小，直接应用 Ring-Allreduce 等等算法去发送大量的小参数可能很难把带宽性能发挥出来，所以采用一个 buffer 池来收集完成前向运算的参数，当池中的参数量达到某个阈值时，再一次性发送出去。</p><blockquote><p>我倒是比较好奇这块是怎么在 TensorFlow 中实现的。</p></blockquote><p>根据参数量的大小，也分别采用不同的 Allreduce，并且在多级硬件框架中采用分级的 Allreduce 方法。</p><p>基本上就是这些。</p><h2 id="2018-Sony-ImageNet-ResNet-50-Training-in-224-Seconds"><a href="#2018-Sony-ImageNet-ResNet-50-Training-in-224-Seconds" class="headerlink" title="2018 Sony - ImageNet/ResNet-50 Training in 224 Seconds"></a>2018 Sony - ImageNet/ResNet-50 Training in 224 Seconds</h2><p>再是索尼的这篇文章。</p><p>他们的成果是 2176 块 V100 在 224 秒中达到 75.03% 的准确率，以及 1088 块 V100 下达到 91.62% 的扩展效率。</p><blockquote><p>为什么是这么奇怪的两个数字…</p></blockquote><p>基本的思路还是跟前面的工作大同小异，他们采用了动态调整 Batch Size 配合 LARS 来作为超参数的优化策略，并且同样配合使用了 fp16/fp32 混合精度，并且在通信方面采用了 2D 环网的 Ring-Allreduce 模式（日本的研究者真是喜欢在网络拓扑上做文章，之前他们那台超算 京 也是在网络方面有独特的设计）。</p><h2 id="2018-Second-order-Optimization-Method-for-Large-Mini-batch-Training-ResNet50-on-ImageNet-in-35-Epochs"><a href="#2018-Second-order-Optimization-Method-for-Large-Mini-batch-Training-ResNet50-on-ImageNet-in-35-Epochs" class="headerlink" title="2018 - Second-order Optimization Method for Large Mini-batch: Training ResNet50 on ImageNet in 35 Epochs"></a>2018 - Second-order Optimization Method for Large Mini-batch: Training ResNet50 on ImageNet in 35 Epochs</h2><p>NIPS 2017 上有一篇论文从数学层面分析过神经网络收敛方面的问题，得出的基本结论就是训练时间更久才能填上收敛的 gap（…嗯，虽然这个从经验上早就能得出来了）。前面的各位都是在考虑怎么把随机梯度下降的 BatchSize 提上去并且同时又把训练精度控制住，比如说各种花式处理学习率，花式改 BatchNorm，动态调整 BatchSize 等等。</p><p>这篇 11 月底的新工作则是换了个思路：大 Batch 带来的问题会不会就是 SGD 本身的限制，无论是 Momentum 还是 RMSprop 本质上都只是对 SGD 的优化修正算法，那如果干脆就不用 SGD 呢？</p><p>于是这里改用了 K-FAC（Kronecker-factored approximate curvature） 这个二阶的优化方法，配合 Allreduce 和半精来做分布式训练，测试结果是要比一阶的 SGD 收敛更快，仅仅 35 个 epoch 就可以把准确率收敛到 75%。同时，使用这种方法在 1024 块 V100 上也能够在 10 分钟就把 ResNet-50 收敛到 74.9%。更进一步的优化策略（针对 K-FAC）还能再减少计算量和内存的占用量。相关工作中也说这里更多的是对 ICLR 2017 上的一个 K-FAC 工作的改进。</p><p>K-FAC 是利用各层参数得到的费希尔信息矩阵（Fisher Information Matrix）计算一个系数，再乘上参数的梯度来进行更新。数学部分先跳过了，大致的过程跟 SGD 差不太多，只是 Kronecker 系数的计算涉及到了更多的东西，相对 SGD 来说，这部分计算是额外的 overhead。</p><p>软件方面，他们的工作是在 Chainer 上完成的，这个框架平时完全没接触过，但是前面 2017 年那篇日本的工作用的也是这个，猜测应该是作者是日本的研究者或者在日本这个框架更流行吧。</p><p>以 3 层网络为例，基本的流程如下：</p><p><img data-src="http://jcf94.com/download/2018-11-26-fasterimagenet-kfac.png" alt=""></p><p>Stage 1、2 分别是网络的前向和后向计算，之后的 3、4、5 是 Allreduce 计算 Kronecker 系数，最后 Stage 6 用每层计算出来的系数进行梯度更新。</p><p>其他诸如学习率的调整和预热的方法这里也有用。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Facebook 的工作是我看到的最早在大规模训练实现这块引路的了，之后 UC Berkeley 提出的优化方案又把整个效果往前推进了一步，其他人后来的文章都是集合了当时各种高效方案的实现，最近看到的索尼的那篇应该算是这个方向的集大成了。</p><p>总结下来，基本的参考点大概有几个：</p><ul><li><p>想办法增大计算吞吐量、减少通信的 overhead</p><ul><li>使用同步而不是异步</li><li>全系统采用 Allreduce 并且针对逻辑拓扑结构设计最适合的 Allreduce 策略</li><li>fp16/fp32 混合</li></ul></li><li><p>应用各种调超参的 trick 加速收敛</p><ul><li>SGD 算法相关的各种改进和调整</li><li>扩放学习率到 k 倍或者动态调整 BatchSize 到 k 倍</li><li>动态调整学习率，LARS 分层分别使用不同的学习率</li><li>其他一些加快收敛的方法例如改进 BatchNorm 等等</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个月十几号的时候索尼发了篇论文，224 秒跑完了 ImageNet……想想大概去年的这个时候还是 Facebook 和 UCBerkeley 在 ImageNet 的大规模方面你争我赶，不得不感叹时间过得真快。&lt;/p&gt;
&lt;p&gt;去年毕业开题大概开的也是这个方向，然后前段时间面试的时候：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;面试官：你最近做了什么？&lt;/p&gt;
&lt;p&gt;我：我 follow 了 FB 和 UCB 他们在 ImageNet 上做训练的工作，大概试了下他们论文里面的想法。&lt;/p&gt;
&lt;p&gt;面试官：哦，那你最多用到多少卡？&lt;/p&gt;
&lt;p&gt;我：e……最多跑到 16 块 V100。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;说完这句自己突然感到一阵尴尬，人家工作多少块卡我多少块卡？真好意思说复现……（捂脸）&lt;/p&gt;
&lt;p&gt;然而我也没办法啊，实验室资源已经算不少了，我还想拿一块卡假装模拟两块卡用呢。0.0&lt;/p&gt;
&lt;p&gt;Anyway，趁着正好要读大法的论文，把之前其他人的东西都理一理。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>TensorFlow 拆包（九）：High Level APIs</title>
    <link href="https://jcf94.com/2018/10/21/2018-10-21-tfunpacking9/"/>
    <id>https://jcf94.com/2018/10/21/2018-10-21-tfunpacking9/</id>
    <published>2018-10-21T02:26:53.000Z</published>
    <updated>2018-12-10T13:12:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>前篇：</p><ul><li><a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a></li><li><a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a></li><li><a href="/2018/02/28/2018-02-28-tfunpacking3/">TensorFlow 拆包（三）：Graph 和 Node</a></li><li><a href="/2018/03/07/2018-03-07-tfunpacking4/">TensorFlow 拆包（四）：Device</a></li><li><a href="/2018/03/09/2018-03-09-tfunpacking5/">TensorFlow 拆包（五）：Distributed</a></li><li><a href="/2018/03/12/2018-03-12-tfunpacking6/">TensorFlow 拆包（六）：RDMA</a></li><li><a href="/2018/04/10/2018-04-10-tfunpacking7/">TensorFlow 拆包（七）：Profiling 踩坑 &amp; Benchmark</a></li><li><a href="/2018/06/11/2018-06-11-tfunpacking8/">TensorFlow 拆包（八）：Dynamic Control Flow in Large-Scale Machine Learning</a></li></ul><p>这篇来研究一下 TF 中的一些高级 API。</p><p>TensorFlow 由于一直是在开源的社区环境中发展起来的，早期的一些 API 都比较简单粗暴（更直白地说就是<strong>不那么好用</strong>），以至于在它之上封装的更友好的 Keras 可能在大部分的使用者群体中会有更高的出现率。后来的 TensorFlow 中也有吸收 Keras 里面一些比较好的结构，有出现像 <code>tf.layers</code> 这样的更高层封装，可以期待一下 2.0 以后会不会大幅优化上层的编码 API 吧。</p><p>那这里说的高级 API 是什么呢？</p><a id="more"></a><p>官网的 guide 里面列了几个：</p><ul><li>Keras：一个神奇的包 <code>tf.keras</code>，官方提供的从 TensorFlow 本身向 Keras API 兼容的实现（感觉怪怪的，底层库中包含了一个对高层封装的兼容？？？）</li><li>Eager Execution：TensorFlow 的动态计算图实现，类似普通的 Numpy 或者 Pytorch 的执行模式</li><li>Importing Data：对输入数据的流水线封装 API</li><li>Estimator：用于把封装好的模型方便地扩展到多卡/多机的高级 API</li></ul><p>其他的还有包括像 StagingArea（构建软件流水，神器！！！）等等目前还在 <code>tf.contrib</code> 中处于实验阶段的很多东西，开源的力量太强大了，每隔一段时间就有很多新功能被社区添加进库中。</p><hr><h1 id="Estimator"><a href="#Estimator" class="headerlink" title="Estimator"></a>Estimator</h1><p>像 <code>tf.keras</code> 和 Estimator 的设计都是为了让用户能够更方便地编写网络，话说简单看了下 Estimator 的用法，API 的设计方式应该大概率是从 Keras 里面借鉴的。</p><p>具体的使用这里就不多记了，<a href="https://github.com/jcf94/my-dl-script/blob/master/mnist/dbg_mnist_estimator.py" target="_blank" rel="noopener">这里</a> 写了个很小的例子，直接开始拆 Estimator 的实现吧。</p><p>核心是 <code>tf.estimator.Estimator</code> 这个类，先看初始化参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    model_fn,</span><br><span class="line">    model_dir=<span class="literal">None</span>,</span><br><span class="line">    config=<span class="literal">None</span>,</span><br><span class="line">    params=<span class="literal">None</span>,</span><br><span class="line">    warm_start_from=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>第一个是用来构建网络的模型函数，具体后面详细分析；<code>model_dir</code> 用于指定保存的参数、checkpoint、log 信息等等存放的目录；再后面的几个都是一些额外的配置选项。</p><blockquote><p>让我觉得非常怪的是官网的介绍页面说 Estimator 的优势是不需要用户建图……我真是一脸懵逼。或许对于 TF 内置的一些事先建好的 Estimator 是这样吧，但是如果想自定义呢？……写文档的人吹的有点过了吧。</p></blockquote><p>创建 Estimator 时，首先初始化各项配置参数信息（值得一提的是 <code>model_dir</code> 是允许被 <code>config</code> 中的选项覆盖的），设置训练或者验证时用的<strong>数据分布策略（DistributionStrategy，后面再详细分析）</strong>，设置参数和图节点分布的 <code>device_fn</code>；之后简单检查 <code>model_fn</code> 的参数是否符合规范，然后完处理完 <code>warm_start</code> 的一些设置就结束了。</p><p>传入的 <code>model_fn</code> 是用于构建 Estimator 代表的模型网络的核心函数，它能够接受的参数名有严格的规定：</p><ul><li>features：网络的输入数据，即一个 batch 的数据；</li><li>labels：网络的标签数据，即一个 batch 的目标标签；</li><li>mode：可选，但是一般都必须要有，要不实现起来会很麻烦。这个值会根据执行的模式由 Estimator 传入，会有 3 种，<code>tf.estimator.ModeKeys.PREDICT</code>、<code>tf.estimator.ModeKeys.TRAIN</code> 和 <code>tf.estimator.ModeKeys.EVALUATE</code>；</li><li>params：可选，对应的是 Estimator 的初始化参数；</li><li>config：可选，对应的是 Estimator 的初始化参数</li></ul><h2 id="Run-the-Estimator"><a href="#Run-the-Estimator" class="headerlink" title="Run the Estimator"></a>Run the Estimator</h2><p>接下来是 Estimator 类的三个调用方法 evaluate、predict 和 train，从字面上就能够看出来各自对应的是什么功能了（Keras 里面对应的 API 应该是 evaluate、predict 和 fit）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">evaluate(</span><br><span class="line">    input_fn,</span><br><span class="line">    steps=<span class="literal">None</span>,</span><br><span class="line">    hooks=<span class="literal">None</span>,</span><br><span class="line">    checkpoint_path=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">predict(</span><br><span class="line">    input_fn,</span><br><span class="line">    predict_keys=<span class="literal">None</span>,</span><br><span class="line">    hooks=<span class="literal">None</span>,</span><br><span class="line">    checkpoint_path=<span class="literal">None</span>,</span><br><span class="line">    yield_single_examples=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train(</span><br><span class="line">    input_fn,</span><br><span class="line">    hooks=<span class="literal">None</span>,</span><br><span class="line">    steps=<span class="literal">None</span>,</span><br><span class="line">    max_steps=<span class="literal">None</span>,</span><br><span class="line">    saving_listeners=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>三个方法的共同参数是这个 <code>input_fn</code>，这是类似前面 <code>model_fn</code> 一样，也需要 Estimator 的创建者写好的输出数据产生函数。这个函数的返回值是一个二元组 <code>(features, labels)</code> 对应了 <code>model_fn</code> 的前两个输入参数。</p><p>train 中的 steps 表示从哪里开始训练，Estimator 将首先从保存的 checkpoint 中找到最接近的保存点，然后开始这次的训练，max_steps 则简单地就是训练的 batch 数了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_model</span><span class="params">(self, input_fn, hooks, saving_listeners)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> self._train_distribution:</span><br><span class="line">    <span class="keyword">return</span> self._train_model_distributed(input_fn, hooks, saving_listeners)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> self._train_model_default(input_fn, hooks, saving_listeners)</span><br></pre></td></tr></table></figure><p>如果在初始化时没有配置 <code>_train_distribution</code> 项，则会使用默认的方式来执行 train 操作，最终把 <code>model_fn</code> 也绑定出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">estimator_spec = self._call_model_fn(</span><br><span class="line">    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)</span><br></pre></td></tr></table></figure><p>向 <code>model_fn</code> 中传入输入数据以及 <code>ModeKeys.TRAIN</code>，接下来实际的执行函数是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_with_estimator_spec</span><span class="params">(self, estimator_spec, worker_hooks, hooks,</span></span></span><br><span class="line"><span class="function"><span class="params">                               global_step_tensor, saving_listeners)</span></span></span><br></pre></td></tr></table></figure><p>添加 TensorBoard 中的 Summary、创建参数保存点、如果有 <code>saving_listeners</code> 则额外添加到运行的 hooks 中，之后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> training.MonitoredTrainingSession(</span><br><span class="line">    master=self._config.master,</span><br><span class="line">    is_chief=self._config.is_chief,</span><br><span class="line">    checkpoint_dir=self._model_dir,</span><br><span class="line">    scaffold=estimator_spec.scaffold,</span><br><span class="line">    hooks=worker_hooks,</span><br><span class="line">    chief_only_hooks=(</span><br><span class="line">        tuple(chief_hooks) + tuple(estimator_spec.training_chief_hooks)),</span><br><span class="line">    save_checkpoint_secs=<span class="number">0</span>,  <span class="comment"># Saving is handled by a hook.</span></span><br><span class="line">    save_summaries_steps=self._config.save_summary_steps,</span><br><span class="line">    config=self._session_config,</span><br><span class="line">    log_step_count_steps=self._config.log_step_count_steps) <span class="keyword">as</span> mon_sess:</span><br><span class="line">  loss = <span class="literal">None</span></span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> mon_sess.should_stop():</span><br><span class="line">    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])</span><br></pre></td></tr></table></figure><p>嗯……这段代码是不是很熟悉，没错，官方建议的常规 TensorFlow 训练代码就是要写成这个格式。</p><p>至此，train 部分基本上分析完了（带 DistributionStrategy 的版本后面再说），整个过程就是把一套常规的 TensorFlow 代码的各个部分做了几级封装，要说有什么特别的就是它把 Summary 和 Saver 都默认包含在内了。</p><blockquote><p>如果按照这个格式解开成普通的 TensorFlow 代码的话，可以说是非常好的官方范例了。</p></blockquote><h2 id="EstimatorSpec"><a href="#EstimatorSpec" class="headerlink" title="EstimatorSpec"></a>EstimatorSpec</h2><p>然后再注意到 <code>model_fn</code> 的返回值，前面也提到了 evaluate、predict 和 train 这三个实际执行的方法其实最终都是把 <code>input_fn</code> 中产生的数据传给 <code>model_fn</code> 来跑，这里的控制差别就需要配合对不同的 mode 选项的分支判断来做，所以一个 <code>model_fn</code> 函数写出来大概是这个样子的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line"></span><br><span class="line">    xxxxxx</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mode == tf.estimator.ModeKeys.PREDICT):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mode == tf.estimator.ModeKeys.EVAL):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, loss=cross_entropy, eval_metric_ops=eval_metric_ops)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mode == tf.estimator.ModeKeys.TRAIN):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, loss=cross_entropy, train_op=train_step, eval_metric_ops=eval_metric_ops)</span><br></pre></td></tr></table></figure><p>不管是哪种模式，<code>model_fn</code> 最终的返回值都需要通过 <code>EstimatorSpec</code> 这个结构来传出去，其属性有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">__new__(</span><br><span class="line">    cls,</span><br><span class="line">    mode,</span><br><span class="line">    predictions=<span class="literal">None</span>,</span><br><span class="line">    loss=<span class="literal">None</span>,</span><br><span class="line">    train_op=<span class="literal">None</span>,</span><br><span class="line">    eval_metric_ops=<span class="literal">None</span>,</span><br><span class="line">    export_outputs=<span class="literal">None</span>,</span><br><span class="line">    training_chief_hooks=<span class="literal">None</span>,</span><br><span class="line">    training_hooks=<span class="literal">None</span>,</span><br><span class="line">    scaffold=<span class="literal">None</span>,</span><br><span class="line">    evaluation_hooks=<span class="literal">None</span>,</span><br><span class="line">    prediction_hooks=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>mode：对应三种不同的模式标识；</li><li>predictions：预测结果，要是一个 Tensor 或者 Tensor 组成的 dict；</li><li>loss：训练的损失函数值，必须是一个标量或者形状为 <code>[1]</code> 的 Tensor；</li><li>train_op：训练 step 的 op，一般是某个 Optimizer 的 <code>minimize()</code> 方法返回的那个；</li><li>eval_metric_ops：一个包含了验证结果的 dict，可以是 <code>Metric</code> 类，或者一个 <code>(metric_tensor, update_op)</code> 的元组；</li><li>其他…略了</li></ul><p>要求 <code>ModeKeys.TRAIN</code> 模式返回的必须包含 loss 和 train_op，<code>ModeKeys.EVAL</code> 模式返回的必须包含 <code>loss</code>，<code>ModeKeys.PREDICT</code> 模式返回的必须包含 predictions。</p><h1 id="DistributionStrategy"><a href="#DistributionStrategy" class="headerlink" title="DistributionStrategy"></a>DistributionStrategy</h1><p>前面说了，Estimator 这套 API 的出现是因为开发者希望能够方便用户快速搭网络，并且易于扩展到各种不同的计算结构上。</p><p>那么 Estimator 本体上面已经拆过了，没什么神秘的，就是个简单封装，距离实现单机到多卡/多机的这种扩展其实还差挺多的，而 DistributionStrategy 就是用来补上中间那个 Gap 的。</p><p>官方提供的资料是个 Github 上的 README：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute" target="_blank" rel="noopener">Distribution Strategy</a></p><p>里面给的使用的例子都非常简明易懂：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">distribution = tf.contrib.distribute.MirroredStrategy()</span><br><span class="line">config = tf.estimator.RunConfig(train_distribute=distribution)</span><br><span class="line"></span><br><span class="line">classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)</span><br><span class="line">classifier.train(input_fn=input_fn)</span><br><span class="line">classifier.evaluate(input_fn=input_fn)</span><br></pre></td></tr></table></figure><p>下面的三行是个普通的 Estimator 的使用，跟前面一样，唯一区别的就是在 <code>tf.estimator.RunConfig</code> 中创建一个 DistributionStrategy，然后作为 config 选项传递给 Estimator 即可。</p><p>确实很方便啊。</p><hr><p>如果有看过<a href="https://github.com/tensorflow/benchmarks" target="_blank" rel="noopener">官方的 benchmarks</a>中对多卡/多机的写法的话，可以发现那个写法大致上跟 DistributionStrategy 的设计非常像。</p><p><code>_train_model_distributed</code> 的大致结构是这个样子的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> self._train_distribution.scope():</span><br><span class="line">    ...</span><br><span class="line">    features, labels = estimator_util.parse_iterator_result(</span><br><span class="line">        iterator.get_next())</span><br><span class="line">    grouped_estimator_spec = self._train_distribution.call_for_each_tower(</span><br><span class="line">        self._call_model_fn,</span><br><span class="line">        features,</span><br><span class="line">        labels,  <span class="comment"># although this will be None it seems</span></span><br><span class="line">        model_fn_lib.ModeKeys.TRAIN,</span><br><span class="line">        self.config)</span><br><span class="line">    loss = self._train_distribution.unwrap(</span><br><span class="line">        self._train_distribution.reduce(</span><br><span class="line">            distribute_lib.get_loss_reduction(),</span><br><span class="line">            grouped_estimator_spec.loss,</span><br><span class="line">            destinations=<span class="string">'/device:CPU:0'</span>))[<span class="number">0</span>]</span><br><span class="line">    distributed_train_op = grouped_estimator_spec.train_op</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p><code>_train_distribution.scope</code> 中封装的是一些 <code>variable_scope</code> 和 <code>custom_getter</code>，用于在用<code>tf.get_variable()</code> 创建的变量之上再套上一层额外的封装控制。</p><blockquote><p>对普通的 <code>tf.get_variable()</code> 套上 <code>variable_scope</code> 之后可以控制这个变量创建的设备位置等等很多东西，第一次看到这种写法的时候还觉得这像是一种 hack 的方法，但是不用改变原本里面的代码，还是非常方便的。</p></blockquote><p>以单机多卡为例，<code>call_for_each_tower</code> 是在每块 GPU 卡上跑一遍 Estimator 中给定的 <code>model_fn</code>，即在每一块卡上独立创建一份数据并行的网络。基类里面这个函数是留空的，要求具体的实现类来完成这个部分。</p><p>话说这里我有个疑问……<code>model_fn</code> 中的正常写法应该是对训练模式返回一个包含了 <code>Optimizer.minimize()</code> 的 EstimatorSpec，但是多卡并行的过程中不是应该需要做梯度的聚合平均之后再更新到每个变量上吗？而且不同的并行模式下，这部分的处理方式应该也是不一样的，不知道这套 API 要怎么把这些全都统一起来。</p><p>看一下 MirroredStrategy 的这个实例里面是怎么实现这个函数的吧，中间部分的代码是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO(isaprykin): Create these threads once instead of during every run()</span></span><br><span class="line"><span class="comment"># call.</span></span><br><span class="line">threads = []</span><br><span class="line"><span class="keyword">for</span> index, d <span class="keyword">in</span> enumerate(distribution.worker_devices):</span><br><span class="line">  variable_creator_fn = shared_variable_creator.make_fn(</span><br><span class="line">      shared_variable_store, index)</span><br><span class="line">  t = MirroredStrategy._MirroredTowerThread(  <span class="comment"># pylint: disable=protected-access</span></span><br><span class="line">      distribution, coord, d, variable_creator_fn, fn,</span><br><span class="line">      *values.select_device(d, args), **values.select_device(d, kwargs))</span><br><span class="line">  threads.append(t)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">  t.start()</span><br></pre></td></tr></table></figure><p><code>distribution.worker_devices</code> 中保存了单机中的多块 GPU 卡设备，这里直接对每块卡挂一个 <code>shared_variable_creator</code> 并且开一些的一对一的线程去处理。</p><p><code>shared_variable_creator</code> 用于处理多卡之间的参数共享，在 device_id 为 0 的设备上调用 <code>get_variable()</code> 函数是创建新变量，并且保存到给定的 <code>shared_variable_store</code> dict 中；在 device_id 大于 0 的设备上调用 <code>get_variable()</code> 则会尝试共用前面创建好的变量。</p><p>接下来看一下创建线程这部分的逻辑。</p><p>主线程和多个子线程之间的控制这里用了 <code>should_run</code> 和 <code>has_paused</code> 两个 <code>threading.Event()</code> 来控制。开始的时候，每个线程都调用 <code>should_run.wait()</code> 来等待，等待主线程调用对应的 <code>should_run.set()</code> 来唤醒它们。主线程随后阻塞在 <code>has_paused.wait()</code>  上，等到每个线程完成自己那部分图的构建之后再用 <code>has_paused.set()</code> 唤醒。</p><blockquote><p>话说为啥一定要用多线程来实现这个部分呢……感觉就用普通的单线程循环一样可以做到这里想要的事情。</p></blockquote><p>那么对梯度的聚合和最终的 apply 呢？似乎这部分代码里面根本没看到啊，每个线程的 <code>run()</code> 函数基本上就是跑完各自的网络部分就没了。唯一看上去非常让人介意的是 <code>run()</code> 中在执行 <code>main_fn()</code> 前套了一堆 Python 的 Context，难道又是用有点 hack 的方法完成的？</p><p>其中 MirroredTowerContext 这个结构继承了 distribute_lib.TowerContext，只用于<code>call_for_each_tower</code> 中用于处理多块卡之间相同代表数据的同步。</p><p>问题是我还是没有看到处理 reduce 等等这些的代码。</p><p>然后……抱着一种怀疑的想法，我重新打开了 Optimizer 中关于梯度计算部分的代码！！发现这里已经跟当时拆包第二篇（<a href="/2018/01/23/2018-01-23-tfunpacking2/">Optimizer in TF</a>）里看到的不一样了。</p><p>例如新的 <code>apply_gradients</code> 中增加了这样的一段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> distribution_strategy_context.has_distribution_strategy():</span><br><span class="line">  grads_and_vars = get_filtered_grad_fn(<span class="keyword">lambda</span>: grads_and_vars)()</span><br><span class="line">  <span class="keyword">return</span> distribution_strategy_context.get_tower_context().merge_call(</span><br><span class="line">      self._distributed_apply, grads_and_vars, global_step, name)</span><br></pre></td></tr></table></figure><p>那么 DistributionStrategy 是如何配合 Estimator 把原本单机的代码直接扩展开来就很明白了。</p><blockquote><p>当时最早拆包开始时大概是 TensorFlow 的 1.6 版左右。</p><p>查了下 Optimizer 中增加部分的 git 记录，差不多是在今年 3 月底的时候加上的，应该是在 TensorFlow 的 1.7 版左右，然后后来又有过一次较大的改动。</p></blockquote><h2 id="Design-Philosophy"><a href="#Design-Philosophy" class="headerlink" title="Design Philosophy"></a>Design Philosophy</h2><p>再看一下<a href="https://tensorflow.google.cn/api_docs/python/tf/contrib/distribute/DistributionStrategy" target="_blank" rel="noopener">官方文档</a>中对 DistributionStrategy 的设计思想。</p><p>首先是一些底层的概念：</p><ul><li>Wrapped values：跨设备相关的变量可以被封装为两种类别，PerDevice 对象表示的变量在每个设备上的值不同，Mirrored 对象表示的变量在每个设备上的值都相同</li><li>Unwrapping and merging：考虑前面提过的这个函数 <code>call_for_each_tower(fn, w)</code>，fn 是模型函数，w 代表一些 Wrapped values。这个函数的调用过程中就包含了变量的 unwrapping 和 merging，假定在设备 <code>d0</code> 上 <code>fn(w0)</code> 得到的结果是 <code>(x, a, v0)</code>，在设备 <code>d1</code> 上 <code>fn(w1)</code> 得到的结果是 <code>(x, b, v1)</code>。首先在调用函数之前，w 需要被解包变成 w0 和 w1 然后分别调用 fn 函数。返回的结果有三种情况，第一个值都返回了一个相同的对象 <code>x</code>，则最终 merge 之后还是对象 x；第二个值是每个设备不一样的，则 merge 之后是一个 PerDevice 对象（其实就是个设备和对应值的 map）；第三个值是每个设备返回的分别是一组 Mirrored 对象的成员，则 merge 之后是一个 Mirrored 对象。所以 <code>call_for_each_tower(fn, w)</code> 在这里返回得到的就是一组 <code>(x, PerDevice{...}, Mirrored{...})</code></li><li>Tower context vs. Cross-tower context：Tower context 指的是对每个设备的封装上下文，通常对每个设备分别跑一遍模型函数就需要这种封装；Cross-tower context 指的是跨设备的封装上下文，比如说像 <code>reduce()</code> 这种所有设备共同参与的一个操作就需要这种封装</li><li>Worker devices vs. parameter devices：负责计算的设备和存参数的设备，没啥好说的。</li></ul><p>更新一个变量的常规操作如下：</p><ol><li>把输入数据集封装在 <code>d.distribute_dataset()</code> 中，然后创建一个 iterator</li><li>对每一个设备共同调用 <code>d.call_for_each_tower()</code> 来分别创建网络模型，并且最终各自得到一组梯度/变量对：<code>d0</code> 上有 <code>{(g0, v0), (g1, v1), ...}</code>，<code>d1</code> 上有 <code>{(g&#39;0, v0), (g&#39;1, v1), ...}</code> 等等这样</li><li>调用 <code>d.reduce(VariableAggregation.SUM, t, v)</code> 或者 <code>d.batch_reduce()</code> 来对梯度求和，并且对应到各自的变量上：<code>{(Sum(g0, g&#39;0), v0), (Sum(g1, g&#39;1), v1), ...}</code></li><li>调用 <code>d.update(v)</code> 来对每一个变量进行更新</li></ol><p>3、4 两步如果用 Optimizer 中的 <code>apply_gradients()</code> 方法可以自动完成（……这就是 Optimizer 后来加进去那部分代码的作用），或者在一个 Cross-tower context 中调用 <code>_distributed_apply()</code> 方法也可以。常规的网络层都应该在 Tower context 中被调用。</p><blockquote><p>话说这个 <code>_distributed_apply()</code> 为什么前面带下划线啊喂，这个方法本来不打算直接给人调的吧？？？大概是 API 还没最终设计好。</p></blockquote><p>嗯，所以 Estimator 本身一点都不神奇，真正这套机制麻烦的地方在 DistributionStrategy 里面，手写一个 DistributionStrategy 应该会是一件很麻烦的事情。</p><p>不知道未来这套机制会如何改进得更好用一些。</p><hr><blockquote><p>2018 12月更新</p><p>最近正在试图手动 DIY 一个 DistributionStrategy，发现到处都找不到相关的资料，官方的文档方面对这部分也是写的不明不白。</p><p>试了下自己继承一个 DistributionStrategy 类，但是发现这个基类的几乎所有功能都是交给另外一个 DistributionStrategyExtend 类来做的，而且直接从空类开始写缺的东西也有点太多了，还没下手成功。准备之后再试试直接继承一个现有的类比如 MirroredStrategy 然后重载掉里面的功能函数试试看。</p><p>Estimator 这套机制想要达到的目标是非常好的，但是似乎……由于 TensorFlow 本身过于庞大和复杂，不知道什么时候这两个东西才能真正成为方便用户使用的好接口。</p></blockquote><hr><h1 id="StagingArea"><a href="#StagingArea" class="headerlink" title="StagingArea"></a>StagingArea</h1><p>从名字上直译过来应该是用于暂存的区域，这套 API 用于跨 step 地把数据保存到网络 data path 之外的地方，然后可以在另外的 step 中把保存下来的数据取出来。</p><p>解释上看起来挺绕的，而实际上用这套 API 实现出来的效果就是——<strong>软件流水</strong>。</p><p>例如以下面这个由三个阶段组成的计算过程为例：</p><p><img data-src="http://jcf94.com/download/2018-10-21-tfunpacking9-staging1.svg" alt=""></p><p>在 a/b 和 b/c 之间分别加入 StagingArea 即：</p><p><img data-src="http://jcf94.com/download/2018-10-21-tfunpacking9-staging2.svg" alt=""></p><p>更重要的是加入了暂存结构之后，事实上 a、b、c 三个计算阶段的依赖就被解耦了：</p><p><img data-src="http://jcf94.com/download/2018-10-21-tfunpacking9-staging3.svg" alt=""></p><p>对执行的流程稍微进行一些修改：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">step1: A1</span><br><span class="line">step2: A2 B1</span><br><span class="line">step3: A3 B2 C1</span><br><span class="line">step4: A4 B3 C2</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>原本必须按顺序执行的三个计算阶段现在就可以互不相关地并行执行了，在某些计算与 I/O、数据通信共同存在的环境中，原本可能存在的数据延迟、等待等等就有可能通过流水线的方式隐藏掉！（例如多机分布式训练的情况，实测效果非常好）</p><p>关于这套 API 如何使用的介绍这里就不多记了，直接来看 TensorFlow 是怎么实现它的。</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>StagingArea 这个类在 <code>tensorflow/python/ops/data_flow_ops.py</code> 中，很早以前应该是在 <code>tf.contrib</code> 里面的，大概试验成熟之后移到正式的包部分了。</p><p><code>put()</code> 和 <code>get()</code> 这两个方法的实现分别调用了 <code>gen_data_flow_ops.stage()</code> 和 <code>gen_data_flow_ops.unstage()</code>，然后你会发现虽然一开始是从 <code>from tensorflow.python.ops import gen_data_flow_ops</code> 中引入了这个包，但是源代码里面是找不到这个包的。</p><p>原因在于这里面的东西都是在 C++ 层代码中定义然后在编译过程中生成的，追到 <code>tensorflow/core/ops/data_flow_ops.cc</code> 中可以看到大量用 <code>REGISTER_OP</code> 宏注册的 op，其中就有 StagingArea 用到的 <code>stage()</code>、<code>unstage()</code> 等等函数。</p><p>当然到这里为止还是没办法找到它的实现，因为 <code>REGISTER_OP</code> 宏只是负责 Python 与 C++ 的接口部分的处理，具体 C++ 层调用的实际内容还要再往 Kernel 里面找：<code>tensorflow/core/kernels/stage_op.cc</code> 。这里才是真正最底层的实现内容了，然后还能看到很多 <code>REGISTER_KERNEL_BUILDER</code> 宏，用于把 C++ 部分编译成的库与上面的接口绑定起来。</p><blockquote><p>在<a href="/2018/02/28/2018-02-28-tfunpacking3/">拆包第三篇</a>简单记过 TensorFlow 中 Op 的创建方式，嗯，在这里用上了。</p></blockquote><hr><p>然后就发现这玩意的实现就是个双向队列的封装，没啥神奇的……╮(╯_╰)╭</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">deque</span>&lt;Tuple&gt; buf_;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Get</span><span class="params">(Tuple* tuple)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    *tuple = <span class="built_in">std</span>::move(buf_.front());</span><br><span class="line">    buf_.pop_front();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Status <span class="title">Put</span><span class="params">(Tuple* tuple)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    buf_.push_back(<span class="built_in">std</span>::move(*tuple));</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前篇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/13/2018-01-13-tfunpacking/&quot;&gt;TensorFlow 拆包（一）：Session.Run()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/23/2018-01-23-tfunpacking2/&quot;&gt;TensorFlow 拆包（二）：TF 的数据流模型实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/02/28/2018-02-28-tfunpacking3/&quot;&gt;TensorFlow 拆包（三）：Graph 和 Node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/07/2018-03-07-tfunpacking4/&quot;&gt;TensorFlow 拆包（四）：Device&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/09/2018-03-09-tfunpacking5/&quot;&gt;TensorFlow 拆包（五）：Distributed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/12/2018-03-12-tfunpacking6/&quot;&gt;TensorFlow 拆包（六）：RDMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/04/10/2018-04-10-tfunpacking7/&quot;&gt;TensorFlow 拆包（七）：Profiling 踩坑 &amp;amp; Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/06/11/2018-06-11-tfunpacking8/&quot;&gt;TensorFlow 拆包（八）：Dynamic Control Flow in Large-Scale Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇来研究一下 TF 中的一些高级 API。&lt;/p&gt;
&lt;p&gt;TensorFlow 由于一直是在开源的社区环境中发展起来的，早期的一些 API 都比较简单粗暴（更直白地说就是&lt;strong&gt;不那么好用&lt;/strong&gt;），以至于在它之上封装的更友好的 Keras 可能在大部分的使用者群体中会有更高的出现率。后来的 TensorFlow 中也有吸收 Keras 里面一些比较好的结构，有出现像 &lt;code&gt;tf.layers&lt;/code&gt; 这样的更高层封装，可以期待一下 2.0 以后会不会大幅优化上层的编码 API 吧。&lt;/p&gt;
&lt;p&gt;那这里说的高级 API 是什么呢？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TensorFlow" scheme="https://jcf94.com/tags/TensorFlow/"/>
    
      <category term="Estimator" scheme="https://jcf94.com/tags/Estimator/"/>
    
  </entry>
  
  <entry>
    <title>CSE 599W： Systems for ML</title>
    <link href="https://jcf94.com/2018/10/04/2018-10-04-cse559w/"/>
    <id>https://jcf94.com/2018/10/04/2018-10-04-cse559w/</id>
    <published>2018-10-04T03:02:54.000Z</published>
    <updated>2018-10-09T11:00:43.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>国庆在家闲不住想干活系列……</p></blockquote><p>本篇的内容是陈天奇大佬今年春季<a href="http://dlsys.cs.washington.edu/" target="_blank" rel="noopener">在华盛顿大学开的一门课</a>。</p><p>大佬是上交 ACM 班的本硕，虽然目前还在 UW 读博中，但是在机器学习圈子里面已经有了很高的名望了，他的 MXNet 和 XGBoost 相信很多人就算没用过也肯定听说过（比如我就没用过…）。前段时间他发布的 TVM 也算是开启了深度学习和系统方面探索的一条新道路。</p><p>课程介绍上讲的是这门课的目标是填上深度学习算法和系统的实现以及优化之间的 gap，粗略地翻了一下 PPT，后面也有比较多的篇幅是介绍 TVM 的，正是我想了解的！</p><p>没找到课程的视频，但是 PPT 可以在上面的课程链接或者<a href="https://1drv.ms/f/s!AuxK9QzqA6LQlI8SyU3aTAKCrZJbHQ" target="_blank" rel="noopener">这里</a>找到。</p><a id="more"></a><p>下面的内容主要按照每篇 PPT 的整理：</p><hr><h1 id="Lecture-1-Introduction-to-Deep-Learning"><a href="#Lecture-1-Introduction-to-Deep-Learning" class="headerlink" title="Lecture 1: Introduction to Deep Learning"></a>Lecture 1: Introduction to Deep Learning</h1><p>回顾了一下基本原理和发展历史。</p><p>机器学习的过程基本上就是 数学模型+评价指标+参数训练，深度学习则是模型特指各种神经网络。</p><p>具体主要涉及到各种不同的模型架构（CNN、RNN、各种变种），目标函数的选择和训练技巧，正则化初始化等等。</p><p>这些就不多记了。</p><p>Lecture 2 是个实验课，实践怎么搭网络。</p><h1 id="Lecture-3-Overview-of-Deep-Learning-System"><a href="#Lecture-3-Overview-of-Deep-Learning-System" class="headerlink" title="Lecture 3: Overview of Deep Learning System"></a>Lecture 3: Overview of Deep Learning System</h1><p>这一节差不多是大纲的性质，每一个小点后面都会分节细讲。</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-arch.png" alt="Basic Architecture"></p><p>基本上所有的深度学习框架都是差不多这个结构，首先来看 User API 层：</p><p>这里举了个线性回归的例子来对比手写 Numpy 和框架代码的差别。基本上网络模型都可以比较方便地用一个计算图的结构来表达，节点表示运算操作，边代表数据依赖。</p><p>那为了方便用户写代码，一个框架也是一定要有自动求导的功能的（如果反向计算还需要手写那就太瓜皮了）。</p><p>然后是 System Components 层：</p><p>这里涉及到了首先是计算图的优化，比如一次运行的时候直接过滤掉用不到的图节点（Deadcode Elimination），内存分配方面的优化，图节点和实际计算设备的对应等等。</p><p>实际跑图的时候如果有多个设备或者多个工作线程，如何调度以及发挥出计算设备的并行性也是一个需要考虑的问题。</p><p>最下面的 Architecture 层：</p><p>目前用来支持 DL 的设备也有很多，典型的如 GPU，其他的加速芯片也是越来越多，不同的设备可能要写对应不同的代码，这部分要怎么优化？</p><p>现在最常规的做法是每一种不同的计算设备会有开发厂商自己提供支持库，但是这个对框架的开发者来说还是有一个要整合的过程。另外，如果系统中存在多种不同的计算设备，计算任务在多设备上要怎么分配和调度也会是一个很大的麻烦。</p><p>为了解决最后的这个问题，目前有一种 Compiler Based Approach，即整个 Architecture 层由一个 High Level Operator Description 加上 Tensor Compiler Stack 来统一解决。这就是之后要提到的 TVM 的设计思路了。</p><h1 id="Lecture-4-Backpropagation-and-Automatic-Differentiation"><a href="#Lecture-4-Backpropagation-and-Automatic-Differentiation" class="headerlink" title="Lecture 4: Backpropagation and Automatic Differentiation"></a>Lecture 4: Backpropagation and Automatic Differentiation</h1><p>详细解释第三节中的自动求导。</p><p>计算机中实现求导这个操作主要有两种方式：基于符号的算术求导和直接用数值进行求导。</p><p>算术求导需要构建一棵符号表示树，然后根据各种算术上的求导规则来写公式。缺点在于：如果遇到特别复杂的函数，则需要推导的符号表示树也会很大；然后如果目标只是想要一个导数值，则保存一棵符号表示树就很浪费了；再然后就是这样做容易有误差（？为什么…按公式算不应该误差更小吗）。</p><p>数值求导则是按导数的定义做，直接对方程取极限：</p><p>$$ \frac{\partial f(x)}{\partial x_i} \approx \lim \limits_{h \to 0} \frac{f(x+he_i)-f(x)}{h}$$</p><p>实现起来特别简单，h 取个 1e-6 就差不多了，但是一般只用来检验求导结果用。</p><p>然后对于网络中每一层的反向部分，其实求导涉及到的都只是跟本层运算相关的内容：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-bp.png" alt="Backpropagation"></p><p>上一层传下来的是 $\frac{\partial Error}{\partial z}$，再往下可以通过链式求导法则一直推导下去，而其他需要的则只是与本层运算有关的$\frac{\partial z}{\partial x}$ 和 $\frac{\partial z}{\partial y}$。</p><p>更详细的推导可见<a href="/2018/01/23/2018-01-23-tfunpacking2/#Automatic-differentiation">这里</a>。</p><p>因此自动求导则是根据以上的规则来创建反向计算图的过程，伪代码以及结果如下：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-autodif1.png" alt="AutoDiff"></p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-autodif2.png" alt="BP &amp; AutoDiff"></p><p>自动求导构建完成反向计算图之后，完整的计算图可以接下来一起用作整体的图优化。</p><h1 id="Lecture-5-GPU-Programming"><a href="#Lecture-5-GPU-Programming" class="headerlink" title="Lecture 5: GPU Programming"></a>Lecture 5: GPU Programming</h1><p>在 CPU 上进行数据运算大致有几个过程（按多级流水分）：Fetch、Decode、ALU Compute、Write Back。由于 CPU 本来也并不是为了纯运算而设计，因而在 ALU Compute 以外的其他部分会有比较大的计算资源和能耗上的 overhead。</p><p>后来增加的向量化指令能够相当程度地改善这种 overhead 的问题，而 GPU 从这个角度来看更像是一种把 ALU 的向量化做的更极致的加速器。</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-cpugpu.png" alt="CPU vs GPU"></p><p>从存储的层次结构上来对比：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-memory.png" alt="Memory Hierarchy"></p><p>GPU 的大量寄存器就使得它能够以比 CPU 小的多的开销来切换线程，这也就能够支撑起大规模的 SIMT 了。</p><p>后面是一些 CUDA 编程的例子，以及如何根据 GPU 的微架构特性高效地发挥出性能来。</p><h1 id="Lecture-6-Optimize-for-Hardware-Backends"><a href="#Lecture-6-Optimize-for-Hardware-Backends" class="headerlink" title="Lecture 6: Optimize for Hardware Backends"></a>Lecture 6: Optimize for Hardware Backends</h1><p>这一节的内容大概在 System Components 和 Architecture 层之间，一份代码面对不同规模的数据（甚至是不同数据块尺寸的数据）往往不作针对性地调整是达不到最佳性能的。</p><p>深入下去需要实际考虑到例如 CPU 的 Cache、GPU 的寄存器等等这些方面，以及 GPU 的多级存储之间的数据搬移开销、数据重用等等，同样是 GPU 也有多种不同的后端。</p><p>不同的 Tiling Patterns、Fuse Patterns、Data Layout、Hardware Backend 合起来使得优化工作也变得相当复杂了。</p><p>为了解决前面说到的所有这些麻烦的问题，然后这里就引出了 TVM Stack。</p><h1 id="Lecture-7-Automatic-Code-Generation-TVM-Stack"><a href="#Lecture-7-Automatic-Code-Generation-TVM-Stack" class="headerlink" title="Lecture 7: Automatic Code Generation - TVM Stack"></a>Lecture 7: Automatic Code Generation - TVM Stack</h1><p>各种不同的<strong>框架</strong>和实际执行运算的各种各样的<strong>硬件后端</strong>之间其实存在着很大的 gap。</p><p>如果从编译器的视角来看待如何解决这个问题，各种框架写的网络可以根据特定的规则转化成某种统一的表示形式，在统一表示的基础上进行一些可重用的图优化，之后再用不同的后端来生成对应不同设备的代码，这就是目前各家都在尝试的设计思路了。</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-ir.png" alt="Computational Graph as IR"></p><p>举例说：TensorFlow 的 XLA 会把高级代码抽象成 XLA HLO 的表示，做目标无关优化之后再用对应后端来生成更深一层的代码。</p><p>NVIDIA 的 TensorRT 的优化策略也是在图转化之后的统一表示上做，例如根据设定好的规则来做一些相邻计算单元的合并（Kernel Fusion）等等。</p><p>当然这种方式实现的时候会遇到一些同样非常麻烦的问题，一个 operator 需要针对不同的硬件平台、数据格式、精度、线程结构写一堆代码生成规则和优化规则。</p><blockquote><p>到头来是把原本 op 实现的复杂度变成了编译规则的复杂度，绕了个圈以后好像还是很麻烦啊。</p></blockquote><p>TVM 借助了一种叫 Tensor Expression Language 的表示方法，同样采用这种类似表示的还有 Halide（一种图像处理语言）、Loopy（基于 Python 的 kernel 生成器）、TACO（稀疏 Tensor 代码生成器）、Tensor Comprehension（类似 TVM）等等。</p><p>这种表示法最初的想法来源于 Halide，核心在于<strong>把代码的计算和调度分开</strong>。</p><p>例如一段最原始的 TVM 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C = tvm.compute((n,), <span class="keyword">lambda</span> i: A[i] + B[i])</span><br><span class="line">s = tvm.create_schedule(C.op)</span><br></pre></td></tr></table></figure><p>生成得到的 C 代码是：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i)</span><br><span class="line">&#123;</span><br><span class="line">    C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>加上额外的调度控制：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C = tvm.compute((n,), <span class="keyword">lambda</span> i: A[i] + B[i])</span><br><span class="line">s = tvm.create_schedule(C.op)</span><br><span class="line">xo, xi = s[C].split(s[C].axis[<span class="number">0</span>], factor=<span class="number">32</span>)</span><br></pre></td></tr></table></figure><p>再生成的代码就变成了：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> xo = <span class="number">0</span>; xo &lt; <span class="built_in">ceil</span>(n / <span class="number">32</span>); ++xo)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> xi = <span class="number">0</span>; xi &lt; <span class="number">32</span>; ++xi)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> i = xo * <span class="number">32</span> + xi;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; n)</span><br><span class="line">        &#123;</span><br><span class="line">            C[i] = A[i] + B[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>甚至于还可以支持绑定中间的 xo 和 xi 到特定的变量上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">C = tvm.compute((n,), <span class="keyword">lambda</span> i: A[i] + B[i])</span><br><span class="line">s = tvm.create_schedule(C.op)</span><br><span class="line">xo, xi = s[C].split(s[C].axis[<span class="number">0</span>], factor=<span class="number">32</span>)</span><br><span class="line">s[C].recorder(xi, xo)</span><br><span class="line">s[C].bind(xo, tvm.thread_axis(“blockIdx.x”)</span><br><span class="line">s[C].bind(xi, tvm.thread_axis(“threadIdx.x”)</span><br></pre></td></tr></table></figure><p>话说这样出来的代码就可以用在 CUDA kernel 里面了：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = threadIdx.x * <span class="number">32</span> + blockIdx.x;</span><br><span class="line"><span class="keyword">if</span> (i &lt; n)</span><br><span class="line">&#123;</span><br><span class="line">    C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体后续的调度部分的设计，首先需要保证生成的代码在逻辑上要能跑出正确的结果，常见的手工优化代码的方法也都要包含在内，并且要能够方便引入其他额外的新技术。</p><p>目前 TVM 的调度部分还在继续开发中，已经从像 Halide、Loopy 这种成熟的语言中吸取过来的方法有例如 Loop Transformations、Thread Bindings、Cache Locality 等，针对 GPU 还开发了一些方法例如 Thread Cooperation、Tensorization、Latency Hiding 等。</p><p>再额外的就是 TVM 还用了 Auto-tuning，由于 TVM 的论文还没看，不确定我的理解对不对。Schedule Space 模型的自动调优就是尝试不同的优化方法组合，然后在整个策略空间里面搜索哪一种优化效果最好最终就采用哪一种吗？</p><p>末尾给的一些测试中，TVM 表现出了相当不错的性能结果。</p><p>当然，TVM 还刚刚开始发展，后面还有一大堆问题留待解决。</p><h1 id="Lecture-8-Hardware-Specialization-in-Deep-Learning"><a href="#Lecture-8-Hardware-Specialization-in-Deep-Learning" class="headerlink" title="Lecture 8: Hardware Specialization in Deep Learning"></a>Lecture 8: Hardware Specialization in Deep Learning</h1><p>上一节的 TVM 是一个纯软件栈，这一节就来探索一下用于深度学习的专用硬件。</p><p>DL 的疯狂发展对计算硬件也有了越来越高的需求，而且不同应用场景的需求还可能会差很多，例如数据中心和移动终端上面的 AI 设备就完全要往两个极端去考虑。</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-evo.png" alt="Evolution of Deep Learning"></p><p>上面这张图讲的是在 DL 的发展过程中，对数据的尺寸以及存储精度的需求也在不断变化，低精度可以节省空间以及加速运算，但是这也要在硬件本身可以直接支持低精运算的前提下才能有效果（硬件是64位双精的，你要那它跑 int8？那就呵呵了…emm，此处并不是针对某 sw 哈哈）。另外，一些出现的新算法是不是能够用硬件高效实现也很关键，实现不了的话可能还是要选择老算法更好。</p><p>不断发展的 DL 算法在实验室里面可以任意瞎搞，效果好就好了，但是如果要应用到实际的生产环境中，那能不能实现/怎么高效实现就非常重要了。</p><p>再再另一方面，摩尔定律也逐渐受限，更低纳米制程的工艺难度越来越大，所有这些问题最后都会导向一个终极的解决方案，那就是 <strong>DL 专用的计算芯片/硬件</strong>了。</p><p>下面用 TPU 来举了个栗子：2015 年流片的 ASIC，92 TOPS 的峰值性能，相比 K80 有 30~80倍的性能功耗比。这些数据看着都吓人。</p><p>那为什么这么强呢？原因在于它直接硬件支持 8 位的整数 Inference（相比 16 位半精要节能 6~30 倍），大量的乘加运算部件（MACs）以及大量的片上存储。</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-tpu.png" alt="TPU Block Diagram"></p><p>TPU 主要的峰值运算性能都来自于右边的一大堆矩阵乘单元和累加器。光片上的 Unified Buffer 和 MMU 就占到了整块 TPU 超过 50% 的芯片面积。</p><p>这种设计也在于尽可能地提高数据的重用程度，提高计算密集度。</p><p>总结一下，像 TPU 这样的 DL 专用的加速器相对 CPU 和 GPU 主要有三个方面的特点：</p><ol><li>通常需要显示管理片内的存储子系统，而 CPU 的 Cache 是隐式的，对程序员透明，GPU 则是可以有自己的 L1 Cache 同时也可以手动维护（Shared Memory）；</li><li>计算主要以 Tensor （矩阵或者向量）为单元，CPU 主要就是标量运算了，配合 SIMD 的话则跟 GPU 一样主要是向量计算；</li><li>如果为 Inference 设计则不需要太高的精度，低比特量化之后可能更适合。</li></ol><p>下面是举了 3 种 Hardware/Software Co-design 的方法：</p><ol><li>Tensor 化（Tensorization）：把矩阵-向量运算变成矩阵矩阵运算，4x8 的矩阵乘上 8x1 的向量变成两个 4x4 和 4x1 的乘积（…提高计算密集度？）</li><li>存储结构针对计算内容做优化：面向卷积优化则需要一个较大的激活函数 Buffer（空间重用），较小的 FIFO 队列（存储参数）；面向 GEMM 优化则需要分配较多的空间用以累加器的块存储。</li><li>低比特量化：可以线性提高存储带宽。</li></ol><blockquote><p>。。。上面的这三个感觉理解的比较模糊。</p></blockquote><hr><p>再往下才是本节的重点内容——<strong>VTA</strong>。</p><p>TVM 构建的是软件栈，硬件加速器方面，他们也提出了一套开源的 FPGA 加速器设计方案，即 VTA（Versatile Tensor Accelerator）。</p><p>VTA 针对不同的带宽、存储和精度需求可以自定义 Tensor Core 的形状、数据类型、内存子系统分配、支持的运算操作等等；对不同类型的代码提供 CISC 或者 RISC 的指令集支持；并且还做了一些 Latency Hiding 的工作。</p><p>大致的框架设计如下：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-vta.png" alt="VTA Design"></p><p>IF 单元从 DRAM 中获取到下一条指令后，会根据类型将其发送到目标部件对应的队列中。</p><p>Load 单元负责准备激活函数和计算核的存储资源、提供Micro-Op 的缓存，取出来的数据放在 Load Buffer 中。</p><p>计算单元负责根据前面的 Micro-Op 以及准备好的数据执行 ALU 运算或者 GEMM 运算，更新寄存器的内容。</p><p>Store 单元负责把前面 Store Buffer 中的寄存器值写回 DRAM。</p><p>整体的运行依靠多个任务队列来维护数据依赖关系，基本上是个<strong>数据流</strong>的设计。（。。。）</p><p>VTA 的控制代码部分则依靠 TVM 来生成。</p><p>加上 VTA 之后，整个 TVM 的完整架构显得更复杂了：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-stack.png" alt="Programmability Challenge"></p><p>顶层是各种成熟的深度学习框架，TVM 充当编译器的角色，底层的硬件执行部分则由 VTA 来实现。</p><h1 id="Lecture-9-Memory-Optimization"><a href="#Lecture-9-Memory-Optimization" class="headerlink" title="Lecture 9: Memory Optimization"></a>Lecture 9: Memory Optimization</h1><p>还是在 System Components 这层，继续深入分析 DL 训练过程中存在的问题。</p><p>回到前面的自动求导部分，这里抛出来一个问题是为什么自动求导是采用<strong>往计算图中扩展反向计算的数据计算通路</strong>的方式，而不是<strong>直接在原来的图上进行反向计算（Backprop in Graph）</strong>？</p><blockquote><p>这个 Backprop in Graph 这里也没有更详细的说明，我大概理解成<strong>递归返回</strong>的那种样子，正向计算是不断递归向下，然后每层递归退出的时候执行反向，完美！</p><p>其实说起来，本质上递归的这个顺序也就是数据依赖关系的拓扑序。</p></blockquote><p>原因呢，则是在于内存上。</p><p>State-of-art 的很多模型都可能会有资源受限的问题，现在确实很多效果好的新模型都越来越大了，一方面计算量在增长，另一方面内存会成为一个很大的麻烦：CPU 的内存还好一点，如果是用在 GPU 上，目前单块卡的显存最多也只有几十 G 的量级。</p><p>先来看一下前向部分的内存使用情况，以下面这几个简单的运算组成的计算图为例：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-expgraph.png" alt=""></p><p><strong>朴素做法</strong>是为每个计算节点都开一块内存，则图越大、计算节点越多，需要的内存量越大。</p><p>然后我们发现这个过程中有很多内存是用过之后就不会再用了的，因此更高级一些的方案是<strong>配合内存池进行动态内存分配</strong>，例如上面 <code>mul</code> 算完之后所占有的内存就不再需要了，因此这块内存可以被内存池回收，然后用在 <code>exp</code> 的计算上。</p><p>再有另外一种方式则是<strong>静态内存规划</strong>，即拿到计算图之后，就按照尽可能重用内存的方案事先分配内存，基本上达到的效果应该要跟上一种内存池的动态分配方案差不多。这种做法有点类似<strong>编译器的寄存器重命名</strong>的过程。</p><p>基本的分配原则也非常简单，只是应用的时候另外要注意如果分配不好是有可能要影响计算的并发性的：</p><ul><li>Inplace：可行的情况下尽可能地原地存储，即把输出存到输入的内存里（前提是这份输入只被一个计算节点依赖）</li><li>Reuse：不再用到的内存尽可能地重用。</li></ul><p>下面的几个内存规划的例子也都比较简单：根据拓扑顺序依次分配和回收；或者先从起点到终点找一条最长路径，把路径上的内存全部设为 Inplace 重用，然后再找别的路径等等。</p><blockquote><p>这里举的内存重用的例子都特别简单，实际上每个计算节点需要的数据尺寸和内存大小都不一定一样，不可能这么简单地就分配好了。</p></blockquote><p>回到前面那个自动求导的两种方案的问题，可以很容易地体会出来<strong>往计算图中扩展反向通路</strong>的方案非常容易做内存优化：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-mmalloc.png" alt=""></p><p>只需要把 Inplace 和 Reuse 用好即可，在 MXNet 上测试出来的效果也非常好。</p><p>深度学习的 BP 算法中存在的最大问题在于反向运算时需要用到前向的一些结果，这事实上就大大地限制了 Reuse 策略的发挥，因为前向算出来的结果总需要找个地方暂存着。</p><p>针对一些内存需求特别大的场景，可以采用<strong>计算换空间</strong>的折中方案：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-mmalloc1.png" alt=""></p><p>只保存前向结果的一部分，当反向运算中需要时，再重新从前面开始把缺失的部分重新算一遍，用 25% 的额外计算量可以把整体的内存使用降到原来的开方级别，在某些场景下还是有非常不错的收益的。</p><blockquote><p>实验室的一位师姐之前在 RNN 的优化里面用到过这种方法，节省下内存之后可以跑更大的 BatchSize，最后得到的效果非常好。</p></blockquote><p>再回到前面两种反向方案的讨论中，这一节的最后给了一个特别有趣的点：内存重用的优化方案从某种程度上来看有点像递归中的内存分布！！</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-mmalloc2.png" alt=""></p><blockquote><p>666666！</p></blockquote><h1 id="Lecture-10-Parallel-Scheduling"><a href="#Lecture-10-Parallel-Scheduling" class="headerlink" title="Lecture 10: Parallel Scheduling"></a>Lecture 10: Parallel Scheduling</h1><p>System Components 这层的另外一个方面是并行调度的问题。</p><p>用户写好一个计算图，如果框架没有能力把机器上所有的硬件资源全都调动起来那就太浪费了。</p><p>关于 DL 中模型并行数据并行这块就不再多提了，在常规的数据并行中，计算和通信之间存在着一个 Gap：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-schedule.png" alt=""></p><p>这样一张有着复杂的计算、通信需求的计算图要如何才能比较好地并行起来呢，答案就是我们需要一个自动的调度系统。</p><p>首先计算图本身可以很好地描述计算之间的数据依赖关系，那在这个基础上的 Scheduler 设计感觉其实也没啥好说的，基本上都是很常规能够想到的解决方案。TensorFlow、MXNet 等等的基础设计原则都是这样，只是实现上可能有所不同。</p><p>同样是队列的调度方案，这里的这种是以每个变量为单位有一个自己的队列，TensorFlow 中是线程池中的每个线程会有个自己的队列。</p><h1 id="Lecture-11-Distributed-Training-and-Communication-Protocols"><a href="#Lecture-11-Distributed-Training-and-Communication-Protocols" class="headerlink" title="Lecture 11: Distributed Training and Communication Protocols"></a>Lecture 11: Distributed Training and Communication Protocols</h1><p>这一节把目标放在上一节调度图中的 Synchronization 部分。</p><p>大量的篇幅是对 Allreduce 的讨论，也没啥好说的，跳过跳过…</p><p>Parameter Server - Worker 架构的同步异步，也没啥好说的…</p><blockquote><p>Emm…这两节不是我偷懒，主要是内容比较基础，跟着 PPT 就好了，没什么特别值得注意的。</p></blockquote><h1 id="Lecture-12-Model-Serving"><a href="#Lecture-12-Model-Serving" class="headerlink" title="Lecture 12: Model Serving"></a>Lecture 12: Model Serving</h1><p>除去后面没有资料的几个 Guest Talk 以外，这节算是课程内容的最后一部分了，主要讲的是实验室的成果上线进行实用的过程中可能会有的问题，例如：</p><ul><li>延迟限制：在云上提供服务的时候，Batch Size 没办法做到越大越好；如果是在终端设备上提供服务，则能够支持运行的模型本身也有轻量级的要求；</li><li>资源限制：设备有功耗、内存限制；云服务则有花费开销的限制；</li><li>精度限制：提供多级 QoS 等等。</li></ul><p>在下面这个视频应用中：</p><p><img data-src="http://jcf94.com/download/2018-10-04-cse559w-resource.png" alt=""></p><p>终端设备的采集、处理、数据传输等各个不同有码率、功耗等限制，云端提供服务的部分也有带宽和花费开销的限制，事实上从 Workload 到 Budget 之间也存在一个巨大的 Gap。</p><p>下面的内容主要从模型压缩和服务系统两个方面来介绍。</p><h2 id="Model-Compression"><a href="#Model-Compression" class="headerlink" title="Model Compression"></a>Model Compression</h2><p>这部分是介绍如何对一个网络模型进行压缩。</p><p>首先是<strong>矩阵/向量的低秩分解</strong>，可以应用在全连接层和卷积层中，能够有效地降低整体的计算量和存储量。</p><blockquote><p>这块还是自己的数学知识比较缺乏，暂时不往下细看了。</p></blockquote><p>然后是<strong>网络剪枝</strong>：训好一个网络之后，通过一个 01 的 Mask 把参数中的某些部分置为 0，再重新训练达到之前相同的预测精度，不断重复以上过程并且逐渐增加 Mask 中 0 的比例，最后就可以得到一个想要的剪枝结果。</p><p><strong>权值共享</strong>：对参数矩阵进行重新采样，把实际值存在一张表中，然后参数矩阵改成存储对应实际值在表中的索引。这也是尽可能地减少存储冗余。</p><p><strong>低比特量化</strong>：这个也比较容易理解，就是降低数据类型的存储精度，32 位单精降到半精、int8 甚至是二进制的 01 值。预测结果可能会有一定的精度损失，但是在可以接受的精度损失范围内可以大大节省参数的存储量，并且配合上低精度的硬件预算部件也能大大加快运算速度。</p><p><strong>知识蒸馏</strong>：用一个训练好的大模型来训练一个小模型。</p><blockquote><p>Knowledge Distillation 这块感觉挺神奇，但是还没细看，不是很理解。</p></blockquote><p>这里还给了一些参考的论文资料：</p><ul><li>Compression of deep convolutional neural networks for fast and low power mobile applications. ICLR 2016</li><li>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. ICLR 2016</li><li>“XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV 2016</li><li>EIE: Efficient Inference Engine on Compressed Deep Neural Network. ISCA 2016</li><li>MCDNN: An Approximation-Based Execution Framework for Deep Stream Processing Under Resource Constraints. MobiSys 2016</li></ul><h2 id="Serving-System"><a href="#Serving-System" class="headerlink" title="Serving System"></a>Serving System</h2><p>一个比较好的服务系统需要达到几个目标：</p><ul><li>写应用的时候要有很高的灵活性</li><li>应用跑在 GPU 上要有很高的效率</li><li>满足各种不同的延迟 SLA 需求</li></ul><p>然后举了个叫 Nexus 的系统为例，后面就不细看了。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;国庆在家闲不住想干活系列……&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本篇的内容是陈天奇大佬今年春季&lt;a href=&quot;http://dlsys.cs.washington.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;在华盛顿大学开的一门课&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;大佬是上交 ACM 班的本硕，虽然目前还在 UW 读博中，但是在机器学习圈子里面已经有了很高的名望了，他的 MXNet 和 XGBoost 相信很多人就算没用过也肯定听说过（比如我就没用过…）。前段时间他发布的 TVM 也算是开启了深度学习和系统方面探索的一条新道路。&lt;/p&gt;
&lt;p&gt;课程介绍上讲的是这门课的目标是填上深度学习算法和系统的实现以及优化之间的 gap，粗略地翻了一下 PPT，后面也有比较多的篇幅是介绍 TVM 的，正是我想了解的！&lt;/p&gt;
&lt;p&gt;没找到课程的视频，但是 PPT 可以在上面的课程链接或者&lt;a href=&quot;https://1drv.ms/f/s!AuxK9QzqA6LQlI8SyU3aTAKCrZJbHQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;找到。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jcf94.com/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="https://jcf94.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>几个多线程的练手 case</title>
    <link href="https://jcf94.com/2018/09/10/2018-09-10-multithreadsproject/"/>
    <id>https://jcf94.com/2018/09/10/2018-09-10-multithreadsproject/</id>
    <published>2018-09-10T15:12:11.000Z</published>
    <updated>2018-10-15T11:01:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>Emm……接前篇。</p><p>主要是整理几个多线程小例子，读写锁和定时器队列是面试的时候考到的，线程池是自己很早就想写着玩的（挖了好久的坑一直没填，内存池也是……）。</p><p>整理出来的代码会放在这里：</p><ul><li><a href="https://github.com/jcf94/multithread_case" target="_blank" rel="noopener">jcf94/multithread_case</a></li></ul><a id="more"></a><h1 id="ThreadPool"><a href="#ThreadPool" class="headerlink" title="ThreadPool"></a>ThreadPool</h1><p>参考了一下别人的实现方案：</p><ul><li><a href="http://progsch.net/wordpress/?p=81" target="_blank" rel="noopener">【A Thread Pool with C++11】</a></li></ul><p>总体上还是比较容易理解的，每个工作线程队里。</p><p>当任务队列为空时，每一条工作线程在一个条件变量上等待；当任务队列非空时，则空闲线程直接从任务队列中取出封装好的 <code>std::function</code> 来执行。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> ThreadRunner::<span class="keyword">operator</span>()()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>()&gt; task;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; lock(pool_.tasks_mutex_);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!pool_.stop_ &amp;&amp; pool_.tasks_.empty())</span><br><span class="line">            &#123;</span><br><span class="line">                pool_.cv_.wait(lock);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (pool_.stop_) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">            task = pool_.tasks_.front();</span><br><span class="line">            pool_.tasks_.pop();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        task();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="TimerQueue"><a href="#TimerQueue" class="headerlink" title="TimerQueue"></a>TimerQueue</h1><p>定时器即让一个注册好的回调函数在某个设定时间到达之后执行。</p><p>话说现在应该有很多系统级的 API 都能够提供这种<strong>延迟执行</strong>或者<strong>阻塞当前线程一段时间</strong>的功能，但是当定时的任务比较多的时候，就有可能还是自己写一个<strong>定时器队列</strong>来维护性能更好点了（主要是不确定到时候所用的定时 API 是怎么实现的，例如如果所有的这些定时任务都用 epoll_wait 的超时触发做，那 epoll 的底层实际上是用红黑树来维护的，猜想效率应该不会太差）。</p><p>另外还有一个定时精度的问题，不同的 API 可能能够提供的精度支持也是不一样的。</p><p>当时面试的时候被问到这个是远程共享写代码，由于之前没接触过这种 case，一开始不是很明白面试官到底想考我什么，就觉得定时器这种东西不是现有的 API 一大堆嘛，然后一脸懵逼。</p><p>最后写了一个线程每隔一个最小的周期（Tick）处理一次队列中的任务，把到达时间的任务取出来执行，任务列表就用优先队列或者说小根堆来实现，保证每次都能够高效地把设定时间最靠前的任务取出来。</p><hr><p>网上查了下，基本上比较不错的思路有两种，一种就是小根堆的实现，另外一种是 Linux 内核中采用的时间轮的定时器方法。</p><ul><li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-timers/index.html" target="_blank" rel="noopener">【Linux 下定时器的实现方式分析】</a></li></ul><p>由于小根堆的堆顶元素是最近触发的一个任务，因此还可以动态改变下一次处理需要等待的 Tick 时间来节省 CPU 资源的消耗。但是感觉这种方式会不会影响定时精度？以及这样似乎没有办法处理“在堆顶元素之前又插入了一个更近的任务”这种情况，所以最后想想还是觉得固定 Tick 比较稳妥。</p><p>然后就是重点要来学习一下时间轮（Timing-Wheel）了。</p><p><img data-src="http://jcf94.com/download/2018-09-10-multithreadsproject-timewheel.jpg" alt="Timing-Wheel"></p><p>这个思路其实很简单，想象有一个钟表盘，上面有一圈时间刻度（从 1 到 N），有一个指针每隔 Tick 时间转动一个刻度，而所有的任务也是按照等待时间除以 Tick 分布在每个刻度上面，当指针扫到某个刻度时，即遍历一遍这个刻度上的任务列表，把到时间的拿出来执行即可。</p><p>当 N 足够大时，显然这个定时器的各个操作的复杂度都是 <code>O(1)</code> 的。</p><p>但是如果 N 过大，则这个时间轮的内存消耗将会比较大，因此又有了多级时间轮的优化思路：以钟表上的时针、分针、秒针为例，时针分为 12 格，分针、秒针分别为 60 格，这样就有一共 132 个格子，每个格子都是一条任务链表。一分钟以内的任务被加到秒针的格子中，一小时以内的任务加到分针的格子中，更久的任务加到时针的格子中。秒针每个 Tick 移动一次，每次移动后直接把当前格子中的所有任务取出来执行；分针则是每 60 个 Tick 移动一次，每次移动把当前格子中的任务下放到秒针轮对应的格子中；时针也是类似。</p><p>更厉害的优化思路可见下面这篇：</p><ul><li><a href="http://www.lpnote.com/2017/11/16/hashed-and-hierarchical-timing-wheels/" target="_blank" rel="noopener">【基于Hash和多级时间轮：实现定时器的高效数据结构】</a></li></ul><h1 id="ReadWriteLock"><a href="#ReadWriteLock" class="headerlink" title="ReadWriteLock"></a>ReadWriteLock</h1><p>最后这个读写锁是面试完之后让半小时内写完，然后邮件回面试官交作业……</p><p>结果读写锁的逻辑很快写完之后 main 函数的测试 case 却调 bug 调了好久……狂汗</p><hr><p>读写锁从功能上来看也可以叫做共享锁，相对普通的互斥锁来说，它会有三种状态：未锁定、读锁定和写锁定。</p><p>读写锁的读操作是可以<strong>共享</strong>的，即处于读锁定的时候另外一个线程还可以继续对其施加读锁定，而写锁定跟一般的锁一样是独占的。</p><p>因此基本的思路就是用一个<strong>读计数</strong>来记录读操作，当读写锁处于解锁或者读锁定状态时，获取读锁即增加读的锁定计数，而此时想要获取写锁就需要等到所有的读操作都释放之后才行。</p><p>写操作可以用一个布尔变量来记录，当读写锁处于写锁定状态时，继续获取读锁或者写锁都需要等前一次的写操作释放。</p><p>下面分别是简单地直接用一个锁的版本：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> rwlock::getRead()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span> (is_writing == <span class="literal">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        _lock.lock();</span><br><span class="line">    &#125;</span><br><span class="line">    read_count++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> rwlock::getWrite()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span> (read_count!=<span class="number">0</span> || is_writing==<span class="literal">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        _lock.lock();</span><br><span class="line">    &#125;</span><br><span class="line">    is_writing = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> rwlock::unlockRead()</span><br><span class="line">&#123;</span><br><span class="line">    read_count--;</span><br><span class="line">    <span class="keyword">if</span> (read_count == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        _lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> rwlock::unlockWrite()</span><br><span class="line">&#123;</span><br><span class="line">    is_writing = <span class="literal">false</span>;</span><br><span class="line">    _lock.unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以及用条件变量实现的版本：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> ReadWriteLock::getRead()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; lock(cv_mutex_);</span><br><span class="line">    </span><br><span class="line">    cv_.wait(lock, [<span class="keyword">this</span>]()&#123;<span class="keyword">return</span> !<span class="keyword">this</span>-&gt;is_writing_;&#125;);</span><br><span class="line"></span><br><span class="line">    read_count_++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> ReadWriteLock::getWrite()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; lock(cv_mutex_);</span><br><span class="line"></span><br><span class="line">    cv_.wait(lock, [<span class="keyword">this</span>]()&#123;<span class="keyword">return</span> <span class="keyword">this</span>-&gt;read_count_==<span class="number">0</span> &amp;&amp; !<span class="keyword">this</span>-&gt;is_writing_;&#125;);</span><br><span class="line"></span><br><span class="line">    is_writing_ = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> ReadWriteLock::unlockRead()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; lock(cv_mutex_);</span><br><span class="line">    read_count_--;</span><br><span class="line">    <span class="keyword">if</span> (read_count_ == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        cv_.notify_one();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> ReadWriteLock::unlockWrite()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; lock(cv_mutex_);</span><br><span class="line">    is_writing_ = <span class="literal">false</span>;</span><br><span class="line">    cv_.notify_one();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Coroutine"><a href="#Coroutine" class="headerlink" title="Coroutine"></a>Coroutine</h1><p>协程。</p><p>严格地说这个跟多线程关系不大，其实这个东西本身大概类似一种用户态的线程，然后关键在于这个线程的运行和调度都是要靠用户自己来管理的。</p><p>很多语言里面都自带协程的支持，例如，举一个别人的<a href="https://zhuanlan.zhihu.com/p/25979906" target="_blank" rel="noopener">例子</a>，一个 js 写的生成斐波那契数列的函数：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">fibonacii</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">let</span> i = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">let</span> j = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">let</span> k = <span class="number">0</span>; k &lt; <span class="number">10</span>; k++) &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(i);</span><br><span class="line">    <span class="built_in">console</span>.log(j);</span><br><span class="line">    i += j;</span><br><span class="line">    j += i;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用协程的写法则是：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> *<span class="title">fibonaciiUsingYield</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">let</span> i = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">let</span> j = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">yield</span> i;</span><br><span class="line">    <span class="keyword">yield</span> j;</span><br><span class="line">    i += j;</span><br><span class="line">    j += i;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">------</span><br><span class="line"><span class="keyword">const</span> gen = fibonaciiUsingYield();</span><br><span class="line">gen.next();</span><br></pre></td></tr></table></figure><p>相当于每一次这个函数都<strong>断点</strong>在 yield 对象上，当调用 next 函数时，函数向前执行一轮。</p><p>那么如何在本身不支持协程的 C/C++ 里面把这种特性用上呢？</p><p>答案是手动实现一个状态机……</p><p>详见轮子哥的考不上三本系列：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/25959684" target="_blank" rel="noopener">【考不上三本也能懂系列——前言】</a></li></ul><p>下面是按照这种方式写的一个简单例子：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID   : Chen Fan</span></span><br><span class="line"><span class="comment">LANG   : G++</span></span><br><span class="line"><span class="comment">PROG   : Coroutine_test</span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">original_func</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (i&lt;<span class="number">10</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, i);</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">func_callable</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    func_callable(<span class="keyword">int</span> n)</span><br><span class="line">        : total_(n) &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> result;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ans;</span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">switch</span>(state_)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                    i_ = <span class="number">0</span>;</span><br><span class="line">                    <span class="keyword">if</span> (i_ &lt; total_) &#123;state_ = <span class="number">1</span>; <span class="keyword">continue</span>;&#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;state_ = <span class="number">3</span>; <span class="keyword">continue</span>;&#125; </span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">                    result = i_;</span><br><span class="line">                    &#123;state_ = <span class="number">2</span>; <span class="keyword">return</span> <span class="literal">false</span>;&#125;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">                    i_++;</span><br><span class="line">                    <span class="keyword">if</span> (i_ &lt; total_) &#123;state_ = <span class="number">1</span>; <span class="keyword">continue</span>;&#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;state_ = <span class="number">3</span>; <span class="keyword">continue</span>;&#125;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">                    &#123;state_ = <span class="number">-1</span>; <span class="keyword">return</span> <span class="literal">true</span>;&#125;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">default</span>:</span><br><span class="line">                    <span class="keyword">throw</span> system_error();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> i_;</span><br><span class="line">    <span class="keyword">int</span> total_;</span><br><span class="line">    <span class="keyword">int</span> state_ = <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">func_callable <span class="title">coroutine_func</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> func_callable(n);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    original_func();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> a = coroutine_func(<span class="number">10</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Coroutine Test:\n"</span>);</span><br><span class="line">    a();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, a.result);</span><br><span class="line">    a();a();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, a.result);</span><br><span class="line">    a();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, a.result);</span><br><span class="line">    a();a();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, a.result);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Result：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">Coroutine Test:</span><br><span class="line">0</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">5</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Emm……接前篇。&lt;/p&gt;
&lt;p&gt;主要是整理几个多线程小例子，读写锁和定时器队列是面试的时候考到的，线程池是自己很早就想写着玩的（挖了好久的坑一直没填，内存池也是……）。&lt;/p&gt;
&lt;p&gt;整理出来的代码会放在这里：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jcf94/multithread_case&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;jcf94/multithread_case&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="Multithreads" scheme="https://jcf94.com/tags/Multithreads/"/>
    
      <category term="ThreadPool" scheme="https://jcf94.com/tags/ThreadPool/"/>
    
      <category term="TimerQueue" scheme="https://jcf94.com/tags/TimerQueue/"/>
    
      <category term="ReadWriteLock" scheme="https://jcf94.com/tags/ReadWriteLock/"/>
    
  </entry>
  
  <entry>
    <title>多线程相关整理</title>
    <link href="https://jcf94.com/2018/09/07/2018-09-07-multithreads/"/>
    <id>https://jcf94.com/2018/09/07/2018-09-07-multithreads/</id>
    <published>2018-09-07T01:50:25.000Z</published>
    <updated>2018-09-12T08:33:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>还是在整理秋招遭遇过的面试题的时候顺便补充一下多线程相关的东西。</p><p>一开始是分了 Basic 和 Project 两个大标题，主要还是想写三个面试遇到的多线程例子的实现，然后发现前面 Basic 部分的东西写的有点多了……篇幅有点长，然后想了想还是把 Project 部分另外开一篇吧。</p><p>这大概就是计划赶不上变化？……就是这么任性。</p><a id="more"></a><hr><p>这里来理一下 C 和 C++ 里面与多线程相关的 API。</p><h1 id="C-Format"><a href="#C-Format" class="headerlink" title="C Format"></a>C Format</h1><p>在 C11 之前，C 标准库里面本身不带线程支持，通常需要用 UNIX/LINUX 系统库里面的 clone、fork 之类或者用 POSIX 标准的 pthread 库来实现。虽然这些东西都包含在 glibc 的库里面了，但是事实上不算 C 语言本身的标准，所以在 cppreference 里面是搜不到的（当时奇怪了好久）。</p><p>pthread 底层用来创建线程的实现应该也是调的 clone、fork 这些系统调用来完成的。</p><p>至于 mutex 锁、信号量这些原语，从 Linux 2.6.x 版本内核之后都是通过 FUTEX 系统调用来实现的，具体以后有空再看。</p><p>另外需要对<strong>线程</strong>和<strong>进程</strong>这两个概念作一下强调，Linux 中本身不分进程和线程统称为 task，后来用于区分这两个概念主要是看一个 task 所拥有的资源情况。进程有自己的内存空间，线程共享父进程的内存空间。</p><hr><h2 id="fork-vfork-clone"><a href="#fork-vfork-clone" class="headerlink" title="fork(), vfork(), clone()"></a>fork(), vfork(), clone()</h2><p>先来看一下三个系统接口。</p><table><thead><tr><th>系统调用</th><th>描述</th></tr></thead><tbody><tr><td>fork()</td><td>创建父进程的<strong>完整副本</strong>，复制父进程的资源，包括所有内存的内容。<strong>写时复制</strong>。</td></tr><tr><td>vfork()</td><td>创建的子进程与父进程共享数据段，并且由 vfork 创建的子进程先运行，父进程在子进程返回前保持阻塞。</td></tr><tr><td>clone()</td><td>可以对创建的子进程做更多控制，启动的时候指定需要执行的函数内容。</td></tr></tbody></table><blockquote><p>需要注意的是这几个 API 在 Mingw 里面是用不了的。</p><p>Bash on Windows 大法好<del>~</del></p></blockquote><p>fork() 的使用方式特别简单：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: GCC</span></span><br><span class="line"><span class="comment">PROG: </span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> aaa = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">pid_t</span> fpid = fork();</span><br><span class="line">    <span class="keyword">int</span> bbb = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (fpid &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Fork Error!"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (fpid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        aaa++;</span><br><span class="line">        bbb++;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"This is son, with pid: %d, aaa: %d(%p), bbb: %d(%p)\n"</span>, getpid(), aaa, &amp;aaa, bbb, &amp;bbb);</span><br><span class="line">    &#125; <span class="keyword">else</span> </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"This is Father, with pid: %d, aaa: %d(%p), bbb: %d(%p)\n"</span>, getpid(), aaa, &amp;aaa, bbb, &amp;bbb);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Son's pid: %d\n"</span>, fpid);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:45:20]</span></span><br><span class="line">$ gcc test.c</span><br><span class="line"></span><br><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:45:50]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">This is Father, with pid: 487, aaa: 1(0x7fffe2ce2d9c), bbb: 1(0x7fffe2ce2da0)</span><br><span class="line">This is son, with pid: 488, aaa: 2(0x7fffe2ce2d9c), bbb: 2(0x7fffe2ce2da0)</span><br><span class="line">Son<span class="string">'s pid: 488</span></span><br></pre></td></tr></table></figure><p>fork() 创建产生的子进程除了这个函数返回的 pid 值以外，与父进程完全一致，父子进程也都会从 fork() 函数返回之后继续向下执行相同的内容。另外，如果把本地的变量值的地址打出来，会发现它们的虚拟地址也都是一样的。</p><p>这里用了一个写时复制的技术，fork 出来的子进程一开始直接用的是父进程的内存空间，所有内容包括虚拟地址都是跟父进程完全一致的，直到发生了数据更改，才会在物理地址空间中作一个新的映射。这也就提高了创建子进程的效率，因为分配新的页表也会是一件比较耗时的工作。</p><p>然后是 vfork()，把上面这段代码中的 fork 直接改成 vfork 之后会得到这样的运行结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:46:37] C:134</span></span><br><span class="line">$ gcc test.c</span><br><span class="line"></span><br><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:46:42]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">This is son, with pid: 550, aaa: 2(0x7ffff4873a2c), bbb: 2(0x7ffff4873a30)</span><br><span class="line">This is Father, with pid: 549, aaa: 0(0x7ffff4873a2c), bbb: 1(0x7ffff4873a30)</span><br><span class="line">Son<span class="string">'s pid: 550</span></span><br><span class="line"><span class="string">*** stack smashing detected ***: ./a.out terminated</span></span><br><span class="line"><span class="string">[1]    549 abort (core dumped)  ./a.out</span></span><br></pre></td></tr></table></figure><p>vfork 调用之后，父进程会被阻塞，所以可以看到不同于之前的情况，这里永远都是 son 这句先输出，执行完毕之后才会轮到父进程执行。</p><p>那为什么会炸了呢……</p><p>在 fpid 等于 0 的分支末尾加上 <code>exit(0);</code> 之后程序就能够正常执行了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:47:10] C:134</span></span><br><span class="line">$ gcc test.c</span><br><span class="line"></span><br><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:47:18]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">This is son, with pid: 584, aaa: 2(0x7fffc8d7641c), bbb: 2(0x7fffc8d76420)</span><br><span class="line">This is Father, with pid: 583, aaa: 2(0x7fffc8d7641c), bbb: 1(0x7fffc8d76420)</span><br><span class="line">Son<span class="string">'s pid: 584</span></span><br></pre></td></tr></table></figure><p>原因是 vfork 不同于 fork 的一点是，创建出来的子进程直接共享父进程的数据段，当子进程跑完之后，他会像一个正常的进程一样对自己的栈空间等等做回收，则之后当父进程开始执行的时候自身的内存数据就被子进程破坏掉了一部分，这也是为什么前面第一次父进程的 aaa 输出来的结果是不对的，而加上 <code>exit(0);</code> 之后，父进程可以正常输出 2。</p><p>话说网上说 vfork 出来的子进程如果用 return 来返回的话会出现很奇怪的 bug，不过我这里测试的时候没有见到，可能跟 gcc 和系统库的版本有关系。</p><p>那么为什么会有 vfork 这个看上去有点问题的实现呢？</p><p>这就需要提到另外一个系统接口 exec 了。exec 的作用是拿另外一个程序的代码来替换到当前进程的正文、数据和堆栈，简单地说就是用来启动一个新程序。</p><blockquote><p>exec 的接口实际上是一套，一共 6 个函数，具体的这里先不展开了。</p></blockquote><p>vfork 自身设计的目标是为 exec 服务的，当需要创建一个新进程来执行一段完全不同的代码时，vfork 直接共享父进程地址空间的做法是<strong>开销最小的</strong>，即保证<strong>先有一个子进程</strong>，然后调用 exec 来载入一段新的代码并且创建自己的独立地址空间，<strong>在子进程开始新程序或者退出之前，内核保证父进程一直处于阻塞状态</strong>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: GCC</span></span><br><span class="line"><span class="comment">PROG: </span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> aaa = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">pid_t</span> fpid = vfork();</span><br><span class="line">    <span class="keyword">int</span> bbb = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (fpid &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Fork Error!"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (fpid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        aaa++;</span><br><span class="line">        bbb++;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"This is son, with pid: %d, aaa: %d(%p), bbb: %d(%p)\n"</span>, getpid(), aaa, &amp;aaa, bbb, &amp;bbb);</span><br><span class="line">        execv(<span class="string">"hello.out"</span>, <span class="literal">NULL</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"This is Father, with pid: %d, aaa: %d(%p), bbb: %d(%p)\n"</span>, getpid(), aaa, &amp;aaa, bbb, &amp;bbb);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Son's pid: %d\n"</span>, fpid);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果大概是这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [16:07:41] C:1</span></span><br><span class="line">$ gcc test.c</span><br><span class="line">test.c: In <span class="keyword">function</span> ‘main’:</span><br><span class="line">test.c:27:9: warning: null argument <span class="built_in">where</span> non-null required (argument 2) [-Wnonnull]</span><br><span class="line">         execv(<span class="string">"hello.out"</span>, NULL);</span><br><span class="line">         ^</span><br><span class="line"></span><br><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [16:09:07]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">This is son, with pid: 654, aaa: 2(0x7fffec2f627c), bbb: 2(0x7fffec2f6280)</span><br><span class="line">This is Father, with pid: 653, aaa: 2(0x7fffec2f627c), bbb: 1(0x7fffec2f6280)</span><br><span class="line">Son<span class="string">'s pid: 654</span></span><br><span class="line"><span class="string">Hello World</span></span><br></pre></td></tr></table></figure><p>最后是 clone，先看下 man 里面的定义：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Prototype for the glibc wrapper function */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _GNU_SOURCE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sched.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">clone</span><span class="params">(<span class="keyword">int</span> (*fn)(<span class="keyword">void</span> *), <span class="keyword">void</span> *child_stack,</span></span></span><br><span class="line"><span class="function"><span class="params">          <span class="keyword">int</span> flags, <span class="keyword">void</span> *arg, ...</span></span></span><br><span class="line"><span class="function"><span class="params">          <span class="comment">/* pid_t *ptid, void *newtls, pid_t *ctid */</span> )</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* For the prototype of the raw system call, see NOTES */</span></span><br></pre></td></tr></table></figure><ul><li><p>fn 是需要执行的函数指针，即 clone 出来的子进程需要执行的函数内容。</p></li><li><p>child_stack 就明显是给子进程分配的系统堆栈空间的位置。</p></li><li><p>flags 用于描述子进程需要从父进程中继承哪些部分的内容，因此通过这个值可以控制产生进程、线程、甚至非父子关系而是兄弟关系的进程等等，功能强大。</p></li><li><p>后面的就是传入新进程中的参数了</p></li></ul><p>测试代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: GCC</span></span><br><span class="line"><span class="comment">PROG: </span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _GNU_SOURCE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sched.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">hello</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"This is: %d, Hello World\n"</span>, getpid());</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">void</span>* <span class="built_in">stack</span> = <span class="built_in">malloc</span>(<span class="number">8192</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> pid = clone(&amp;hello, (<span class="keyword">char</span>*)<span class="built_in">stack</span>+<span class="number">8192</span>, CLONE_PARENT, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Father pid: %d, new pid: %d\n"</span>, getpid(), pid);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面这份代码中有两个地方需要额外注意一下：</p><ol><li>在 &lt;sched.h&gt; 头文件引用前要加上 <code>#define _GNU_SOURCE</code> 的宏，表明下文代码不可移植，可能会用到一些非 GNU 标准的内容（例如 clone）。</li><li>clone() 中的第二个参数指定的是栈空间，然后因为<strong>栈是反向增长的！！</strong>，所以这里需要传入申请的空间的尾部。</li></ol><p>结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [16:46:24]</span></span><br><span class="line">$ gcc test.c</span><br><span class="line"></span><br><span class="line"><span class="comment"># jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [16:46:54]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">Father pid: 989, new pid: 990</span><br><span class="line">This is: 990, Hello World</span><br></pre></td></tr></table></figure><p>这三个接口的最底层涉及到的都是 <code>do_fork()</code> 这个调用，只是传入的参数不同，clone 可以认为就是个 <code>do_fork()</code> 的 API 外衣。</p><h2 id="pthreads"><a href="#pthreads" class="headerlink" title="pthreads"></a>pthreads</h2><p>pthreads 的全称应该是 POSIX Threads，是 POSIX 的线程标准，它定义了一套 C 语言标准的线程控制 API，由一个 &lt;pthread.h&gt; 的头文件和一个线程库来实现，主要包含了：<strong>线程管理、互斥锁、条件变量、线程同步</strong>等等这些线程操作的 API。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: GCC</span></span><br><span class="line"><span class="comment">PROG: </span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;pthread.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">hello</span><span class="params">(<span class="keyword">void</span>* arg)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"This is: %d, Hello World\n"</span>, getpid());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">pthread_t</span> tt;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pthread_create(&amp;tt, <span class="literal">NULL</span>, hello, <span class="literal">NULL</span>))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Thread Create Error\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    pthread_join(tt, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Running] <span class="built_in">cd</span> <span class="string">"c:\Users\jcf\Desktop\multithread\" &amp;&amp; gcc test.c -o test &amp;&amp; "</span>c:\Users\jcf\Desktop\multithread\"<span class="built_in">test</span></span><br><span class="line">This is: 18308, Hello World</span><br></pre></td></tr></table></figure><p>话说从使用方式上来看，<code>pthread_create()</code> 的接口跟 clone 就特别像，大概率底层实现就是用 clone 做的，不过传入的线程函数的格式不太一样。</p><p>pthread 也提供了互斥锁和条件变量这些结构： <code>pthread_mutex_t</code> 、<code>pthread_cond_t</code>。具体的使用方式跟后面的差别不大，下文再整理。</p><p>头文件 &lt;semaphore.h&gt; 中也提供了信号量的支持。</p><h2 id="C11"><a href="#C11" class="headerlink" title="C11"></a>C11</h2><p>C11 之后，标准库里面提供了线程支持，包含在头文件 &lt;threads.h&gt; 中（这下可以在 cppreference 里面查到啦）。</p><p>基本也就是线程创建、等待、互斥、条件变量等等的支持，感觉看上去跟 pthread 基本一致。</p><blockquote><p>而且不知道为什么，虽然在标准库里面查到了这个库，但是似乎用的人特别少。</p><p>那 C 的部分还是用 pthread 吧。</p></blockquote><h1 id="C-Format-1"><a href="#C-Format-1" class="headerlink" title="C++ Format"></a>C++ Format</h1><ul><li><a href="https://www.zhihu.com/question/36236334" target="_blank" rel="noopener">【学习c++多线程编程主要用pthread还是c++11中的thread类？】</a></li></ul><p>从知乎讨论上面来看，大家对 C++11 的 thread 意见还是比较大的。</p><hr><p>C++ 这部分……其实涉及到的东西非常多，需要一堆不同的库联合起来一起用：</p><p>线程支持库 &lt;thread&gt; 提供了线程创建、调度、等待等等一系列管理操作；</p><p>互斥库 &lt;mutex&gt; 提供了基本的互斥量 mutex，RAII 的锁控制方式 lock_guard 和 unique_lock 等等；</p><p>条件变量库 <condition_variable> 提供了条件变量的支持；</p><p>异步支持库 &lt;future&gt; 提供了像 promise、future、async 等等这种异步语义（在 Nodejs 里面用过，之前还真没听说 C++ 里面还带这种玩意）；</p><p>原子操作库 &lt;atomic&gt; 提供了一系列与原子操作相关的支持；</p><p>另外 C++ 中任何可以被调用的东西都是<strong>函数对象</strong>，前面用来创建线程用的目标函数也需要由函数对象库 &lt;functional&gt; 来管理，std::bind、std::invoke 等等这些管理参数调用，也可以用 lambda 表达式等等。</p><blockquote><p>话说 &lt;thread&gt; 库是不是也还是 pthread 的封装？？？</p></blockquote><h2 id="functional"><a href="#functional" class="headerlink" title="functional"></a>functional</h2><p>有关 <code>std::function</code> 和 Lambda 表达式，很早之前稍微有记过一些：</p><ul><li><a href="/2017/02/17/2017-02-17-cppnew/">【C++11 及之上的一些新东西】</a></li></ul><h2 id="mutex-lock-guard-unique-lock"><a href="#mutex-lock-guard-unique-lock" class="headerlink" title="mutex, lock_guard, unique_lock"></a>mutex, lock_guard, unique_lock</h2><p>C++11 中的基础互斥锁结构是 <code>std::mutex</code>，用法应该基本跟 pthread 的一样。&lt;mutex&gt; 中额外还提供了两个符合 RAII 标准的锁控制封装，以更加异常安全的方式来管理互斥锁。</p><p><code>std::lock_guard</code> 就是个简单的互斥封装容器，构造时锁定给定的锁，然后析构的时候自动释放。事实上它能操作的锁不一定只限于 <code>std::mutex</code>，任何有 <code>lock()</code> 和 <code>un_lock()</code> 两个成员函数的对象都可以。</p><p><code>std::unique_lock</code> 功能更多一点。构造时可选地对传入的锁上锁（也可以选择不锁），析构时自动释放。并且同时它还提供了 <code>lock()</code>、<code>try_lock()</code>、<code>unlock()</code> 等等这些成员函数，使用起来就更灵活了，除了离开作用域自动析构释放这一点之外，在作用域中还可以手动控制加锁解锁。</p><h2 id="condition-variable"><a href="#condition-variable" class="headerlink" title="condition_variable"></a>condition_variable</h2><p>条件变量需要结合互斥锁一起使用，这里的 <code>std::condition_variable</code> 尤其在 wait 的时候必须配合 <code>std::unique_lock</code> 来用。</p><p>条件变量的核心操作是等待（wait）和唤醒（notify），通常情况下，需要在条件变量上等待的线程需要：</p><ol><li>首先需要获得 <code>std::unique_lock&lt;std::mutex&gt;</code> 锁（重要！！）；</li><li>执行 <code>wait()</code>、<code>wait_for()</code> 或者 <code>wait_until()</code> ，这三个函数需要把前面的 unique_lock 作为参数传入，执行时将原子地释放传入的 unique_lock，然后挂起当前线程进入等待状态；</li><li>当条件变量被其他线程唤醒（notify）或者超时（对于 wait_for、wait_until）时，当前线程结束等待状态，<strong>unique_lock 自动获得锁</strong>，然后往下继续执行。</li></ol><p>在条件变量上执行唤醒操作的线程需要：</p><ol><li>首先同样要获得锁，这里不是一定要用 <code>std::unique_lock&lt;std::mutex&gt;</code>，其他方式管理也行；</li><li>可以对用于判断的其他数据进行操作；</li><li>对条件变量执行 <code>notify_one()</code> 或者 <code>notify_all()</code>，则其他处于等待状态的线程会被唤醒。</li></ol><p>文档中对 <code>notify_one</code> 的描述是会唤醒<strong>一个</strong>等待的线程，但是并不一定是哪一个，跟进入 wait 状态的线程的先后顺序无关，<code>notify_all</code> 则是唤醒<strong>当前正处于等待状态中的所有线程</strong>。</p><p>由于 wait 操作本身自带对 unique_lock 的加锁解锁操作，因此 notify 这边也需要注意前面这个 mutex 锁的状况，<strong>必须保证 wait 在调用的时候 mutex 是锁上的，然后 wait 被唤醒的时候锁是开着的</strong>。如果 wait 唤醒时试图获取锁失败则会<strong>被阻塞在等互斥锁的状态</strong>，这个下面有测试。</p><ul><li><a href="https://stackoverflow.com/questions/13099660/c11-why-does-stdcondition-variable-use-stdunique-lock" target="_blank" rel="noopener">C++11: why does std::condition_variable use std::unique_lock?</a></li></ul><p>还有一个重要的注意点是多个线程对某个条件变量的 notify 和 wait 操作可以看成是<strong>对一个原子变量的顺序操作</strong>，这就意味着如果先调用 notify，再调用 wait，则 wait 是不会从唤醒中恢复的。</p><blockquote><p>看上去这个注意点很正常啊，正常就应该是这样的啊。但是实际多线程操作中非常容易出现：自认为 notify 会发生在 wait 以后，实际执行却不是，然后导致死锁的 bug。</p></blockquote><p>Talk is cheap, show me the code!</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: G++</span></span><br><span class="line"><span class="comment">PROG: CV_TEST</span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;condition_variable&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mutex&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;thread&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    mutex cv_m;</span><br><span class="line">    condition_variable cv;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------- 1 -----------</span></span><br><span class="line">    <span class="comment">// &#123;</span></span><br><span class="line">    <span class="comment">//     unique_lock&lt;std::mutex&gt; lock(cv_m);</span></span><br><span class="line">    <span class="comment">//     cv.notify_one();</span></span><br><span class="line">    <span class="comment">// &#125;</span></span><br><span class="line"></span><br><span class="line">    this_thread::sleep_for(chrono::seconds(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">    <span class="function">thread <span class="title">th1</span><span class="params">([&amp;cv, &amp;cv_m]</span></span></span><br><span class="line"><span class="function"><span class="params">    &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">        unique_lock&lt;<span class="built_in">std</span>::mutex&gt; lock(cv_m);</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="built_in">printf</span>(<span class="string">"th1 Start Waiting\n"</span>);</span></span></span><br><span class="line"><span class="function"><span class="params">        cv.wait(lock);</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="built_in">printf</span>(<span class="string">"th1 wake up\n"</span>);</span></span></span><br><span class="line"><span class="function"><span class="params">    &#125;)</span></span>;</span><br><span class="line"></span><br><span class="line">    this_thread::sleep_for(chrono::seconds(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------- 2 -----------</span></span><br><span class="line">    &#123;</span><br><span class="line">        unique_lock&lt;<span class="built_in">std</span>::mutex&gt; lock(cv_m);</span><br><span class="line">        cv.notify_one();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------- 3 -----------</span></span><br><span class="line">    <span class="comment">// cv_m.lock();</span></span><br><span class="line">    <span class="comment">// cv.notify_one();</span></span><br><span class="line">    <span class="comment">// cv_m.unlock();</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------- 4 -----------</span></span><br><span class="line">    <span class="comment">// cv_m.lock();</span></span><br><span class="line">    <span class="comment">// cv.notify_one();</span></span><br><span class="line">    <span class="comment">// this_thread::sleep_for(chrono::seconds(2));</span></span><br><span class="line">    <span class="comment">// cv_m.unlock();</span></span><br><span class="line"></span><br><span class="line">    th1.join();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>做了个小测试，对于上面打上标记的 4 块代码：</p><ol><li>中间加了个 sleep 2 秒来确保先调用 notify，然后 wait，妥妥的死锁！</li><li>确保先调用 wait，然后 notify，正常工作！</li><li>这么写也是可以正常工作的，但是如果把下面那段 <code>cv_m.unlock()</code> 删掉，则结果就会死锁！！！原因恰恰是在于 wait 被唤醒的时候要首先试图获得锁，由于 cv_m 这时候是锁着的，然后 wait 线程就被阻塞在获取互斥锁的状态了。</li><li>为了确认 3 里面的这一点，我又写了 4 这个测试。从这里可以明确的是，notify 操作之后，wait 线程虽然仍然阻塞，但是这个阻塞状态跟前面线程挂起的等待状态是不同的，而是卡在 cv_m 这个锁上。</li></ol><p>所以看上去最好的方式是 notify 的时候根本就别管锁？如果不上锁，不就没这么多麻烦了吗……事实上，更好的写法应该是这样的：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: G++</span></span><br><span class="line"><span class="comment">PROG: CV_TEST</span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;condition_variable&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mutex&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;thread&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    mutex cv_m;</span><br><span class="line">    condition_variable cv;</span><br><span class="line">    <span class="keyword">bool</span> ready = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------- 1 -----------</span></span><br><span class="line">    <span class="comment">// &#123;</span></span><br><span class="line">    <span class="comment">//     lock_guard&lt;mutex&gt; lock(cv_m);</span></span><br><span class="line">    <span class="comment">//     ready = true;</span></span><br><span class="line">    <span class="comment">//     cv.notify_one();</span></span><br><span class="line">    <span class="comment">// &#125;</span></span><br><span class="line"></span><br><span class="line">    this_thread::sleep_for(chrono::seconds(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">    <span class="function">thread <span class="title">th1</span><span class="params">([&amp;cv, &amp;cv_m, &amp;ready]</span></span></span><br><span class="line"><span class="function"><span class="params">    &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">        unique_lock&lt;mutex&gt; lock(cv_m);</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="built_in">printf</span>(<span class="string">"th1 Start Waiting\n"</span>);</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="comment">//cv.wait(lock, [&amp;ready]&#123;return ready;&#125;);</span></span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">while</span> (!ready) cv.wait(lock);</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="built_in">printf</span>(<span class="string">"th1 wake up\n"</span>);</span></span></span><br><span class="line"><span class="function"><span class="params">    &#125;)</span></span>;</span><br><span class="line"></span><br><span class="line">    this_thread::sleep_for(chrono::seconds(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------- 2 -----------</span></span><br><span class="line">    &#123;</span><br><span class="line">        lock_guard&lt;mutex&gt; lock(cv_m);</span><br><span class="line">        ready = <span class="literal">true</span>;</span><br><span class="line">        cv.notify_one();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    th1.join();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>三种 wait 函数均有一种附加条件的多参数调用方式，等同于在一个 while 循环中调用单参数版的 wait 函数。另外用一个 ready 变量标识等待情况，在 notify 时，cv_m 这个锁实际上是用于保护这个 ready 变量用的。用这种方式写则无论 notify 代码块发生在 wait 线程前还是发生在之后，wait 线程均会正常返回了。</p><h2 id="future"><a href="#future" class="headerlink" title="future"></a>future</h2><p>这个头文件里面的内容很有意思，核心的类主要是 <code>std::promise</code>、<code>std::packaged_task</code>、<code>std::future</code> 这几个。</p><p><code>std::future</code> 是一个对异步操作结果的封装类，一般需要配合 <code>std::async</code>、<code>std::packaged_task</code> 和 <code>std::promise</code> 的异步操作使用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID   : Chen Fan</span></span><br><span class="line"><span class="comment">LANG   : G++</span></span><br><span class="line"><span class="comment">PROG   : future_test</span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;future&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">task</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    this_thread::sleep_for(chrono::seconds(<span class="number">1</span>));</span><br><span class="line">    <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">promise_task</span><span class="params">(future&lt;<span class="keyword">int</span>&gt;&amp; future_int)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = future_int.get();</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// ----------- future + async -----------</span></span><br><span class="line">    future&lt;<span class="keyword">int</span>&gt; a;</span><br><span class="line">    a = async(task, <span class="number">10</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; a.get() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------- future + packaged_task -----------</span></span><br><span class="line">    packaged_task&lt;<span class="keyword">int</span>(<span class="keyword">int</span>)&gt; b_pack(task);</span><br><span class="line">    future&lt;<span class="keyword">int</span>&gt; b = b_pack.get_future();</span><br><span class="line">    b_pack(<span class="number">20</span>);</span><br><span class="line">    <span class="comment">//thread thb(move(b_pack), 20);</span></span><br><span class="line">    <span class="comment">//thb.join();</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; b.get() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------- future + promise -----------</span></span><br><span class="line">    promise&lt;<span class="keyword">int</span>&gt; c_prom;</span><br><span class="line">    future&lt;<span class="keyword">int</span>&gt; c = c_prom.get_future();</span><br><span class="line">    <span class="function">thread <span class="title">thc</span><span class="params">(promise_task, ref(c))</span></span>;</span><br><span class="line">    this_thread::sleep_for(chrono::seconds(<span class="number">1</span>));</span><br><span class="line">    c_prom.set_value(<span class="number">30</span>);</span><br><span class="line">    thc.join();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>std::async</code> 的作用是在另外一个线程中异步（默认操作，也可以设置在调用线程中同步执行）地执行给定的函数，函数执行的返回值则是由一个 <code>std::future</code> 对象来接收。</p><p><code>std::packaged_task</code> 则是一个可调用目标（函数、Lambda表达式或者其他函数对象，即 <code>std::function</code> 的对象）的类模板封装，这个封装主要也就是把函数对象的执行和返回值给分开。package_task 对象可以直接加参数调用，或者放在另外一个线程中调用，结果会在函数体执行完毕之后存到对应的 future 结构中。</p><p>最后是 <code>std::promise</code>，这个对象感觉有点像 placeholder 占位符的作用。promise 通过 <code>set_value()</code> 来提供数据，在 future 绑定的 promise 准备完成之前，future 的 <code>get()</code> 会阻塞所在的线程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;还是在整理秋招遭遇过的面试题的时候顺便补充一下多线程相关的东西。&lt;/p&gt;
&lt;p&gt;一开始是分了 Basic 和 Project 两个大标题，主要还是想写三个面试遇到的多线程例子的实现，然后发现前面 Basic 部分的东西写的有点多了……篇幅有点长，然后想了想还是把 Project 部分另外开一篇吧。&lt;/p&gt;
&lt;p&gt;这大概就是计划赶不上变化？……就是这么任性。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="Multithreads" scheme="https://jcf94.com/tags/Multithreads/"/>
    
  </entry>
  
  <entry>
    <title>（试图）深入理解 Cache</title>
    <link href="https://jcf94.com/2018/09/04/2018-09-04-cache/"/>
    <id>https://jcf94.com/2018/09/04/2018-09-04-cache/</id>
    <published>2018-09-04T06:11:47.000Z</published>
    <updated>2018-10-04T02:46:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>可能是因为简历里面写了个大大的 “计算机系统结构方向”，然后面的几个厂的岗位也都是偏系统方向，秋招面试的时候被好几个面试官都<strong>按在地上</strong>狂问系统方面的问题。</p><p>其中大概尤其与 Cache 有关的内容比较有代表性，于是准备根据几次回忆出来的自己和小伙伴遇到的面试题好好理一理 Cache 这块的内容。</p><p><img data-src="http://jcf94.com/download/2018-09-04-cache-hierarchy.png" alt="Hierarchy"></p><a id="more"></a><h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><p>上图是一张最基础也最常见的存储器层次结构图，表达的是计算机系统中各级存储器的速度、容量、价格等等的金字塔关系。</p><p>Cache 这个思想本身特别简单，利用的核心原理就是数据的<strong>局部性</strong>，即<strong>把最常用到的东西放在最容易拿到的地方</strong>，这种局部性<strong>即包含了数据的空间局部性也包含了使用数据的时间局部性</strong>，但实际用起来效果却是非常地好，并且广泛应用在各种不同的场合中，例如：CPU 中的 Cache 用来加速对主存数据的访问，TLB 可以看成是对虚拟地址-物理地址转换的页表的 Cache，分布式环境中有的时候也会做个本地数据缓存，也是 Cache 的思想。</p><p>其实《硬软件接口》里面已经有介绍过 Cache 了，前面也有记过：</p><ul><li><a href="http://jcf94.com/2018/02/06/2018-02-06-cod2/#CHAPTER-5-Large-and-Fast-Exploiting-Memory-Hierarchy">【计算机组成与设计.硬件/软件接口 学习笔记（二）：CHAPTER 5. Large and Fast: Exploiting Memory Hierarchy】</a></li><li><a href="http://jcf94.com/2015/10/23/2015-10-23-virtualmemory/">【硬件/软件接口 Virtual Memory】</a></li></ul><p>编程的时候 Cache 对程序员透明，写代码的人直接看到的是内存（……更准确地说看到的是虚拟的地址空间），但是 CPU 实际执行的时候访问的数据全部都是从 Cache 里面来的。每次访问一块新的内存数据时，首先检查 Cache 中是否存在，有就返回，没有就触发一次 Cache Miss，从主存把数据 load 到 Cache 中以后再返回。</p><p>然而 Cache 这个简单的想法在实际实现的时候可能会复杂的多，例如 Cache 分级，然后 Cache 还有多种数据和地址的映射关系，什么时候更新写回，地址冲突的时候怎么替换等等很多麻烦的问题都需要在实际实现的时候考虑。</p><p>这里还有两篇知乎上的小白科普文：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/31422201" target="_blank" rel="noopener">【L1，L2，L3 Cache究竟在哪里？】</a></li><li><a href="https://zhuanlan.zhihu.com/p/31859105" target="_blank" rel="noopener">【Cache是怎么组织和工作的？】</a></li></ul><h1 id="Interview-Questions"><a href="#Interview-Questions" class="headerlink" title="Interview Questions"></a>Interview Questions</h1><p>面试的时候我和我的小伙伴们在 Cache 上踩了很多坑，虽然平时工作中很少有遇到需要考虑的这么深的，也就一般很少想到 Cache 这一层的工作内容了，但是终究说起来还是自己的基础没打扎实。</p><h2 id="虚拟地址-or-物理地址？"><a href="#虚拟地址-or-物理地址？" class="headerlink" title="虚拟地址 or 物理地址？"></a>虚拟地址 or 物理地址？</h2><blockquote><p>L1 Cache 标记数据块用的是虚拟地址还是物理地址？</p></blockquote><p>往后的 Cache 里面肯定用的是物理地址，这个没有任何悬念，关键在这个最靠近 CPU 的 L1 Cache 上。</p><p>这个问题一开始我脑子里冒出来的是当时看《硬软件接口》虚拟内存那章时画的<a href="http://jcf94.com/2015/10/23/2015-10-23-virtualmemory/">那张图</a>：CPU 先拿虚拟地址去查 TLB，然后再找 Cache，那妥妥的就是物理地址了。</p><p>后来一想这本书上是以一个假想的简单数据通路结构为例进行介绍，可能细节上后来有变动，然后去翻了下《量化》，里面没有详细讲，但是看图感觉像是用的虚拟地址。</p><p>再后来去网上搜这个问题的时候，说物理地址的也有，说虚拟地址的也有，还各有各的理由，于是就更迷了。</p><p>直到我在<a href="https://zhuanlan.zhihu.com/p/31875174" target="_blank" rel="noopener">这个专栏文章中</a>看到了这张图：</p><p><img data-src="http://jcf94.com/download/2018-09-04-cache-location.jpg" alt="Cache 与 MMU 的位置关系"></p><p>MMU 是 CPU 中用于虚拟地址-物理地址转换的工作单元（主要是页表、TLB 甚至多级 TLB 等等组成），这张图中根据芯片实际实现时 MMU 和 Cache 的位置关系，则很明显 L1 Cache 可能用虚拟地址也可能用物理地址了。</p><p>那我们来考虑一下这两种实现的区别：</p><ul><li><p>Logical Cache/ Virtual Cache</p><p>感觉这种实现访问速度应该更快一点，因为不需要经过地址翻译就能够直接在 Cache 中搜索数据了。</p><p>问题是每个进程的虚拟地址空间都是独立的，如果有多个进程在同一个核上切换，则每次切换的时候都需要把 Cache 刷掉（话说我其实不太清楚这个代价会有多大，也不知道这个会不会成为制约性能的一个原因），当然，也有可能有多个虚拟地址对应到同一个物理地址上的这种情况，那这种情况要怎么处理就需要考虑更多东西了。</p><p>……真麻烦啊。</p></li><li><p>Physical Cache</p><p>查 Cache 前不管怎么样先把地址转换做了，这样前面 Logical Cache 存在的问题也就不存在了，而且也不需要一切换进程就刷一遍整个 L1 Cache，如果有多个虚拟地址对应到同一个物理地址上也无所谓。</p><p>问题大概就在于 MMU 的地址转换上，如果 TLB Miss 了，查页表还是要访问到主存上，那再查 Cache 就需要花更多的时间了。</p></li></ul><hr><p><a href="https://www.wikiwand.com/en/CPU_cache#/Address_translation" target="_blank" rel="noopener">CPU_cache 的维基百科页</a>对地址转换这块也有大段篇幅的介绍。Cache 受地址转换影响在 Latency、Aliasing、Granularity 等几个方面都需要考虑到，根据用虚拟地址还是物理地址进行查找和标记，常见的也有下面四种 Cache 实现方式：</p><ol><li><p>Physically Indexed, Physically Tagged(PIPT)</p><p>对应前面的 Physical Cache，没啥大毛病，就是慢</p></li><li><p>Virtually Indexed, Virtually Tagged(VIVT)</p><p>对应前面的 Logical Cache/ Virtual Cache，带来的麻烦问题很多</p></li><li><p>Virtually Indexed, Physically Tagged(VIPT)</p><p>可能是最好的一种实现方式了，目前市场上的一些现代处理器应该大多都是基于这种方式或者在这个基础上优化的。</p><p>由于查虚拟地址的索引和查 TLB 可以同时进行，这种方式的延迟会比 PIPT 低很多，但是实际数据还是要等到 MMU 把物理地址算出来之后对比 tag 才能确定。</p><p>另一方面由于用了物理地址作为 tag，VIVT 中可能会有的虚拟地址冲突的问题也解决了。</p></li><li><p>Physically Indexed, Virtually Tagged(PIVT)</p><p>这个…大概只会出现在文献中，实际实现的时候会集合 PIPT 和 VIVT 所有的缺点。</p></li></ol><p>另外，对 Cache 数据的查找和标记还需要考虑到全相联、组相联等不同的映射关系的实现。</p><h2 id="对-Cache-每一级的访存需要多少个-Cycle-有概念吗？"><a href="#对-Cache-每一级的访存需要多少个-Cycle-有概念吗？" class="headerlink" title="对 Cache 每一级的访存需要多少个 Cycle 有概念吗？"></a>对 Cache 每一级的访存需要多少个 Cycle 有概念吗？</h2><blockquote><p>对 Context Switch 和 Cache 访存需要大概花费多少个 Cycle 有概念吗？</p></blockquote><p>并没有……卒。</p><hr><p>看一下网上找到的答案：</p><ul><li>Register：1 Cycle</li><li>L1 Cache：3 Cycles</li><li>L2 Cache：10+ Cycles</li><li>L3 Cache：20~30+ Cycles</li><li>Main Memory：~100 Cycles</li></ul><p>同样都是用 SRAM 做的，为什么会有速度差异呢？原因大概有以下几个：</p><ol><li><p>容量大小</p><p>显而易见的是，Cache 的容量会随着级数的增加而增大。由于 Cache 需要做到随机访存，即能够直接访问到存储器的任意一个位置，在制程和设计完全一致的情况下，容量越大就需要花费更多的时间来做到随机访存（延迟跟容量的开方大致成正比）。</p></li><li><p>芯片上与 CPU 的距离</p><p>在芯片面积有限的情况下，L1 Cache 会被放置在离 CPU 核心非常近的地方，而 L2 Cache 就只能放到边缘位置了。</p><p>L1 中的指令 Cache 会在 Fetch 单元附近，数据 Cache 会在 Load/ Store 单元附近，L2 Cache 就要在 CPU 流水线外面了，L3 更远要在核外了。</p></li><li><p>具体实现的差异</p><p>L1 和 L2 在设计时的侧重点会有所区别，L1 更注重速度，而 L2 要在 L1 Miss 之后才发挥作用，因此更注重节能和容量。</p><p>查询一个地址时，L1 Cache 会把多个 Cache Line 的 tag 和数据全部取出来，然后再比较 tag 看哪一个命中或者都没命中。</p><p>而 L2 Cache 虽然也是 N 路组相联，但是比较时会先取 tag，当找到命中的之后再去把对应的数据取出来。</p><p>L3 做在核外，通常是多个核共享，因此还需要额外考虑一致性等等更多的东西。</p></li></ol><hr><p>关于 Context Switch 这点，进程和线程的切换其实都要涉及到上下文的切换。</p><p>进程切换时由于虚拟地址空间不同，因此需要切换页面映射、刷新 TLB 等等，如果是线程切换则开销会小很多。</p><h2 id="为什么需要分级？"><a href="#为什么需要分级？" class="headerlink" title="为什么需要分级？"></a>为什么需要分级？</h2><p>这个问题直接有别人解答了，感觉说的也还算清楚：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/32058808" target="_blank" rel="noopener">【Cache为什么有那么多级？为什么一级比一级大？是不是Cache越大越好？】</a></li></ul><h2 id="组成结构？"><a href="#组成结构？" class="headerlink" title="组成结构？"></a>组成结构？</h2><p>当时被问到这个问题的时候有点懵逼，不确定面试官想表达的是什么意思，我回答 “Index、tag、cache line data” 的时候被否决了……然后后来就没有答上来了。</p><p>如果指的不是全相联、组相联这种实现的话，可能想问的是硬件实现？</p><p>我对 Cache 硬件结构的了解就只是知道它是用 SRAM 做的，其他的就不懂了，卒。</p><hr><p>这一块能够找到的资料也比较模糊，最后是从<a href="https://1drv.ms/b/s!AuxK9QzqA6LQkvZxNVaponZFKqF6uA" target="_blank" rel="noopener">王齐的《浅谈 Cache Memory》</a>中找到了比较靠谱的答案：</p><p>组相联方式组织的 Cache 会分成两个部分，Tag 部分和数据部分分开存放，例如一个 8 路组相联的 Cache 结构是这样的：</p><p><img data-src="http://jcf94.com/download/2018-09-04-cache-8-way.png" alt="8-Way Set-Associative"></p><p>左边是地址 Tag 以及当前 Cache Line 的状态，右边是实际存放的数据。</p><p>这两类字段由于功能和特性不同，会使用两种不同类型的存储器来存放。Tag 阵列多使用 <strong>CAM（Content Addressable Memory）</strong>来存放，以利于并行查找，数据字段用的才是<strong>多端口多 Bank 的 SRAM</strong>。一般说的 Cache 大小也都指的是 SRAM 数据块的大小，CAM 这部分不包含在内。</p><p>CAM 对应的应该是前面结构图中一个 Set 中的多个 Way 的结构。首先根据需要访问的虚拟地址确定 Index 找到在哪个 Set 中，然后对该 Set 中的多条记录并行进行 Tag 的比对。</p><p>CAM 的基本结构如下：</p><p><img data-src="http://jcf94.com/download/2018-09-04-cache-cam.png" alt="CAM 的基本组成结构"></p><p>上图的 CAM 有 3 个 Word，分别对应一条横向的 ML（Match Line），每一个 Word 由 4 个 Bits （CAM Cell）组成。在每一列中，Bits 分别与两个 SL（Search Line）对应。</p><p>使用 CAM 进行查找时，首先需要把需要搜索的目标（Tag）放入 Search Data Register/Drivers 中，分解成多个 Bits 之后，通过 SL 发送到所有的 CAM Cell 中，每个 Cell 的 Hit/Miss 信息会向右传递给各自的 ML，最终 ML 汇总得到自己的 Hit/Miss 情况，这样就能够确定下来在当前的多个 Way 里面有没有命中的数据了。</p><p>后面更细节的就跳过了。</p><h2 id="如何测出一块未知-CPU-的-Cache-参数？"><a href="#如何测出一块未知-CPU-的-Cache-参数？" class="headerlink" title="如何测出一块未知 CPU 的 Cache 参数？"></a>如何测出一块未知 CPU 的 Cache 参数？</h2><blockquote><p>假如你是一个 Intel 的工程师，有一天你的竞争对手 AMD 推出了一款新的 CPU，然后你想要知道关于其中的 Cache 的信息，要怎么做？没有其他任何的资料，只能通过实际测试的方式。</p></blockquote><p>这个问题是紧接着上一个的，由于我被问到上一个问题的时候已经懵逼了，这题基本上完全没答上来，其实后来想想应该至少能把 Cache 的大小测出来，当时回答的时候真的是表现得太差了。</p><hr><p>参考：</p><ul><li><a href="https://blog.csdn.net/GVFDBDF/article/details/49817979" target="_blank" rel="noopener">【cache测试及其矩阵优化】</a></li><li><a href="http://igoro.com/archive/gallery-of-processor-cache-effects/" target="_blank" rel="noopener">【Gallery of Processor Cache Effects】</a></li></ul><p>根据第一篇的程序稍微改了一下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: G++</span></span><br><span class="line"><span class="comment">PROG: </span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KB (1024/4)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MB (1024 * KB)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SIZE (128 * MB)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">getTime</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timeval</span> <span class="title">tv</span>;</span></span><br><span class="line">    gettimeofday(&amp;tv, <span class="literal">NULL</span>);</span><br><span class="line">    <span class="comment">//获取秒</span></span><br><span class="line">    <span class="keyword">double</span> sec = (<span class="keyword">double</span>)tv.tv_sec;</span><br><span class="line">    <span class="comment">//获取微秒</span></span><br><span class="line">    <span class="keyword">double</span> usec = (<span class="keyword">double</span>)tv.tv_usec;</span><br><span class="line">    <span class="comment">//返回微秒数</span></span><br><span class="line">    <span class="keyword">return</span> sec * <span class="number">1000000</span> + usec;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cache_line_test</span><span class="params">(<span class="keyword">int</span> *<span class="built_in">array</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> stride=<span class="number">1</span>;stride&lt;<span class="number">64</span>*KB;stride*=<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">double</span> begin_time = getTime();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;SIZE;i+=stride)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">array</span>[i] *= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">double</span> end_time = getTime();</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Stride: %5d,  Line Size: %5d Bytes,  Average Cost: %10lf us\n"</span>, stride, stride*<span class="number">4</span>, (end_time - begin_time) / (SIZE/stride));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cache_block_test</span><span class="params">(<span class="keyword">int</span> *<span class="built_in">array</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> num=KB;num&lt;MB;num*=<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> len = num<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> tot = SIZE<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">double</span> begin_time = getTime();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;tot;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">array</span>[(i*<span class="number">16</span>) &amp; len] *= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">double</span> end_time = getTime();</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Num: %8d,  Size: %8d KB,  cost %10lf us\n"</span>, num, num*<span class="number">4</span>/<span class="number">1024</span>, end_time - begin_time);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> num=MB;num&lt;<span class="number">16</span>*MB;num+=MB)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> len = num<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> tot = SIZE<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">double</span> begin_time = getTime();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;tot;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">array</span>[(i*<span class="number">16</span>) &amp; len] *= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">double</span> end_time = getTime();</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Num: %8d,  Size: %8d MB,  cost %10lf us\n"</span>, num, num*<span class="number">4</span>/<span class="number">1024</span>/<span class="number">1024</span>, end_time - begin_time);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> *<span class="built_in">array</span> = <span class="keyword">new</span> <span class="keyword">int</span>[SIZE];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;SIZE;i++) <span class="built_in">array</span>[i] = i;</span><br><span class="line"></span><br><span class="line">    cache_line_test(<span class="built_in">array</span>);</span><br><span class="line">    cache_block_test(<span class="built_in">array</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Stride:     1,  Line Size:     4 Bytes,  Average Cost:   0.002586 us</span><br><span class="line">Stride:     2,  Line Size:     8 Bytes,  Average Cost:   0.002613 us</span><br><span class="line">Stride:     4,  Line Size:    16 Bytes,  Average Cost:   0.002622 us</span><br><span class="line">Stride:     8,  Line Size:    32 Bytes,  Average Cost:   0.003078 us</span><br><span class="line">Stride:    16,  Line Size:    64 Bytes,  Average Cost:   0.004280 us</span><br><span class="line">Stride:    32,  Line Size:   128 Bytes,  Average Cost:   0.011414 us</span><br><span class="line">Stride:    64,  Line Size:   256 Bytes,  Average Cost:   0.011412 us</span><br><span class="line">Stride:   128,  Line Size:   512 Bytes,  Average Cost:   0.007610 us</span><br><span class="line">Stride:   256,  Line Size:  1024 Bytes,  Average Cost:   0.015221 us</span><br><span class="line">Stride:   512,  Line Size:  2048 Bytes,  Average Cost:   0.015732 us</span><br><span class="line">Stride:  1024,  Line Size:  4096 Bytes,  Average Cost:   0.000000 us</span><br><span class="line">Stride:  2048,  Line Size:  8192 Bytes,  Average Cost:   0.000000 us</span><br><span class="line">Stride:  4096,  Line Size: 16384 Bytes,  Average Cost:   0.000000 us</span><br><span class="line">Stride:  8192,  Line Size: 32768 Bytes,  Average Cost:   0.000000 us</span><br><span class="line">Num:      256,  Size:        1 KB,  cost 86789.000000 us</span><br><span class="line">Num:      512,  Size:        2 KB,  cost 88739.000000 us</span><br><span class="line">Num:     1024,  Size:        4 KB,  cost 86736.000000 us</span><br><span class="line">Num:     2048,  Size:        8 KB,  cost 91756.000000 us</span><br><span class="line">Num:     4096,  Size:       16 KB,  cost 88762.000000 us</span><br><span class="line">Num:     8192,  Size:       32 KB,  cost 94745.000000 us</span><br><span class="line">Num:    16384,  Size:       64 KB,  cost 95744.000000 us</span><br><span class="line">Num:    32768,  Size:      128 KB,  cost 105752.000000 us</span><br><span class="line">Num:    65536,  Size:      256 KB,  cost 105684.000000 us</span><br><span class="line">Num:   131072,  Size:      512 KB,  cost 108742.000000 us</span><br><span class="line">Num:   262144,  Size:        1 MB,  cost 99733.000000 us</span><br><span class="line">Num:   524288,  Size:        2 MB,  cost 114694.000000 us</span><br><span class="line">Num:   786432,  Size:        3 MB,  cost 100699.000000 us</span><br><span class="line">Num:  1048576,  Size:        4 MB,  cost 130651.000000 us</span><br><span class="line">Num:  1310720,  Size:        5 MB,  cost 99732.000000 us</span><br><span class="line">Num:  1572864,  Size:        6 MB,  cost 120678.000000 us</span><br><span class="line">Num:  1835008,  Size:        7 MB,  cost 118715.000000 us</span><br><span class="line">Num:  2097152,  Size:        8 MB,  cost 151562.000000 us</span><br><span class="line">Num:  2359296,  Size:        9 MB,  cost 96772.000000 us</span><br><span class="line">Num:  2621440,  Size:       10 MB,  cost 104690.000000 us</span><br><span class="line">Num:  2883584,  Size:       11 MB,  cost 104754.000000 us</span><br><span class="line">Num:  3145728,  Size:       12 MB,  cost 152557.000000 us</span><br><span class="line">Num:  3407872,  Size:       13 MB,  cost 105716.000000 us</span><br><span class="line">Num:  3670016,  Size:       14 MB,  cost 156583.000000 us</span><br><span class="line">Num:  3932160,  Size:       15 MB,  cost 141678.000000 us</span><br></pre></td></tr></table></figure><p>我笔记本是 i7 7700HQ，在网上可以查到详细的 Cache 信息</p><table><thead><tr><th>Cache:</th><th>L1 data</th><th>L1 instruction</th><th>L2</th><th>L3</th></tr></thead><tbody><tr><td>Size:</td><td>4 x 32 KB</td><td>4 x 32 KB</td><td>4 x 256 KB</td><td>6 MB</td></tr><tr><td>Associativity:</td><td>8-way set associative</td><td>8-way set associative</td><td>4-way set associative</td><td>12-way set associative</td></tr><tr><td>Line size:</td><td>64 bytes</td><td>64 bytes</td><td>64 bytes</td><td>64 bytes</td></tr><tr><td>Comments:</td><td>Direct-mapped</td><td>Direct-mapped</td><td>Non-inclusive Direct-mapped</td><td>Inclusive Shared between all cores</td></tr></tbody></table><p>Cache Line 比较好测，上面的代码中可以明显看到从 64 Bytes 开始，平均访问时间出现跳变。</p><p>L1、L2、L3 各自的大小……说实话我觉得这样测效果并不好，变化倒是确实有，就是其中可能还有很多误差成分在。</p><p>CSAPP 上面讲 Cache 的那章用的是类似的方法，根据这样的测试结果可以画出一张“存储器山”的图。</p><h2 id="多核情况下-Cache-会有什么问题？"><a href="#多核情况下-Cache-会有什么问题？" class="headerlink" title="多核情况下 Cache 会有什么问题？"></a>多核情况下 Cache 会有什么问题？</h2><p>这个问的主要应该就是 Cache 的一致性、写回、写直达等等这些方面的内容。</p><p>另外还有一个叫做伪共享（False Sharing）的问题：</p><p><img data-src="http://jcf94.com/download/2018-09-04-cache-sbtfalse_sharing.PNG" alt="False Sharing"></p><p>多核 CPU 通常都是 L1、L2 每个核独立，共享 L3。如上图这种情况，两个核实际操作的数据是独立的，但是它们恰好在一个 Cache Line 里面，则其中一个作了修改之后，另一个的 Cache Line 也会跟着失效，引起了本来不必要的效率问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;可能是因为简历里面写了个大大的 “计算机系统结构方向”，然后面的几个厂的岗位也都是偏系统方向，秋招面试的时候被好几个面试官都&lt;strong&gt;按在地上&lt;/strong&gt;狂问系统方面的问题。&lt;/p&gt;
&lt;p&gt;其中大概尤其与 Cache 有关的内容比较有代表性，于是准备根据几次回忆出来的自己和小伙伴遇到的面试题好好理一理 Cache 这块的内容。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://jcf94.com/download/2018-09-04-cache-hierarchy.png&quot; alt=&quot;Hierarchy&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Computer Architecture" scheme="https://jcf94.com/categories/Computer-Architecture/"/>
    
    
      <category term="Cache" scheme="https://jcf94.com/tags/Cache/"/>
    
      <category term="面试" scheme="https://jcf94.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>理一个 LCA 模版</title>
    <link href="https://jcf94.com/2018/06/20/2018-06-20-lca/"/>
    <id>https://jcf94.com/2018/06/20/2018-06-20-lca/</id>
    <published>2018-06-20T13:19:07.000Z</published>
    <updated>2018-07-10T14:54:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>工作累了头昏脑涨，刷个题冷静一下。</p><p>顺手来理一下 LCA 的板子。</p><hr><h1 id="Tarjan-离线-LCA"><a href="#Tarjan-离线-LCA" class="headerlink" title="Tarjan 离线 LCA"></a>Tarjan 离线 LCA</h1><p>前面在 <a href="/2017/09/27/2017-09-27-tarjan/">【Tarjan 大佬的算法们】</a> 中提到过他的离线 LCA 算法，就从这里开始。</p><a id="more"></a><p>原始算法具体见前文吧，还引用了别人的一个链接，里面有个动画演示的挺清楚的。</p><p>离线 LCA 的关键在于 dfs 遍历整个树的过程中，对于被询问的点对 <code>(v, w)</code>，要求其中一个点要先被访问过，然后再遍历到另一个点则可以通过查询前一个点所属的并查集来找到它们的 LCA，例如 v 先被访问，接下来遍历到 w，则它们的 LCA 是 <code>getfather(v)</code>，反正若 w 先被访问，接下来遍历到 v，则它们的 LCA 是 <code>getfather(w)</code>。查询结束再将 v 和 w 合并到同一个集合中（向树上父节点的方向合并）。</p><p>Tarjan 论文中的原始算法的树节点应该是保证有序的，然后通过先处理每一对 <code>(vi, wi)</code>，使得 <code>vi &lt; wi</code> 来保证每次访问到 wi 时，它对应的 vi 都被访问过。</p><p>在通用的树中就只能在每一层 dfs 中询问跟当前层搜的父节点相关的边来查找这种对应关系了。</p><p>记两道模版题：</p><h2 id="HDU-2586"><a href="#HDU-2586" class="headerlink" title="HDU 2586"></a>HDU 2586</h2><p>【题意】</p><p>给出一棵树，询问树上的两个点，要求回答两个点之间的最近距离。</p><p>【分析】</p><p>树首先是任意给的，假定我们建出来的树根是 R，则对于一次询问的点对 <code>(a, b)</code>，可以在这棵树上找到它们的 LCA，记为 c。则 a 和 b 之间的距离就是：</p><p>$$Dis(R, a) - Dis(R, c) + Dis(R, b) - Dis(R, c) = Dis(R, a) + Dis(R, b) - 2*Dis(R, c)$$</p><p>在找 LCA 的 dfs 过程中顺手把每个节点到根节点的距离记出来即可。</p><h2 id="HDU-2874"><a href="#HDU-2874" class="headerlink" title="HDU 2874"></a>HDU 2874</h2><p>【题意】</p><p>跟前面类似，加了个条件是可能有多棵树。</p><p>【分析】</p><p>每对点对是否在同一棵树上可以在读取数据的时候直接用并查集判断联通性记下来，之后对所有未访问过的点做 Tarjan LCA 即可。</p><p>【模版】</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: G++</span></span><br><span class="line"><span class="comment">PROG: hdu2874</span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> father[<span class="number">10010</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getfather</span><span class="params">(<span class="keyword">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (father[x] != x) father[x] = getfather(father[x]);</span><br><span class="line">    <span class="keyword">return</span> father[x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">link</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    father[getfather(x)] = getfather(y);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">nod</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> a, b, c;</span><br><span class="line">&#125; node;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">op</span><span class="params">(node a, node b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.a == b.a) <span class="keyword">return</span> a.b &lt; b.b;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> a.a &lt; b.a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">node edge[<span class="number">20010</span>];</span><br><span class="line"><span class="keyword">int</span> start[<span class="number">10010</span>], num[<span class="number">10010</span>], dist[<span class="number">10010</span>];</span><br><span class="line"><span class="keyword">bool</span> flag[<span class="number">10010</span>];</span><br><span class="line"></span><br><span class="line">node q[<span class="number">2000010</span>];</span><br><span class="line"><span class="keyword">int</span> qstart[<span class="number">10010</span>], qnum[<span class="number">10010</span>];</span><br><span class="line"><span class="keyword">int</span> res[<span class="number">1000010</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">tarjan</span><span class="params">(<span class="keyword">int</span> now)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    father[now] = now;</span><br><span class="line">    flag[now] = <span class="literal">true</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;num[now];i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> next = edge[start[now]+i].b;</span><br><span class="line">        <span class="keyword">if</span> (!flag[next])</span><br><span class="line">        &#123;</span><br><span class="line">            dist[next] = dist[now]+edge[start[now]+i].c;</span><br><span class="line">            tarjan(next);</span><br><span class="line">            link(next, now);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;qnum[now];i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> next = q[qstart[now]+i].b;</span><br><span class="line">        <span class="keyword">if</span> (res[q[qstart[now]+i].c] != <span class="number">-1</span> &amp;&amp; flag[next])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> lca = getfather(next);</span><br><span class="line">            res[q[qstart[now]+i].c] = dist[now]+dist[next]<span class="number">-2</span>*dist[lca];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    freopen(<span class="string">"in.txt"</span>, <span class="string">"r"</span>, <span class="built_in">stdin</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> n, m, c;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;n, &amp;m, &amp;c) != EOF)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++) father[i] = i;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> index = i &lt;&lt; <span class="number">1</span>;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;edge[index].a, &amp;edge[index].b, &amp;edge[index].c);</span><br><span class="line">            edge[index+<span class="number">1</span>].b = edge[index].a;</span><br><span class="line">            edge[index+<span class="number">1</span>].a = edge[index].b;</span><br><span class="line">            edge[index+<span class="number">1</span>].c = edge[index].c;</span><br><span class="line">            link(edge[index].a, edge[index].b);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(start, <span class="number">0</span>, <span class="keyword">sizeof</span>(start));</span><br><span class="line">        <span class="built_in">memset</span>(num, <span class="number">0</span>, <span class="keyword">sizeof</span>(num));</span><br><span class="line">        m = m &lt;&lt; <span class="number">1</span>;</span><br><span class="line">        sort(&amp;edge[<span class="number">0</span>], &amp;edge[m], op);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>,o=<span class="number">-1</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (o!=edge[i].a)</span><br><span class="line">            &#123;</span><br><span class="line">                o = edge[i].a;</span><br><span class="line">                start[o] = i;</span><br><span class="line">            &#125;</span><br><span class="line">            num[o]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">memset</span>(res, <span class="number">0</span>, <span class="keyword">sizeof</span>(res));</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;c;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> index = i &lt;&lt; <span class="number">1</span>;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;q[index].a, &amp;q[index].b);</span><br><span class="line">            q[index+<span class="number">1</span>].a = q[index].b;</span><br><span class="line">            q[index+<span class="number">1</span>].b = q[index].a;</span><br><span class="line">            q[index].c = i;</span><br><span class="line">            q[index+<span class="number">1</span>].c = i;</span><br><span class="line">            <span class="keyword">if</span> (getfather(q[index].a) != getfather(q[index].b)) res[i] = <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(qstart, <span class="number">0</span>, <span class="keyword">sizeof</span>(qstart));</span><br><span class="line">        <span class="built_in">memset</span>(qnum, <span class="number">0</span>, <span class="keyword">sizeof</span>(qnum));</span><br><span class="line">        c = c &lt;&lt; <span class="number">1</span>;</span><br><span class="line">        sort(&amp;q[<span class="number">0</span>], &amp;q[c], op);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>,o=<span class="number">-1</span>;i&lt;c;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (o != q[i].a)</span><br><span class="line">            &#123;</span><br><span class="line">                o = q[i].a;</span><br><span class="line">                qstart[o] = i;</span><br><span class="line">            &#125;</span><br><span class="line">            qnum[o]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">memset</span>(flag, <span class="number">0</span>, <span class="keyword">sizeof</span>(flag));</span><br><span class="line">        <span class="built_in">memset</span>(dist, <span class="number">0</span>, <span class="keyword">sizeof</span>(dist));</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">        <span class="keyword">if</span> (!flag[i]) tarjan(i);</span><br><span class="line"></span><br><span class="line">        c = c &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;c;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (res[i] == <span class="number">-1</span>) <span class="built_in">printf</span>(<span class="string">"Not connected\n"</span>);</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, res[i]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>dfs 过程中是先询问边还是先进行下一层的 dfs 不影响结果。</p><blockquote><p>话说虽然前向星写习惯了，不过看人家写的数组邻接链表也挺好看的。</p></blockquote><h1 id="动态树（LCT）"><a href="#动态树（LCT）" class="headerlink" title="动态树（LCT）"></a>动态树（LCT）</h1><p>前面 <a href="/2015/04/27/2015-04-27-HDU-3966/">【HDU 3966】</a> 用过一次 LCT 和树链剖分来维护树上一条路径上的点值，翻回去看的时候……妈呀，以前题解怎么都没好好写，自己都看不懂了，顺便拿到这里重新理一下思路。</p><p>LCT 的核心思路是用 splay 森林来维护 Preferred Path，核心操作是调整 Preferred Path 的 Access 操作，对于 LCT 来说，找 a、b 两点的 LCA 只需要两步：</p><ol><li>首先 <code>Access(a)</code> ，此时 a 所在的 splay 树即为从根节点到 a 点所构成的 Preferred Path；</li><li>之后 <code>Access(b)</code>，找到从根节点到 b 点所构成的 Preferred Path 之后，此时 Splay 的根即为 a 和 b 的 LCA。</li></ol><p>当然也有可能两次 Access 操作之后的 Preferred Path 的根不是同一个，那就意味着这两个点是分属于两棵不同的树，不存在 LCA，特判一下就好了。</p><p>所以上面那题的 LCT 模版是：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: G++</span></span><br><span class="line"><span class="comment">PROG: HDU2874-LCT</span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">nod</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> a, b, c;</span><br><span class="line">&#125; node;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">op</span><span class="params">(node a, node b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.a == b.a) <span class="keyword">return</span> a.b &lt; b.b;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> a.a &lt; b.a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">node edge[<span class="number">20010</span>];</span><br><span class="line"><span class="keyword">int</span> start[<span class="number">10010</span>], num[<span class="number">10010</span>], dist[<span class="number">10010</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sons[<span class="number">10010</span>][<span class="number">2</span>];</span><br><span class="line"><span class="keyword">int</span> father[<span class="number">10010</span>];</span><br><span class="line"><span class="keyword">bool</span> root[<span class="number">10010</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bfs</span><span class="params">(<span class="keyword">int</span> s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">    q.push(s);</span><br><span class="line">    root[s] = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">while</span> (!q.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> now = q.front();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;num[now];i++)</span><br><span class="line">        <span class="keyword">if</span> (!root[edge[start[now]+i].b])</span><br><span class="line">        &#123;</span><br><span class="line">            father[edge[start[now]+i].b] = now;</span><br><span class="line">            dist[edge[start[now]+i].b] = dist[now] + edge[start[now]+i].c;</span><br><span class="line">            root[edge[start[now]+i].b] = <span class="literal">true</span>;</span><br><span class="line">            q.push(edge[start[now]+i].b);</span><br><span class="line">        &#125;</span><br><span class="line">        q.pop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">rotate</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> w)</span> <span class="comment">//rotate(node,0/1)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> y=father[x];</span><br><span class="line"></span><br><span class="line">    sons[y][!w]=sons[x][w];</span><br><span class="line">    <span class="keyword">if</span> (sons[x][w]) father[sons[x][w]]=y;</span><br><span class="line">    father[x]=father[y];</span><br><span class="line">    <span class="keyword">if</span> (father[y]&amp;&amp;(!root[y])) sons[father[y]][y==sons[father[y]][<span class="number">1</span>]]=x;</span><br><span class="line">    sons[x][w]=y;</span><br><span class="line">    father[y]=x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (root[y])</span><br><span class="line">    &#123;</span><br><span class="line">        root[x]=<span class="literal">true</span>;</span><br><span class="line">        root[y]=<span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">splay</span><span class="params">(<span class="keyword">int</span> x)</span> <span class="comment">//splay(node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(!root[x])</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (root[father[x]]) rotate(x,x==sons[father[x]][<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">else</span> </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> t=father[x];</span><br><span class="line">            <span class="keyword">int</span> w=(sons[father[t]][<span class="number">0</span>]==t);</span><br><span class="line">            <span class="keyword">if</span> (sons[t][w]==x)</span><br><span class="line">            &#123;</span><br><span class="line">                rotate(x,!w);</span><br><span class="line">                rotate(x,w);</span><br><span class="line">            &#125; <span class="keyword">else</span> </span><br><span class="line">            &#123;</span><br><span class="line">                rotate(t,w);</span><br><span class="line">                rotate(x,w);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">access</span><span class="params">(<span class="keyword">int</span> v)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> u=v;</span><br><span class="line">    v=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(u)</span><br><span class="line">    &#123;</span><br><span class="line">        splay(u);</span><br><span class="line">        root[sons[u][<span class="number">1</span>]]=<span class="literal">true</span>;</span><br><span class="line">        sons[u][<span class="number">1</span>]=v;</span><br><span class="line">        root[v]=<span class="literal">false</span>;</span><br><span class="line">        v=u;</span><br><span class="line">        u=father[u];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">check</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    access(x);</span><br><span class="line">    <span class="keyword">int</span> root_x = x;</span><br><span class="line">    <span class="keyword">while</span> (father[root_x]) root_x = father[root_x];</span><br><span class="line">    <span class="keyword">while</span> (sons[root_x][<span class="number">0</span>]) root_x = sons[root_x][<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> u = y, v = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(u)</span><br><span class="line">    &#123;</span><br><span class="line">        splay(u);</span><br><span class="line">        root[sons[u][<span class="number">1</span>]] = <span class="literal">true</span>;</span><br><span class="line">        sons[u][<span class="number">1</span>] = v;</span><br><span class="line">        root[v] = <span class="literal">false</span>;</span><br><span class="line">        v = u;</span><br><span class="line">        u = father[u];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> root_y = v;</span><br><span class="line">    <span class="keyword">while</span> (sons[root_y][<span class="number">0</span>]) root_y = sons[root_y][<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">if</span> (root_x != root_y) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> dist[x] + dist[y] - <span class="number">2</span>*dist[v];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    freopen(<span class="string">"in.txt"</span>, <span class="string">"r"</span>, <span class="built_in">stdin</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> n, m, c;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;n, &amp;m, &amp;c) != EOF)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> index = i &lt;&lt; <span class="number">1</span>;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;edge[index].a, &amp;edge[index].b, &amp;edge[index].c);</span><br><span class="line">            edge[index+<span class="number">1</span>].b = edge[index].a;</span><br><span class="line">            edge[index+<span class="number">1</span>].a = edge[index].b;</span><br><span class="line">            edge[index+<span class="number">1</span>].c = edge[index].c;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(start, <span class="number">0</span>, <span class="keyword">sizeof</span>(start));</span><br><span class="line">        <span class="built_in">memset</span>(num, <span class="number">0</span>, <span class="keyword">sizeof</span>(num));</span><br><span class="line">        m = m &lt;&lt; <span class="number">1</span>;</span><br><span class="line">        sort(&amp;edge[<span class="number">0</span>], &amp;edge[m], op);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>,o=<span class="number">-1</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (o!=edge[i].a)</span><br><span class="line">            &#123;</span><br><span class="line">                o = edge[i].a;</span><br><span class="line">                start[o] = i;</span><br><span class="line">            &#125;</span><br><span class="line">            num[o]++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(root, <span class="number">0</span>, <span class="keyword">sizeof</span>(root));</span><br><span class="line">        <span class="built_in">memset</span>(dist, <span class="number">0</span>, <span class="keyword">sizeof</span>(dist));</span><br><span class="line">        <span class="built_in">memset</span>(father, <span class="number">0</span>, <span class="keyword">sizeof</span>(father));</span><br><span class="line">        <span class="built_in">memset</span>(sons, <span class="number">0</span>, <span class="keyword">sizeof</span>(sons));</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">        <span class="keyword">if</span> (!root[i]) bfs(i);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;c;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> x, y;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;x, &amp;y);</span><br><span class="line">            <span class="keyword">int</span> res = check(x, y);</span><br><span class="line">            <span class="keyword">if</span> (res == <span class="number">-1</span>) <span class="built_in">printf</span>(<span class="string">"Not connected\n"</span>);</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, res);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从最后的求解过程上来看，应该比离线的要更顺一点，毕竟在线算法不用多考虑记录输出顺序什么的。</p><h1 id="树链剖分"><a href="#树链剖分" class="headerlink" title="树链剖分"></a>树链剖分</h1><p>还是看前面 <a href="/2015/04/27/2015-04-27-HDU-3966/">【HDU 3966】</a> 的时候……发现树链剖分忘得差不多了，顺手再补一个树剖找 LCA 的模版吧。</p><p>树链剖分的原理是把整棵树按照一条一条树边组成的链划开，每条链相当于一个区间，那对树上的某条路径的操作就成了对树上的<strong>一条或者多条树链</strong>的操作了，具体维护区间的部分可以用树状数组啊、线段树啊什么的来做。</p><p>划分树链的基本思路是对整棵树进行轻重边划分，定义 <code>size(x)</code> 是以 x 为根的子树的节点个数，这里的重边就是 x 和它节点数更多的一个子节点（size 更大的一个子节点）组成的边。</p><p>这里会有两个性质：</p><ol><li>若 <code>(father, son)</code> 是一条轻边，则 <code>size(son) &lt;= size(father)/2</code></li><li>从树根到某一个点的路径上的轻边的个数不会超过 <code>O(logn)</code></li></ol><p>划分好轻重边之后，从根节点开始把所有连着的重边连起来，就成了一条重链，重链就是前面说的需要从树上剖出来的树链啦。</p><p>这个过程可以用一次 bfs 加两次队列遍历来完成：</p><ol><li>首先 bfs 构图，把所有节点的父节点 father 和距离根节点的层数 level 标记好；</li><li>根据前面 bfs 过程中记录下来的队列顺序，反向遍历一遍，自底向上维护好每个节点的 size，标记重边也是在这里完成；</li><li>根据前面 bfs 过程中记录下来的队列顺序，正向遍历一遍，自顶向下对每条重边进行剖分，大概是一个切下来一条链放好，再切下一条链放好这样的过程。</li></ol><p>那么 LCA 要怎么找呢？</p><p>在树链结构上找 a、b 两点的 LCA 即走完各自所在的树链，沿着它的轻边向上找，直到找到各自路径上的两个点在同一条树链上，那么深度较浅的那个点就是 a、b 两点的 LCA 了。</p><p>由于从树根到某一个点的路径上的轻边的个数不会超过 <code>O(logn)</code> 这个性质，每次找 LCA 的复杂度是 <code>O(logn)</code>。</p><p>模版如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* ***********************************************</span></span><br><span class="line"><span class="comment">MYID: Chen Fan</span></span><br><span class="line"><span class="comment">LANG: G++</span></span><br><span class="line"><span class="comment">PROG: HDU2874-TreeCut</span></span><br><span class="line"><span class="comment">************************************************ */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">nod</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> a, b, c;</span><br><span class="line">&#125; node;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">op</span><span class="params">(node a, node b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.a == b.a) <span class="keyword">return</span> a.b &lt; b.b;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> a.a &lt; b.a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">node edge[<span class="number">20010</span>];</span><br><span class="line"><span class="keyword">int</span> start[<span class="number">10010</span>], num[<span class="number">10010</span>], dist[<span class="number">10010</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> son[<span class="number">10010</span>], father[<span class="number">10010</span>], level[<span class="number">10010</span>], size[<span class="number">10010</span>], top[<span class="number">10010</span>], pos[<span class="number">10010</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> q[<span class="number">10010</span>];</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bfs</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> head = <span class="number">0</span>, tail = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">memset</span>(level, <span class="number">0</span>, <span class="keyword">sizeof</span>(level));</span><br><span class="line">    <span class="built_in">memset</span>(father, <span class="number">0</span>, <span class="keyword">sizeof</span>(father));</span><br><span class="line">    <span class="built_in">memset</span>(dist, <span class="number">0</span>, <span class="keyword">sizeof</span>(dist));</span><br><span class="line">    <span class="built_in">memset</span>(son, <span class="number">0</span>, <span class="keyword">sizeof</span>(son));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> x=<span class="number">1</span>;x&lt;=n;x++)</span><br><span class="line">    <span class="keyword">if</span> (!level[x])</span><br><span class="line">    &#123;</span><br><span class="line">        q[head] = x;</span><br><span class="line">        level[x] = <span class="number">1</span>;</span><br><span class="line">        tail = head;</span><br><span class="line">        <span class="keyword">while</span> (head &lt;= tail)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> now = q[head];</span><br><span class="line">            size[now] = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;num[now];i++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">int</span> next = edge[start[now]+i].b;</span><br><span class="line">                <span class="keyword">if</span> (next != father[now])</span><br><span class="line">                &#123;</span><br><span class="line">                    father[next] = now;</span><br><span class="line">                    dist[next] = dist[now]+edge[start[now]+i].c;</span><br><span class="line">                    level[next] = level[now]+<span class="number">1</span>;</span><br><span class="line">                    tail++;</span><br><span class="line">                    q[tail] = next;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            head ++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=n<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> now = q[i];</span><br><span class="line">        <span class="keyword">if</span> (father[now])</span><br><span class="line">        &#123;</span><br><span class="line">            size[father[now]]+=size[now];</span><br><span class="line">            <span class="keyword">if</span> (son[father[now]]==<span class="number">0</span> || size[now]&gt;size[son[father[now]]])</span><br><span class="line">            son[father[now]] = now;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> tot = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> now = q[i];</span><br><span class="line">        <span class="keyword">if</span> (son[father[now]] == now) top[now] = top[father[now]];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            top[now] = now;</span><br><span class="line">            <span class="keyword">while</span> (now)</span><br><span class="line">            &#123;</span><br><span class="line">                pos[now] = tot++;</span><br><span class="line">                now = son[now];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">lca</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (top[x] != top[y])</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (level[top[x]] &lt; level[top[y]]) swap(x, y);</span><br><span class="line">        x = father[top[x]];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (x==<span class="number">0</span> &amp;&amp; y==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">if</span> (level[x] &gt; level[y]) swap(x, y);</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    freopen(<span class="string">"in.txt"</span>, <span class="string">"r"</span>, <span class="built_in">stdin</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> n, m, c;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;n, &amp;m, &amp;c) != EOF)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> index = i &lt;&lt; <span class="number">1</span>;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;edge[index].a, &amp;edge[index].b, &amp;edge[index].c);</span><br><span class="line">            edge[index+<span class="number">1</span>].b = edge[index].a;</span><br><span class="line">            edge[index+<span class="number">1</span>].a = edge[index].b;</span><br><span class="line">            edge[index+<span class="number">1</span>].c = edge[index].c;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(start, <span class="number">0</span>, <span class="keyword">sizeof</span>(start));</span><br><span class="line">        <span class="built_in">memset</span>(num, <span class="number">0</span>, <span class="keyword">sizeof</span>(num));</span><br><span class="line">        m = m &lt;&lt; <span class="number">1</span>;</span><br><span class="line">        sort(&amp;edge[<span class="number">0</span>], &amp;edge[m], op);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>,o=<span class="number">-1</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (o!=edge[i].a)</span><br><span class="line">            &#123;</span><br><span class="line">                o = edge[i].a;</span><br><span class="line">                start[o] = i;</span><br><span class="line">            &#125;</span><br><span class="line">            num[o]++;</span><br><span class="line">        &#125;</span><br><span class="line">        bfs(n);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;c;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> x, y;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;x, &amp;y);</span><br><span class="line">            <span class="keyword">int</span> res = lca(x, y);</span><br><span class="line">            <span class="keyword">if</span> (res == <span class="number">-1</span>) <span class="built_in">printf</span>(<span class="string">"Not connected\n"</span>);</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, dist[x] + dist[y] - <span class="number">2</span>*dist[res]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>话说从 HDU 提交的结果上来看，树链剖分是最快的，LCT 次之，Tarjan 离线最慢。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;工作累了头昏脑涨，刷个题冷静一下。&lt;/p&gt;
&lt;p&gt;顺手来理一下 LCA 的板子。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&quot;Tarjan-离线-LCA&quot;&gt;&lt;a href=&quot;#Tarjan-离线-LCA&quot; class=&quot;headerlink&quot; title=&quot;Tarjan 离线 LCA&quot;&gt;&lt;/a&gt;Tarjan 离线 LCA&lt;/h1&gt;&lt;p&gt;前面在 &lt;a href=&quot;/2017/09/27/2017-09-27-tarjan/&quot;&gt;【Tarjan 大佬的算法们】&lt;/a&gt; 中提到过他的离线 LCA 算法，就从这里开始。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法学习" scheme="https://jcf94.com/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Tarjan" scheme="https://jcf94.com/tags/Tarjan/"/>
    
      <category term="LCA" scheme="https://jcf94.com/tags/LCA/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 拆包（八）：Dynamic Control Flow in Large-Scale Machine Learning</title>
    <link href="https://jcf94.com/2018/06/11/2018-06-11-tfunpacking8/"/>
    <id>https://jcf94.com/2018/06/11/2018-06-11-tfunpacking8/</id>
    <published>2018-06-11T00:44:45.000Z</published>
    <updated>2018-10-21T02:28:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>前篇：</p><ul><li><a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a></li><li><a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a></li><li><a href="/2018/02/28/2018-02-28-tfunpacking3/">TensorFlow 拆包（三）：Graph 和 Node</a></li><li><a href="/2018/03/07/2018-03-07-tfunpacking4/">TensorFlow 拆包（四）：Device</a></li><li><a href="/2018/03/09/2018-03-09-tfunpacking5/">TensorFlow 拆包（五）：Distributed</a></li><li><a href="/2018/03/12/2018-03-12-tfunpacking6/">TensorFlow 拆包（六）：RDMA</a></li><li><a href="/2018/04/10/2018-04-10-tfunpacking7/">TensorFlow 拆包（七）：Profiling 踩坑 &amp; Benchmark</a></li></ul><p>严格上来说本篇不应该算在拆包里面，因为记的是 TF 团队最近发的一篇论文里面的东西。</p><p>前面拆包的第二篇记过关于 TensorFlow 中的数据流模型实现，实际上这套数据流模型已经是非常完备的，只是目前大家用 Python 搭出来的简单网络形式还很难把它的真正潜力发挥出来。</p><blockquote><p>正当我们往这个方向做的时候，得，Google 发论文了。</p></blockquote><a id="more"></a><hr><p>这篇 <a href="https://dl.acm.org/citation.cfm?id=3190551" target="_blank" rel="noopener">Dynamic Control Flow in Large-Scale Machine Learning</a> 发表在 EuroSys 18 上，系统结构方向的 B 类会议。</p><p>其实文中所提到的几乎所有内容都是 <strong>TensorFlow 原有</strong>的，或者说 TensorFlow 当初设计架构的时候就已经考虑到了未来这种使用方式的需求，这篇文章只是整理了一下这部分的设计思路（内容大部分跟以前发的控制流白皮书是一致的，见 TF 拆包第二篇），然后做了一定的测试，从实践上证明给大家看这样做是有效的。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>首先提了一下深度学习中对控制流的需求，主要是像 RNN、MoEs 这样的任务中会明确地需要一些控制流的支持。但从更宏观的角度来看，使用动态控制流对任何应用都是有用的，理论上可以比较好地地把计算和通信部分给 overlap 开，尤其对提高<strong>异构系统</strong>（CPU、GPU、TPU等等）的计算效率是有很大的好处的。</p><p>目前常见的一些机器学习框架基本上都是用数据流图的方式来组织计算。</p><p>关于如何实现数据流的控制部分，主要有两种方式：</p><ol><li>in-graph 方式：例如 TensorFlow 和 Theano，控制流部分可以作为一个 op 嵌入到计算图中；</li><li>out-of-graph 方式：这也是大多数框架的常规方式，包括 MxNet、torch、Caffe 以及 TensorFlow 的常规用法，控制流部分由更上层的 host 语言来完成（主要指 Python）</li></ol><blockquote><p>除了 TensorFlow 以外，别的框架似乎都很少用数据流这个词来指代自己的设计，可能原因就在这里？其他框架虽然整体计算还是以数据流图的形式做的，但并不是真正用一套数据流的运行时去支撑的。</p></blockquote><p>用 TensorFlow 来举例，方式 2 的写法通常是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python <span class="keyword">for</span> i <span class="keyword">in</span> range(xxx):</span><br><span class="line">    xxx</span><br><span class="line">    sess.run(train_step)</span><br><span class="line">    xxx</span><br></pre></td></tr></table></figure><p>多轮控制是写在 Python 层的代码中，每一次循环只跑训练的一步。</p><blockquote><p>恐怕我们见到的大部分 TF 代码都是这个样子的吧。</p></blockquote><p>方式 1 则是控制部分已经是图的一部分了，那最后我们只要 <code>sess.run(total_train_step)</code> 一次，就能够达到跟前面一样的训练效果。</p><p>单一的计算图更便于进行全图的优化，且这种实现能保证整个计算过程都停留在运行时里面（而不是像原先那样，跑一轮进退一次运行时，再跑一轮再进退一次运行时），减少很多不必要的开销。</p><p>数据流运行时的特性是<strong>一旦某个 op 的依赖都满足了，它就马上可以被调度执行了</strong>，在 out-of-graph 方式中，这种数据流的调度粒度只限定在一次 step 中，而 in-graph 方式甚至能把并行性扩展到多次 step 间，这样就能够最大程度地挖掘<strong>数据流异步、并行的能力</strong>了。</p><p>最初的 TensorFlow 白皮书中也有介绍过关于数据流部分的实现，但是并没有给出详细的设计方案以及测试结果，这篇文章就是把这部分补上。</p><p>总的来说，本文的内容包括：</p><ul><li>In-graph 动态控制流的设计，包括自动求导部分</li><li>In-graph 动态控制流在 TensorFlow 中的具体实现，包括在多种异构设备上分发的能力</li><li>对动态控制流性能的测试，并且分析多种不同选择带来的影响</li><li>关于如何用好动态控制流的使用经验</li></ul><blockquote><p>话说前两部分都是 TensorFlow 原有的。。。。。。</p></blockquote><h1 id="Design-and-Implementation"><a href="#Design-and-Implementation" class="headerlink" title="Design and Implementation"></a>Design and Implementation</h1><p>2、3、4、5 章的大部分内容与 <a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a> 中记录的类似，就不多重复了。</p><p>需要额外提一下的是，由于跨 step 的 op 有可能被并行执行，这也就意味着可能要用上更多的内存。TensorFlow 的控制流中也考虑了内存的问题，建立在 GPU 上的 frame 如果使用的显存超过某个上限则会自动做与 CPU 的内存切换的动作，把不用的部分数据换出去，把接下来要用的数据换进来。</p><p>例如 <code>tf.while_loop()</code> 的函数接口中就有个 <code>swap_memory</code> 的参数。</p><blockquote><p>6666666666….</p></blockquote><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>说实话，这篇文章的测试结果部分我觉得写的有点乱。</p><blockquote><p>前面都是搬以前原有的内容，然后在本文的重点部分又写的这么乱，Google 的大佬们你们是认真的吗？</p></blockquote><p>测试的系统配置是 Intel 服务器配上 K40 和以太网，每个节点一块卡，某些例子中用到了 8 卡的 DGX-1 V100。</p><hr><p>一开始的两个测试用的是构造出来的模拟算例。</p><p>图 11 的结果感觉有点迷。</p><p>图 12 是模拟 RNN 的结构，把一个类似 8 层的 RNN 计算分布在 8 块卡上，把 <code>tf.while_loop()</code> 支持的并行 iteration 数从 1 调到 32，可以发现并行性发挥出来之后效果确实是挺好的，最高大约有 5 倍左右的性能提升。并行 iteration 数为 1 的时候其实就相当于跟 out-of-graph 一样。</p><p><img data-src="http://jcf94.com/download/2018-06-11-tfunpacking8-p12.png" alt="图12"></p><p>后面模型并行的测试是把一个实际的 8 层 LSTM 分布在 8 块卡上，具体的并行方式与图 12 的测试类似。在 1~8 块卡上分别测试，加速比也还可以。</p><hr><p>接下来对一个单层 LSTM 的测试是对比是否开启内存交换。不开内存交换时，序列长度加到 600 就出现超内存的现象了，而开启内存交换则可以在保证能跑的前提下还不会损失性能。</p><p>从追踪出来的 profiling 结果中也能看到，在这种计算模式下内存拷贝和 GPU 计算 overlap 得比较好，这也是性能不受影响的重要原因。</p><hr><p>再下一个测试是固定 LSTM 的序列长度为 200，调整 Batch Size 的大小来对比动态 RNN 和手动循环展开的效果。动态 RNN 稍微损失了一点点性能，但是差距不大。</p><p>另一方面动态 RNN 比手动做循环展开在内存方面有更大的优势，类似上一个测试，动态 RNN 开了内存交换之后可以跑更大的 Batch Size。</p><hr><p>最后是对 DQN 强化学习网络的测试，尽管 DQN 现在已经不用了被其他更好的方法替代了（？？？），还是希望能从它的测试中展现一下动态控制流的效果。</p><p>DQN 中包含了多个网络，根据不同的情况需要做出许多不同的操作。使用动态控制流的方法把所有的操作都包含在一个计算图中之后，最终能够比原始情况得到 21% 的性能提升。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>总结来看，这篇文章重新整理了有关 TensorFlow 中控制流部分的实现思路，证明 in-graph 方式的纯数据流实现是有意义的。但是我对它的测试部分并不太满意，用到的是模拟的 workload，说服力不够，并且感觉测试的内容还是偏少。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前篇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/13/2018-01-13-tfunpacking/&quot;&gt;TensorFlow 拆包（一）：Session.Run()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/23/2018-01-23-tfunpacking2/&quot;&gt;TensorFlow 拆包（二）：TF 的数据流模型实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/02/28/2018-02-28-tfunpacking3/&quot;&gt;TensorFlow 拆包（三）：Graph 和 Node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/07/2018-03-07-tfunpacking4/&quot;&gt;TensorFlow 拆包（四）：Device&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/09/2018-03-09-tfunpacking5/&quot;&gt;TensorFlow 拆包（五）：Distributed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/12/2018-03-12-tfunpacking6/&quot;&gt;TensorFlow 拆包（六）：RDMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/04/10/2018-04-10-tfunpacking7/&quot;&gt;TensorFlow 拆包（七）：Profiling 踩坑 &amp;amp; Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;严格上来说本篇不应该算在拆包里面，因为记的是 TF 团队最近发的一篇论文里面的东西。&lt;/p&gt;
&lt;p&gt;前面拆包的第二篇记过关于 TensorFlow 中的数据流模型实现，实际上这套数据流模型已经是非常完备的，只是目前大家用 Python 搭出来的简单网络形式还很难把它的真正潜力发挥出来。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;正当我们往这个方向做的时候，得，Google 发论文了。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TensorFlow" scheme="https://jcf94.com/tags/TensorFlow/"/>
    
      <category term="Dataflow" scheme="https://jcf94.com/tags/Dataflow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 拆包（七）：Profiling 踩坑 &amp; Benchmark</title>
    <link href="https://jcf94.com/2018/04/10/2018-04-10-tfunpacking7/"/>
    <id>https://jcf94.com/2018/04/10/2018-04-10-tfunpacking7/</id>
    <published>2018-04-10T05:26:52.000Z</published>
    <updated>2018-10-30T08:37:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>接上篇：</p><ul><li><a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a></li><li><a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a></li><li><a href="/2018/02/28/2018-02-28-tfunpacking3/">TensorFlow 拆包（三）：Graph 和 Node</a></li><li><a href="/2018/03/07/2018-03-07-tfunpacking4/">TensorFlow 拆包（四）：Device</a></li><li><a href="/2018/03/09/2018-03-09-tfunpacking5/">TensorFlow 拆包（五）：Distributed</a></li><li><a href="/2018/03/12/2018-03-12-tfunpacking6/">TensorFlow 拆包（六）：RDMA</a></li></ul><p>开始分析性能瓶颈了，本篇记录一下研究 TF 中自带的 Profiling 工具时遇到的几个坑点。</p><a id="more"></a><h1 id="profiler"><a href="#profiler" class="headerlink" title="profiler"></a>profiler</h1><p>大概 17 年 5 月左右，<code>/tensorflow/core/</code> 中新加了一个 profiler 的目录，里面是把原本在 contrib 中的 profiling 工具移过来了，大概正式 release 应该是在 1.6、1.7 里面。</p><ul><li><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler" target="_blank" rel="noopener">TensorFlow Profiler and Advisor</a></li></ul><p>关于生成 profiling 的 context 文件详见 <code>tf.profiler</code> 相关的内容，这里直接开始记录怎么用 <code>tfprof</code> 这个工具。</p><p>试了一下 pip 包里面应该是没有单独包含的，需要从源码手动编译：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel build //tensorflow/core/profiler</span><br></pre></td></tr></table></figure><p>然后使用也是要从 bazel-bin 的目录中打开：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel-bin/tensorflow/core/profiler/profiler --profile_path=xxxxx</span><br></pre></td></tr></table></figure><h2 id="profiler-ui"><a href="#profiler-ui" class="headerlink" title="profiler_ui"></a>profiler_ui</h2><p>profiler 的 README 中，示例代码除了 profiler 以外还有个 profiler_ui，基本上是一个类似 tensorboard 的网页前端，方便调用后端的 profiler 进行可视化查看用的。</p><p>这里虽然写着暂未开源，但是在 TensorFlow 的 github 总目录里面可以找到一个<a href="https://github.com/tensorflow/profiler-ui" target="_blank" rel="noopener">叫 profiler-ui 的项目</a>，就是那个未完善开源的 ui 版了。</p><p>看了下，安装需要用到 go 以及 Google 自家的 pprof 工具，可能是因为耦合的其他部件比较多，所以暂时还没有并入 TF 的主代码中去。不过这里的 Installation 已经足够我们自己装上了。</p><blockquote><p>装 pprof 的时候会有个坑点，CentOS 库中可以找到 gperftools 这个工具，也是 Google 提供的，yum 装上之后可执行文件的名字也叫 pprof ！！但是跟这里用到的 pprof 不是一个玩意！！</p></blockquote><p>之后按照示例上的说明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ui.py --profile_context_path=xxxx</span><br></pre></td></tr></table></figure><p>即可启用。</p><p>在我尝试使用它的时候，距离这个库上一次 git 的更新已经过去 1 个月左右了，不知道是 python 版本还是什么原因，直接运行可能会遇到找不到 <code>server</code> 的路径等等的 bug，直接在 <code>ui.py</code> 里面稍微改一下就好。</p><h1 id="Profiling"><a href="#Profiling" class="headerlink" title="Profiling"></a>Profiling</h1><p>运行 TF 时保存出来的 profiling 文件包含了大量信息，主要有几个方面：</p><ul><li>scope：应该是 python 层代码中用 <code>tf.name_scope()</code> 包起来的视图</li><li>graph：TensorFlow 计算图的视图</li><li>op：把 TensorFlow 计算图再细化一层</li><li>code：Python 代码视图</li></ul><p>默认会按列表把所选的视图中的一些信息给输出出来，另外用<code>-output</code> 选项可以指定输出成另外的格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tfprof&gt;</span><br><span class="line">xxx xxx -output timeline:outfile=xxxxx</span><br><span class="line"><span class="comment"># 把结果输出成 chrome 用的时间线 trace 文件，可以在 chrome 地址栏中输入 chrome://tracing 打开</span></span><br><span class="line"><span class="comment"># 只支持 graph、scope、code 这 3 种视图</span></span><br><span class="line">xxx xxx -output pprof:outfile=xxxxx</span><br><span class="line"><span class="comment"># 把结果输出成 pprof 用的可视化文件（所以前面装 pprof 就是为了这个）</span></span><br><span class="line"><span class="comment"># 只支持 code 这种视图</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># pprof 可视化文件之后可以用 pprof 来变成图片（猜测大概是类似 GraphViz 的数据结构）</span></span><br><span class="line">pprof -svg --nodecount=10000 --sample_index=1 xxxxxx.prof &gt; xxxxxx.svg</span><br></pre></td></tr></table></figure><p>profiler_ui 打开时的第一个页面就是 graph 视图生成的 timeline：</p><p><img data-src="http://jcf94.com/download/2018-04-10-tfunpacking7-graph_timeline.png" alt=""></p><p>其中包含了计算图中每个 node 在卡上的情况，运行时间、数据流动依赖关系等等。（话说显示的太复杂了，事实上我觉得还是很难看）</p><p>然后默认的 scope 视图以及 code 视图得到的 timeline 我也感觉并没有什么用。</p><p>code 视图输出成的 pprof 图片倒是还可以看一下，但是感觉用处也不大</p><blockquote><p>所以最后感觉还是不知道该怎么用好这套 profiling 工具</p></blockquote><h2 id="Options"><a href="#Options" class="headerlink" title="Options"></a>Options</h2><p>在 tfprof 界面直接回车可以看到默认的选项，然后这里面的内容都是可以改的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tfprof&gt;                             </span><br><span class="line">-max_depth                  10      </span><br><span class="line">-min_bytes                  0       </span><br><span class="line">-min_peak_bytes             0       </span><br><span class="line">-min_residual_bytes         0       </span><br><span class="line">-min_output_bytes           0       </span><br><span class="line">-min_micros                 0       </span><br><span class="line">-min_accelerator_micros     0       </span><br><span class="line">-min_cpu_micros             0       </span><br><span class="line">-min_params                 0       </span><br><span class="line">-min_float_ops              0       </span><br><span class="line">-min_occurrence             0       </span><br><span class="line">-step                       -1      </span><br><span class="line">-order_by                   name    </span><br><span class="line">-account_type_regexes       .*      </span><br><span class="line">-start_name_regexes         .*      </span><br><span class="line">-trim_name_regexes                  </span><br><span class="line">-show_name_regexes          .*      </span><br><span class="line">-hide_name_regexes                  </span><br><span class="line">-account_displayed_op_only  <span class="literal">false</span>   </span><br><span class="line">-select                     micros  </span><br><span class="line">-output                     stdout:</span><br></pre></td></tr></table></figure><p>稍微挑几个写一下：</p><p><code>-max_depth</code>：指定显示前多少个 node（配合下面的 -order_by ？）</p><p><code>-step</code>：profiling 记录的文件可能包含了很多个 step，用这个选项来指定当前分析哪个 step 的信息，默认 -1 是对所有 step 做平均</p><p><code>-order_by</code>：打出来列表的时候，按照什么来排序：</p><ul><li>name：node 的名称</li><li>depth：node 在节点树中的深度</li><li>bytes：占用的内存数</li><li>peak_bytes：占用的峰值内存数</li><li>residual_bytes：计算完成之后，还剩下不释放的内存数</li><li>output_bytes：输出的大小</li><li>micros：node 计算所花费的时间</li><li>accelerator_micros：node 计算所花费的加速卡时间（区别于 CPU 的其他设备）</li><li>cpu_micros：node 计算所花费的 CPU 时间</li><li>params：node 中包含的参数量</li><li>float_ops：node 所需要的浮点运算次数</li><li>occurrence：node 在图中出现的次数</li></ul><p><code>-account_type_regexes</code>：筛选出类型里面带有某些前缀的 node 有多少个</p><p><code>-start_name_regexes</code>：筛选出名字中带某些前缀的 node</p><p><code>-trim_name_regexes</code>：隐藏掉名字中带某些前缀的 node</p><p><code>-show_name_regexes</code>：筛选出名字中带某些字符的 node</p><p><code>-hide_name_regexes</code>：隐藏掉名字中带某些字符的 node</p><p><code>-select</code>：选择视图中的哪些内容（有点像从数据库里面找东西的感觉），输出 timeline 的时候配合这个应该能够得到不同的数据：</p><ul><li>bytes：占用的内存数</li><li>peak_bytes：占用的峰值内存数</li><li>residual_bytes：计算完成之后，还剩下不释放的内存数</li><li>output_bytes：输出的大小</li><li>micros：计算所花费的时间</li><li>accelerator_micros：计算所花费的加速卡时间</li><li>cpu_micros：计算所花费的 CPU 时间</li><li>params： 参数量</li><li>float_ops：浮点运算次数</li><li>occurrence：在计算图中出现的次数</li><li>tensor_value：tensor 数据的值（估计需要配合 checkpoint 用）</li><li>device：op 放在哪个设备上</li><li>op_types：op 类型</li><li>input_shapes：输入的形状</li></ul><h1 id="Trace"><a href="#Trace" class="headerlink" title="Trace"></a>Trace</h1><p>抛开上面那个目前还没有正式 Release 的 Profiling 接口不说，实际可以用来做分析的是一套生成 trace_file 的 API。</p><p>用法也很简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</span><br><span class="line">run_metadata = tf.RunMetadata()</span><br><span class="line"></span><br><span class="line">_ = sess.run(optimizer, options=options, run_metadata=run_metadata)</span><br><span class="line"></span><br><span class="line">fetched_timeline = timeline.Timeline(run_metadata.step_stats)</span><br><span class="line">chrome_trace = fetched_timeline.generate_chrome_trace_format()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(FLAGS.trace_file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(chrome_trace)</span><br><span class="line">print(<span class="string">'Chrome Trace File write in %s'</span> % FLAGS.trace_file)</span><br></pre></td></tr></table></figure><p>在 RunOptions 中设置好追踪的级别，然后作为参数一起参与 <code>Session.run()</code>，最后记录得到的每个 step 的追踪数据通过 run_metadata 的结构返回出来。通过对追踪结果的解析即可生成我们可以理解的图形数据了，这个用的是 chrome 支持的 json 格式，在 chrome 地址栏中输入 <code>chrome://tracing/</code> 即可很方便地查看，timeline 最后出来的效果跟上面的是一致的。</p><blockquote><p>应该说，前面这个 Profiling 的 API 应该底层封装的也是这套机制。</p></blockquote><p>在 DirectSession 中可以非常容易地找到与 trace_level 相关的代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">  <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;DeviceTracer&gt; tracer;</span><br><span class="line">  <span class="keyword">if</span> (run_options.trace_level() &gt;= RunOptions::HARDWARE_TRACE) &#123;</span><br><span class="line">    tracer = CreateDeviceTracer();</span><br><span class="line">    <span class="comment">// tracer may be NULL on platforms without accelerators.</span></span><br><span class="line">    <span class="keyword">if</span> (tracer) &#123;</span><br><span class="line">      Status s = tracer-&gt;Start();</span><br><span class="line">      <span class="keyword">if</span> (!s.ok()) &#123;</span><br><span class="line">        run_state.executors_done.Notify();</span><br><span class="line">        <span class="keyword">delete</span> barrier;</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">...</span><br><span class="line">  <span class="keyword">if</span> (tracer) &#123;</span><br><span class="line">    TF_RETURN_IF_ERROR(tracer-&gt;Stop());</span><br><span class="line">    TF_RETURN_IF_ERROR(tracer-&gt;Collect(run_state.collector.get()));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>其中 DeviceTracer 是一个预留给多种设备来方便进行性能分析的接口，可惜的是目前里面的实现只有 GPU 的，需要依靠 CUDA 提供的 CUPTI 库。所以大概追踪过程中得到的与 CPU 相关的信息应该也是 CUPTI 附带的，如果是纯 CPU 版本的 TensorFlow，<code>CreateDeviceTracer()</code> 直接返回的是一个空指针。</p><h2 id="Distributed"><a href="#Distributed" class="headerlink" title="Distributed"></a>Distributed</h2><p>由于分布式环境下的 Session 的执行模式与单机情况下有所不同，因而分布式下运行 trace 的工作方式也会有所区别。</p><p>MasterSession 中首次执行 PartialRun 时会初始化 PerStepState：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// If this is the first partial run, initialize the PerStepState.</span></span><br><span class="line"><span class="keyword">if</span> (!run_state-&gt;step_started) &#123;</span><br><span class="line">  run_state-&gt;step_started = <span class="literal">true</span>;</span><br><span class="line">  PerStepState pss;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">auto</span> count = run_state-&gt;count;</span><br><span class="line">  pss.collect_timeline =</span><br><span class="line">      req.options().trace_level() == RunOptions::FULL_TRACE;</span><br><span class="line">  pss.collect_rpcs = req.options().trace_level() == RunOptions::FULL_TRACE;</span><br><span class="line">  pss.report_tensor_allocations_upon_oom =</span><br><span class="line">      req.options().report_tensor_allocations_upon_oom();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Build the cost model every 'build_cost_model_every' steps after skipping</span></span><br><span class="line">  <span class="comment">// an</span></span><br><span class="line">  <span class="comment">// initial 'build_cost_model_after' steps.</span></span><br><span class="line">  <span class="keyword">const</span> int64 build_cost_model_after =</span><br><span class="line">      session_opts_.config.graph_options().build_cost_model_after();</span><br><span class="line">  <span class="keyword">const</span> int64 build_cost_model_every =</span><br><span class="line">      session_opts_.config.graph_options().build_cost_model();</span><br><span class="line">  pss.collect_costs =</span><br><span class="line">      build_cost_model_every &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">      ((count + <span class="number">1</span> - build_cost_model_after) % build_cost_model_every == <span class="number">0</span>);</span><br><span class="line">  pss.collect_partition_graphs = req.options().output_partition_graphs();</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;ProfileHandler&gt; ph = run_state-&gt;rcg-&gt;GetProfileHandler(</span><br><span class="line">      run_state-&gt;step_id, count, req.options());</span><br><span class="line">  <span class="keyword">if</span> (ph) &#123;</span><br><span class="line">    pss.collect_timeline = <span class="literal">true</span>;</span><br><span class="line">    pss.collect_rpcs = ph-&gt;should_collect_rpcs();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  run_state-&gt;pss = <span class="built_in">std</span>::move(pss);</span><br><span class="line">  run_state-&gt;ph = <span class="built_in">std</span>::move(ph);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里会根据 trace_level 的值来设置一些标记。</p><p>下一步，pss 中的内容又会被写到 exec_opts 结构中：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Collect execution cost stats on a smoothly decreasing frequency.</span></span><br><span class="line">ExecutorOpts exec_opts;</span><br><span class="line"><span class="keyword">if</span> (pss-&gt;report_tensor_allocations_upon_oom) &#123;</span><br><span class="line">  exec_opts.set_report_tensor_allocations_upon_oom(<span class="literal">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (pss-&gt;collect_costs) &#123;</span><br><span class="line">  exec_opts.set_record_costs(<span class="literal">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (pss-&gt;collect_timeline) &#123;</span><br><span class="line">  exec_opts.set_record_timeline(<span class="literal">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (pss-&gt;collect_rpcs) &#123;</span><br><span class="line">  SetRPCLogging(<span class="literal">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (pss-&gt;collect_partition_graphs) &#123;</span><br><span class="line">  exec_opts.set_record_partition_graphs(<span class="literal">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (pss-&gt;collect_costs || pss-&gt;collect_timeline) &#123;</span><br><span class="line">  pss-&gt;step_stats.resize(partitions_.size());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个结构会被封装在 rpc 的 call 中发送给 WorkerService 来处理。</p><p>在 Worker 的运行结构中，可以看到这样的代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StepStatsCollector* collector = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">if</span> (request-&gt;exec_opts().report_tensor_allocations_upon_oom() ||</span><br><span class="line">    request-&gt;exec_opts().record_timeline() ||</span><br><span class="line">    request-&gt;exec_opts().record_costs()) &#123;</span><br><span class="line">  collector = <span class="keyword">new</span> StepStatsCollector(response-&gt;mutable_step_stats());</span><br><span class="line">  <span class="comment">// TODO(mrry,pbar): GPU tracing for distributed steps.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>好吧，StepStatsCollector 都已经创建了，但是可惜后续具体的 GPU tracing 部分还没有往里面完善。</p><h1 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h1><p>TensorFlow 官方的 <a href="https://tensorflow.google.cn/performance" target="_blank" rel="noopener">Performance 页</a> 和 <a href="https://tensorflow.google.cn/performance/benchmarks" target="_blank" rel="noopener">Benchmarks 页</a> 中给出了官方测性能用的 benchmark 脚本：</p><ul><li><a href="https://github.com/tensorflow/benchmarks" target="_blank" rel="noopener">Github: tensorflow/benchmarks</a></li></ul><p>基本上把目前 TF 里最高效的 API 都用上了，并且包含了各种常见的多机多卡方案，很值得作为高效的样例脚本来参照。</p><blockquote><p>有个问题是这个库目前没有定期 release，各种更新全都合并到 master 分支里面去了，然后随着 TF 版本的不断更新，它的 master 分支是跟着 TF 的 master 分支走的。</p><p>因此要想正常跑最新的 benchmarks，就需要装 tf-nightly-gpu 包或者源码编译一个比较新的 TensorFlow 分支。</p></blockquote><p>简单分析一下这份脚本的结构。</p><p>从 <code>tf_cnn_benchmarks.py</code> 这个入口进去之后，核心的执行流程在 <code>benchmark_cnn.py</code> 中。</p><p>只测试前向走 <code>BenchmarkCNN._eval_cnn()</code>，测试训练全过程走 <code>BenchmarkCNN._benchmark_cnn()</code> 。</p><p>前面的 FLAG 解析什么的直接略过，从训练部分开始看。</p><h2 id="benchmark-cnn"><a href="#benchmark-cnn" class="headerlink" title="_benchmark_cnn"></a>_benchmark_cnn</h2><p>首先构建计算图：<code>(image_producer_ops, enqueue_ops, fetches) = self._build_model()</code> </p><p><code>image_producer_ops</code> 是处理输入数据的部分，<code>enqueue_ops</code> 涉及到计算图中的流水线队列，最后的 <code>fetches</code> 是等一下 <code>sess.run()</code> 中的目标 op。</p><p>设置 <code>tf.summary</code> 以及 <code>tf.train.Saver</code> 等等，Saver 中传入的是 <code>variable_mgr.savable_variables()</code>。</p><p>创建 <code>tf.train.Supervisor</code> 时同时完成变量初始化，初始化 op 组包含：</p><ul><li><code>tf.local_variables_initializer</code> 初始化本地变量</li><li><code>tf.tables_initializer</code> 初始化用到的各种表（哈希表等等）</li><li>本地变量初始化之后，执行<code>variable_mgr.get_post_init_ops()</code> 完成自定义的一些初始化执行动作，这个部分要根据不同的参数维护算法来定</li><li>如果有同步用的队列 barrier，也一起在这里完成初始化</li></ul><p>之后 <code>sv.managed_session</code> 开始真正的执行循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> sv.managed_session(...) <span class="keyword">as</span> sess:</span><br><span class="line">    如果使用的是真实数据，则往 enqueue_ops 中插入提取数据到队列的 op</span><br><span class="line">    初始化 global_step</span><br><span class="line">    创建一个 global_step_watcher，在新线程中监控 global_step 的变动情况</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done_fn():     这个函数与 global_step_watcher 相关，用于控制训练循环什么时候结束</span><br><span class="line">        ...</span><br><span class="line">        benchmark_one_step() 训练一个 step</span><br><span class="line">        ...</span><br><span class="line">    后续再处理一些收尾内容</span><br></pre></td></tr></table></figure><h2 id="build-model"><a href="#build-model" class="headerlink" title="_build_model"></a>_build_model</h2><p>回到第一步看一下计算图的构建部分。</p><p><code>(image_producer_ops, image_producer_stages) = self._build_image_processing(shift_ratio=0)</code> 创建输入数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">对于当前设备上的每一块 GPU，variable_mgr.create_outer_variable_scope() 创建命名域：</span><br><span class="line">    add_forward_pass_and_gradients()    添加网络的前向部分，并且计算得到梯度</span><br><span class="line">    根据当前的任务是训练还是预测，处理准备网络中需要返回的内容</span><br><span class="line">    从计算图中提取出 Batch Normalization 的更新部分，添加到 <span class="number">0</span> 号卡的更新部分中，BatchNorm 只需要一块卡来计算</span><br></pre></td></tr></table></figure><p>如果图中用了 staging_area 的数据组织方式，这里另外再添加一下，扩充 enqueue_ops。</p><p><code>fetches = self._build_fetches()</code> 最终收集前面所有的信息，构建出等一下需要传入 sess.run() 中去的目标</p><p>完成前面的内容后，把 image_producer_ops，enqueue_ops，fetches 三部分内容返回给上一层的函数。</p><h2 id="add-forward-pass-and-gradients"><a href="#add-forward-pass-and-gradients" class="headerlink" title="add_forward_pass_and_gradients"></a>add_forward_pass_and_gradients</h2><p>创建随机数据作为输入，或者处理传入的数据产生器。</p><p><code>logits, aux_logits = self.model.build_network()</code> 构建完整的前向网络。</p><p>添加输出结果以及计算 loss 误差。</p><p><code>variable_mgr.trainable_variables_on_device()</code> 获取当前 GPU 上所有的可训练参数。</p><p>如果当前是最后一块 GPU 卡，那么再额外计算 L2_loss，添加到前面的 loss 中去，L2_loss 只需要计算一次。</p><p><code>grads = tf.gradients(scaled_loss, params, aggregation_method=aggmeth)</code> 根据前面收集的当前 GPU 上的可训练参数信息构建反向的梯度计算图，返回得到的是图中所有的梯度。</p><p>接下来再获取一次 <code>variable_mgr.trainable_variables_on_device()</code> ，然后把得到的参数与前面的梯度打包在一起返回回去，准备接下来的参数更新。<strong>需要注意的是</strong>，第一次调用 <code>trainable_variables_on_device</code> 时传入了一个 <code>writable=False</code> 的参数，这里传入的是 <code>writable=True</code>，在某些特别的多卡参数管理算法中，用于梯度计算和最终梯度更新写回的目标是不一样的。</p><p>所有前面的这些都封装在一个 results 的 dict 中返回回去。</p><h2 id="build-fetches"><a href="#build-fetches" class="headerlink" title="_build_fetches"></a>_build_fetches</h2><p>这里算是计算图构建的收尾部分了，传入的内容是所有 GPU 上计算图的合集。</p><p><code>variable_mgr.preprocess_device_grads()</code> 预处理出需要在哪些设备上执行梯度更新操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对于梯度更新设备中的每一块 GPU：</span><br><span class="line">    tf.reduce_mean()                          计算前面所有卡上梯度的平均值</span><br><span class="line">    variable_mgr.get_gradients_to_apply()     获取有哪些梯度是要在当前设备上更新的</span><br><span class="line">    get_learning_rate()                       计算学习率</span><br><span class="line">    get_optimizer()                           获取梯度更新用的 Optimizer</span><br><span class="line">    variable_mgr.append_apply_gradients_ops() 应用 Optimizer 进行梯度更新</span><br></pre></td></tr></table></figure><p>把前面所有的东西打包在 fetches 这个 dict 中返回回去。</p><h2 id="VariableMgr"><a href="#VariableMgr" class="headerlink" title="VariableMgr"></a>VariableMgr</h2><p>可以看到上面有很多核心的操作都是通过 <code>variable_mgr</code> 结构完成的，这套脚本定义了一个 VariableMgr 类，想要自己修改参数管理、更新的算法只需要重写这里面的一些函数即可。</p><p>前面出现过的比较有用的几个接口函数：</p><ul><li>def <strong>create_outer_variable_scope</strong>(self, device_num)</li></ul><p>封装变量命名域，主要用于维护变量创建时要做的事情，一般情况下直接返回一个普通的 <code>tf.variable_scope</code>，需要对变量创建进行额外操作的话需要自己构造一个 custom_getter 作为参数传入<code>tf.variable_scope</code>。</p><ul><li>def <strong>preprocess_device_grads</strong>(self, device_grads)</li></ul><p>预处理出需要做梯度更新操作的设备，以及对应设备上的梯度和参数。</p><ul><li>def <strong>get_gradients_to_apply</strong>(self, device_num, gradient_state)</li></ul><p>与上一个函数对应使用，用于找出每个设备需要处理哪些参数更新任务。</p><ul><li>def <strong>append_apply_gradients_ops</strong>(self, gradient_state, opt, grads, training_ops, loss_scale_params)</li></ul><p>在设备上针对每一对需要更新的变量及其梯度，应用 apply_gradients 操作。</p><ul><li>def <strong>get_post_init_ops</strong>(self)</li></ul><p>用于额外附加一些在所有变量完成初始化之后，开始训练之前，需要执行的操作。</p><ul><li>def <strong>get_devices</strong>(self)</li></ul><p>返回当前节点中可用的 GPU 列表，在某些 PS-WORKER 的实现方式中，返回的是 <code>tf.replica_device_setter</code> 的封装。</p><ul><li>def <strong>savable_variables</strong>(self)</li></ul><p>返回哪些变量是需要被 <code>tf.Saver</code> 保存进检查点的。</p><ul><li>def <strong>trainable_variables_on_device</strong>(self, rel_device_num, abs_device_num, writable=False)</li></ul><p>返回当前设备上的可训练参数（即能计算梯度，可以进行反向更新的参数）。输入的两个 device_num 分别是 GPU 在<strong>当前节点中</strong>的编号以及<strong>在全局环境中</strong>的编号。</p><p>writable 用于标识需要被写回更新的参数，在有些情况下图中可能存在多份参数备份，writable 为 False 时返回的是图中用于求梯度以及构建反向数据通路用的参数，为 True 时返回的是等一下 apply_gradients 需要应用梯度更新操作的参数。</p><h2 id="replica-device-setter-amp-variable-scope-custom-getter"><a href="#replica-device-setter-amp-variable-scope-custom-getter" class="headerlink" title="replica_device_setter &amp; variable_scope-custom_getter"></a>replica_device_setter &amp; variable_scope-custom_getter</h2><p>前面建图时用到的两个很重要的接口，用于额外处理 op 在设备上的分配操作。</p><p>benchmarks 脚本中的用法大概是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> device_num <span class="keyword">in</span> range(len(self.devices)):</span><br><span class="line">    <span class="keyword">with</span> self.variable_mgr.create_outer_variable_scope(device_num):</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="keyword">with</span> tf.device(self.devices[rel_device_num]):</span><br><span class="line">        <span class="comment"># self.devices[] 里面是实现设好的 tf.replica_device_setter</span></span><br><span class="line">            <span class="comment">#...build_network</span></span><br></pre></td></tr></table></figure><p>对节点中的每一块 GPU 卡，首先套上一个 variable_scope（里面可能会使用到 custom_getter），在构建 op 时再套一层 replica_device_setter。</p><p><code>tf.replica_device_setter</code> 需要配合 <code>tf.device</code> 使用，作用范围是其 python 作用域以内的所有 op，这个函数简单地说就是对传入的 op 进行判断，如果是计算型的 op 就正常分配在运算设备上，如果是需要在 PS-WORKER 之间共享的参数型 op 则需要在参数服务器上。它的返回值是需要分配给的 device 的名字，所以直接用 <code>tf.device</code> 指定即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replica_device_setter</span><span class="params">(ps_tasks=<span class="number">0</span>, ps_device=<span class="string">"/job:ps"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                          worker_device=<span class="string">"/job:worker"</span>, merge_devices=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                          cluster=None, ps_ops=None, ps_strategy=None)</span></span></span><br></pre></td></tr></table></figure><p>具体的源码实现上，主要是对新创建 op 的类型进行判断，如果在 ps_ops 包含的范围内（为 None 时会用一个 STANDARD_PS_OPS 来作为检查范围）则用某种 ps 分配策略放到参数服务器上，否则放到默认的计算设备上。</p><p>默认的 ps_strategy 不指定的话就是用的 round-robin，简单地说就是按顺序依次分。</p><p><code>tf.variable_scope</code> 中 custom_getter 的作用范围就只限于作用域以内所有的 <code>tf.get_variable</code> 调用了（注意，必须是 <code>tf.get_variable</code>，这个对 <code>tf.variable</code> 是无效的）。前面 replica_device_setter 只是指定了参数存放的位置，这里则可以对参数创建进行更多的改动。</p><p>例如 StagedVariableGetter 做的事情就是把变量封装上一层 StagingArea，计算图中需要读取变量的时候返回一个 StagingArea.get，对于 apply_gradient 这种需要修改变量本身的操作，则返回参数本体（也就是前面看到的 writable 这个参数起作用的方式）。</p><h1 id="VariableMgr-instances"><a href="#VariableMgr-instances" class="headerlink" title="VariableMgr instances"></a>VariableMgr instances</h1><p>官方的 Benchmark 脚本中提供了 8 种内置的 VariableMgr 实例。</p><h2 id="VariableMgrIndependent"><a href="#VariableMgrIndependent" class="headerlink" title="VariableMgrIndependent"></a>VariableMgrIndependent</h2><p>不同卡之间完全不作数据交互，单纯用来测单机多卡的理论计算速度用。</p><p>不需要封装 custom_getter 和 replica_device_setter。</p><h2 id="VariableMgrLocalFetchFromPS"><a href="#VariableMgrLocalFetchFromPS" class="headerlink" title="VariableMgrLocalFetchFromPS"></a>VariableMgrLocalFetchFromPS</h2><p>多卡中的参数统一存储，不同卡在计算时直接从统一的 PS 中读取需要的数据。</p><p>不需要封装 custom_getter。</p><p>get_device 这里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.benchmark_cnn.local_parameter_device_flag == <span class="string">'gpu'</span>:</span><br><span class="line">  <span class="keyword">return</span> [</span><br><span class="line">      variable_mgr_util.ParamServerDeviceSetter(d, raw_devices)</span><br><span class="line">      <span class="keyword">for</span> d <span class="keyword">in</span> raw_devices</span><br><span class="line">  ]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="keyword">return</span> [</span><br><span class="line">      tf.train.replica_device_setter(</span><br><span class="line">          worker_device=d,</span><br><span class="line">          ps_device=self.benchmark_cnn.param_server_device,</span><br><span class="line">          ps_tasks=<span class="number">1</span>) <span class="keyword">for</span> d <span class="keyword">in</span> raw_devices</span><br><span class="line">  ]</span><br></pre></td></tr></table></figure><p>如果参数存放在 CPU 上，直接对每个 GPU 设备返回一个指定好 ps_device 的 replica_device_setter。</p><p>如果选择参数存放在 GPU 上，这里的做法是将所有参数均衡负载平分在各块卡上。</p><h2 id="VariableMgrLocalFetchFromStagedPS"><a href="#VariableMgrLocalFetchFromStagedPS" class="headerlink" title="VariableMgrLocalFetchFromStagedPS"></a>VariableMgrLocalFetchFromStagedPS</h2><p>多卡中的参数统一存储，相比之前的增加了 StagingArea 的流水线操作。</p><p>custom_getter 中为每个变量额外创建了一个 StagingArea，计算图中需要读取变量的时候返回对应的 StagingArea.get。</p><p>trainable_variables_on_device 中 writable 为 True 时，返回变量本体，否则返回对应的 StagingArea.get。</p><p>其他部分与上一种方式相同。</p><h2 id="VariableMgrLocalReplicated"><a href="#VariableMgrLocalReplicated" class="headerlink" title="VariableMgrLocalReplicated"></a>VariableMgrLocalReplicated</h2><p>每块卡上的计算图完全独立，各卡都是自己存储自己的参数，梯度更新的时候再采用某种 Allreduce 的算法对各卡上的参数做统一规约。</p><p>get_post_init_ops 在初始化完成后拷贝 GPU0 上的参数到其他卡上覆盖掉，保证所有卡的初始参数一致。</p><p>不需要封装 custom_getter 和 replica_device_setter。</p><p>preprocess_device_grads 中返回的梯度是调用某种规约算法去综合所有卡上的梯度值，之后再跟本地的参数一起交给 apply_gradient 去更新即可。</p><p>因此这里的计算流程是，初始所有卡上参数一致，训练完一步之后规约梯度，规约完成后所有卡上得到的梯度也都一致了，再 apply 更新到本地的卡上，这样下一步开始时所有卡上的参数仍然是一致的。</p><h2 id="VariableMgrDistributedAllReduce"><a href="#VariableMgrDistributedAllReduce" class="headerlink" title="VariableMgrDistributedAllReduce"></a>VariableMgrDistributedAllReduce</h2><p>用于分布式。</p><p>这是脚本中唯一一种需要用到 single_session 的模式，基本上跟 Replicated 的方式一致，每块卡上都独立存数据，更新时全局规约，特殊点在于这种方式只需要由一个 python 进程来启动，所有 worker 上的图构造等等都是由一个 controller 的角色完成，其他所有的 worker 都像平时的 ps 一样 join_server 即可。</p><p>大体实现上跟上一种一致</p><h2 id="VariableMgrDistributedFetchFromPS"><a href="#VariableMgrDistributedFetchFromPS" class="headerlink" title="VariableMgrDistributedFetchFromPS"></a>VariableMgrDistributedFetchFromPS</h2><p>用于分布式。</p><p>大体实现跟单节点的 FetchFromPS 一致。</p><p>custom_getter 使用了 OverrideCachingDevice，虽然由 replica_device_setter 指定好了所有参数都保存在 ps 上，但是在 worker 还可以做一次数据缓存。caching_device 这个参数与 <code>tf.get_variable()</code> 中的参数对应，即 worker 端的多块卡从远程 ps 获取数据只在第一块卡拉取数据时通过网络取一次，后续的几次直接从缓存中读取。缓存数据的分配方案跟单节点 ps 时 CPU/GPU 上存储参数的方案类似。</p><h2 id="VariableMgrDistributedFetchFromStagedPS"><a href="#VariableMgrDistributedFetchFromStagedPS" class="headerlink" title="VariableMgrDistributedFetchFromStagedPS"></a>VariableMgrDistributedFetchFromStagedPS</h2><p>用于分布式。</p><p>在上一种的基础上加上了 StagingArea。</p><h2 id="VariableMgrDistributedReplicated"><a href="#VariableMgrDistributedReplicated" class="headerlink" title="VariableMgrDistributedReplicated"></a>VariableMgrDistributedReplicated</h2><p>用于分布式。</p><p>计算流程其实跟 DistributedAllReduce 是一致的，大体上跟前面类似实现相一致。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接上篇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/13/2018-01-13-tfunpacking/&quot;&gt;TensorFlow 拆包（一）：Session.Run()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/23/2018-01-23-tfunpacking2/&quot;&gt;TensorFlow 拆包（二）：TF 的数据流模型实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/02/28/2018-02-28-tfunpacking3/&quot;&gt;TensorFlow 拆包（三）：Graph 和 Node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/07/2018-03-07-tfunpacking4/&quot;&gt;TensorFlow 拆包（四）：Device&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/09/2018-03-09-tfunpacking5/&quot;&gt;TensorFlow 拆包（五）：Distributed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/12/2018-03-12-tfunpacking6/&quot;&gt;TensorFlow 拆包（六）：RDMA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;开始分析性能瓶颈了，本篇记录一下研究 TF 中自带的 Profiling 工具时遇到的几个坑点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TensorFlow" scheme="https://jcf94.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 拆包（六）：RDMA</title>
    <link href="https://jcf94.com/2018/03/12/2018-03-12-tfunpacking6/"/>
    <id>https://jcf94.com/2018/03/12/2018-03-12-tfunpacking6/</id>
    <published>2018-03-12T01:21:50.000Z</published>
    <updated>2018-09-23T10:58:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>接上篇：</p><ul><li><a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a></li><li><a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a></li><li><a href="/2018/02/28/2018-02-28-tfunpacking3/">TensorFlow 拆包（三）：Graph 和 Node</a></li><li><a href="/2018/03/07/2018-03-07-tfunpacking4/">TensorFlow 拆包（四）：Device</a></li><li><a href="/2018/03/09/2018-03-09-tfunpacking5/">TensorFlow 拆包（五）：Distributed</a></li></ul><p>本篇分析 TensorFlow 1.6.0 中的 RDMA 以及其他的传输优化的实现。</p><a id="more"></a><p>虽然我前面一篇论文做的工作也是这个，在当时 Yahoo 实现的 RDMA 版还没有收录进官方 Repo 的时候稍微做过一下对比，现在来看看收进官方库之后的具体实现是什么样的。</p><hr><p>之前对 GPU Direct 的 API 不是很了解，后来才发现这里的 RDMA 实现其实也是直接支持 GPU Direct 的。</p><h1 id="VerbsServer"><a href="#VerbsServer" class="headerlink" title="VerbsServer"></a>VerbsServer</h1><p>TensorFlow 分布式环境中的 Server 结构通过 ServerFactory 构建，这个工厂模式会根据传入的 protocol 选项创建不同的 Server。</p><p>当选择 <code>protocol = grpc+verbs</code> 时，<code>tf.train.Server()</code>创建 VerbsServer。</p><blockquote><p>在代码里面搜 <code>public ServerFactory</code> 可以找到 4 个继承类，除了 verbs 之外，其他分别是：</p><p><code>protocol = grpc</code> 对应 <code>GrpcServer</code>，即原本默认的 gRPC 通信方式</p><p><code>protocol = grpc+mpi</code> 对应 <code>MPIServer</code>，用 MPI 来完成通信</p><p><code>protocol = grpc+gdr</code> 对应 <code>GdrServer</code>，即 GPU Direct 支持</p></blockquote><p>从 protocol 的名字上也可以看出来，这几个新增的通信方式实现都还需要依赖 gRPC，例如 RDMA 一开始的 IB 卡配置、连接建立什么的都还需要先用 gRPC 来完成。</p><p><code>VerbsServer()</code>在构造时直接初始化了一个<code>GrpcServer()</code>，之后的<code>Init()</code>、<code>Start()</code>也都是先启动 GrpcServer 中对应的方法。同时创建一个 VerbsService（重载的 ServerCompletionQueue 类）。</p><p>然后创建一个 <code>RdmaMgr()</code> 用于管理 RDMA 底层的连接，<code>RdmaRendezvousMgr()</code>用于管理数据存储。</p><h1 id="RdmaMgr"><a href="#RdmaMgr" class="headerlink" title="RdmaMgr"></a>RdmaMgr</h1><p>RdmaMgr 中维护了 RdmaAdapter 和 RdmaChannel 这两个结构，RdmaAdapter 负责维护 RDMA 通信需要的底层结构（rdma device context、protection domain、事件 channel、完成队列等等），RdmaChannel 则是代表每一个独立的 RDMA 连接。</p><p>RdmaMgr 初始化时对所有不在本地的 worker 创建一个 RdmaChannel（创建和设置 queue pair，初始化 buffer），插入到 channel_table_ 中，。</p><p>VerbsServer 启动时，首先启动 GrpcServer（<code>GrpcServer::Start()</code>），再通过 RdmaMgr 来建立 rdma 通道之间的链接（<code>rdma_mgr_-&gt;SetupChannels()</code>），之后在<code>rdma_mgr_-&gt;ConnectivityCheck()</code>中，分别测试各个 rdma 链接的连通性，然后用<code>rdma_adapter_-&gt;StartPolling()</code>启动 RdmaAdapter 中的守护进程，等待后续 RDMA 传输信息，后面 Grpc 的传输通道应该就不需要再用了。</p><h1 id="RdmaRendezvousMgr"><a href="#RdmaRendezvousMgr" class="headerlink" title="RdmaRendezvousMgr"></a>RdmaRendezvousMgr</h1><p>RdmaRendezvousMgr 继承于 BaseRendezvousMgr，具体的实例类是从 BaseRemoteRendezvous 继承过来的 RdmaRemoteRendezvous，整个 RDMA 的传输过程由 <code>RdmaRemoteRendezvous::RecvFromRemoteAsync()</code> 开始。</p><p>基本上做 RDMA 优化的想法都是一致的，跟我们之前的实现很类似，其他的详见官方文档：</p><ul><li><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/README.md" target="_blank" rel="noopener">How to compile, use and configure RDMA-enabled TensorFlow</a></li></ul><h1 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h1><p>说起来，事实上 RDMA 的移植过程中的最大问题还是内存，因为要用 RDMA 就需要事先把内存注册到 IB 卡上，这样 IB 卡才能够有权限直接读写内存并且保证这块被注册过的内存不会被换页换出去。</p><p>所以对于他们的 RDMA 实现来说，我最关心的还是这个部分。</p><p>TensorFlow 原本的实现就是在传输的时候动态给 send 操作分配内存。</p><p>通常的思路有三种：</p><ol><li>数据传输的时候现场把需要被传输的 Tensor 内存注册到 IB 卡上，用过以后再释放掉。这样很明显会有很大的 overhead。</li><li>提前注册一块固定的内存，传输的时候把需要发送的数据 copy 进去，然后在那块预注册的内存上进行 RDMA 操作。Overhead 会小一点，但是中间需要的 <code>memcpy</code> 操作还是有点浪费。</li><li>内存池！！手动维护一个内存池，实现就把内存池注册到 IB 卡上，然后传输过程中，动态申请内存、用后动态释放内存都从内存池里面走。</li></ol><p>想想都知道第三种方案是最好的，然而我们当时实现时没有成功把内存池写出来（后来，这事就成了挖好了但是一直没填上的大坑了<a href="/2017/04/19/2017-04-19-memorypool/">【写着玩之 内存池】</a>、<a href="/2017/09/18/2017-09-18-rdmalib/">【写着玩之 RDMA 轮子】</a>。。。。。。很难受），所以其实采取的是 1、2 混合的方案。</p><p>当要发送的数据比较少的时候，memcpy 速度快，所以拷过去再发效果好点，发送的数据大的时候，就现场注册。然后我们还分别试了用了 RDMA_WRITE 和 RDMA_READ 这两种方式，想办法把注册内存的时间跟计算、通信这些 overlap 开（虽然效果很有限，但是能做一点是一点咯）。</p><p>官方库里目前的实现用上了内存池：</p><ol><li>对于 DMAable Tensor（注册在支持 RDMA 的 CPU 上或者注册在支持 GPU Direct 的 GPU 上的 Tensor），都采用<strong>直接从源 Tensor 写到目标 Tensor 中</strong>的方案，完全避免了内存注册和内存拷贝。（66666666）</li><li>非 DMAable 的 Tensor，用 Protobuf 序列化之后通过预注册的内存传输（方案 2）。</li><li>不支持 GPU Direct 的 GPU 数据，虽然还是要拷回 CPU 端，但是 CPU 到 CPU 的传输用的还是 RDMA。</li></ol><h2 id="Memory-Pool"><a href="#Memory-Pool" class="headerlink" title="Memory Pool"></a>Memory Pool</h2><p>那么就来详细看一下这里的内存池实现，这里实际上是借用了 TensorFlow 本身自带的内存池（。。。嗯，我前面挖了很长时间的内存池坑到时候就扒这个来填吧）。TensorFlow 本身的内存分配就是通过自己维护的内存池来完成的。</p><p>关于 TensorFlow 的内存管理实现，可以参照<a href="http://blog.csdn.net/qq_33096883/article/details/77479647" target="_blank" rel="noopener">这里</a>的说明，主要采用的还是比较简单的 BFC 算法。</p><p><code>tensorflow/core/framework/allocator.[ch]</code> 是内存分配器的主要入口，具体的分配器实现在 <code>tensorflow/core/common_runtime/</code> 中的 <code>xxx_allocator.[ch]</code> 中。</p><p>RDMA 部分的内存管理由一个单例模式的 RdmaMemoryMgr 结构来完成。</p><p>在 <code>VerbsServer::Start()</code> 中有这么一步 <code>rdma_mgr_-&gt;InitAllocators()</code>：</p><ul><li>首先获取到本地所有的 Allocator，CPU 分配器以及可能的 GPU 分配器。</li><li>把 RdmaMemory 中插入/删除 Memory Region 的操作（主要是处理内存到卡上注册的这一步）包装成 Visitor 的接口函数。这里的 Memory Region 跟 TensorFlow 自己内存池里面的不是一个概念（指的目标倒可能是一致的），而是指的一整块独立内存（一次性用 ibv_reg_mr 把一整块内存注册好，一块内存对应一个 mr）。</li><li>后面再检查当前环境是否支持 GPU Direct，是则同样把 GPU 的 Visitor 也加上。</li></ul><p>之后需要用 RDMA 进行传输的时候，就可以从已有的 Memory Region 表中找出某块内存所对应的 mr、rkey 等等，而无需再注册了。</p><h1 id="Transport-protocol"><a href="#Transport-protocol" class="headerlink" title="Transport protocol"></a>Transport protocol</h1><p>TensorFlow 中的传输设计是<strong>异步发送，阻塞接收</strong>，这个设计也很容易理解，没有收到数据之前，当前节点之后的肯定执行不了，而发送者将内容提交之后数据流图到这里为止就结束了，可以把计算资源用于其他地方。</p><p>这张图来源于代码中的文档部分。</p><p><img data-src="http://jcf94.com/download/2018-03-12-tfunpacking6-verbs_with_0_copies.png" alt=""></p><h2 id="RecvFromRemoteAsync"><a href="#RecvFromRemoteAsync" class="headerlink" title="RecvFromRemoteAsync"></a>RecvFromRemoteAsync</h2><p>一次完整的传输过程从 <code>RdmaRemoteRendezvous::RecvFromRemoteAsync()</code> 开始：</p><ul><li>解析某条需要传输的 Tensor 的源地址和目标地址，确认目标地址是本机。</li><li>从 rdma_mgr_ 记录的 channel 中找出源地址对应的传输通道。</li><li>向传输通道中发送一条 Tensor 请求（构造一个新的 RdmaTensorRequest 并插入到 channel 的 request_table_ 中，启动 <code>RdmaTensorRequest-&gt;Start()</code>）。</li></ul><blockquote><p>接收端这边的所有操作都封装在 RdmaTensorRequest 结构中，相对的，发送端这边用来响应请求的所有操作都封装在 RdmaTensorResponse 结构中。</p></blockquote><p><code>Start()</code> 首先在 RdmaMemoryMgr 中检查本次需要接收的 Tensor 的Meta Data 是否存在（用 Rendezvous 的 key 作为关键字）。Meta Data 记录的是所传输的 Tensor 的详细信息：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TensorMetaData</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  TensorShape tensor_shape_;</span><br><span class="line">  DataType data_type_;</span><br><span class="line">  <span class="keyword">size_t</span> proto_size_;</span><br><span class="line">  <span class="keyword">bool</span> is_dead_;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最主要的内容是形状（TensorShape）和数据类型（DataType）。如果 Meta Data 记录存在，则可以直接新建一个 Tensor（构建 Tensor 需要传入的参数是 <strong>Allocator</strong>、<strong>数据类型</strong>以及<strong>形状</strong>）。从 RdmaMemoryMgr 的 Memory Region 表中直接可以查到所分配的内存的 mr 信息。如果本机不支持 GPU Direct，则把目标改成本地 CPU 端的内存接收；如果当前传输的 Tensor 的数据类型甚至都不支持 memcpy，则把它先序列化成 Protobuf 再传输。</p><blockquote><p>话说这里序列化是现场 malloc 内存，现场 ibv_reg_mr，为什么不直接从 CPU 的 Allocator 里面分配呢？</p></blockquote><p>之后发送 RDMA_MESSAGE_TENSOR_REQUEST 消息。消息中包含的内容除了 key 之外，最重要的就是前面准备好的 Tensor 地址和 rkey 了，这个标识了远程的发送端等一下要把数据写到什么地方去。</p><h2 id="Sender-Side-Recv-Message"><a href="#Sender-Side-Recv-Message" class="headerlink" title="Sender Side Recv Message"></a>Sender Side Recv Message</h2><p>守护进程 <code>RdmaAdapter::Process_CQ</code> 处理所有传入的 RDMA 消息，接收到 RDMA_MESSAGE_TENSOR_REQUEST 时，解析出收到的数据，构造一个新的 RdmaTensorResponse 并插入到 channel 的 responses_table_ 中，启动 <code>RdmaTensorResponse-&gt;Start()</code>）。</p><p>这一步调用 <code>RecvLocalAsync()</code> 从本地的 Rendezvous 中异步获取接收端所请求的 Tensor 数据，成功完成本地数据提取之后用 <code>RdmaTensorResponse::RecvHandler()</code> 开始准备数据的回传：</p><ul><li>对比本地提取的 Tensor 数据与接收端需要的 Meta Data 的各项是否一致（话说为什么会有不一致的情况呢？）</li><li>GPU：<ol><li>数据一致且支持 GPU Direct：同步一次 GPU 流，完成后直接用 <code>RdmaTensorResponse::SendContent()</code> 发出去；</li><li>数据一致但不支持 GPU Direct：把 Tensor 从 GPU 上拷到 CPU 上，发出去；</li><li>数据不一致：需要重新请求同步 Meta Data，但是这条 Tensor 在数据流图中可能还会被其他节点的运算改变，因此需要先把 Tensor 从 GPU 上拷出来，然后发送更新后的 Meta Data 信息给接收端；</li><li>数据不支持 memcpy：把 GPU 数据序列化之后发送出去。</li></ol></li><li>CPU：<ol><li>数据一致：直接发送；</li><li>数据不一致：发送更新后的 Meta Data；</li><li>数据不支持 memcpy：序列化之后发送。</li></ol></li></ul><blockquote><p>这个 Meta Data 重新请求的部分可以改成接收端直接用 RDMA READ 来抓取。</p></blockquote><p>发送完成之后回收用过的资源，然后把当前 RdmaTensorResponse 从 responses_table_ 中删掉。</p><h2 id="RecvTensorContent"><a href="#RecvTensorContent" class="headerlink" title="RecvTensorContent"></a>RecvTensorContent</h2><p>接收端收到传入的 Tensor 数据之后，根据是否需要从 CPU 拷回 GPU 或者是否需要反序列化等等，做出对应的操作，调用 RdmaTensorRequest 创建时上层传入的 <code>done()</code> 回调函数，最后回收资源，把当前 RdmaTensorRequest 从 request_table_ 中删掉。</p><hr><p>看到这里思路可以说是很清晰了，感觉上这里的 RDMA 实现也已经相当完善了（比我们当时做的好多了），细节上可能还能再抠一抠，不过再往上应该不会再能有什么大的性能提升了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接上篇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/13/2018-01-13-tfunpacking/&quot;&gt;TensorFlow 拆包（一）：Session.Run()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/23/2018-01-23-tfunpacking2/&quot;&gt;TensorFlow 拆包（二）：TF 的数据流模型实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/02/28/2018-02-28-tfunpacking3/&quot;&gt;TensorFlow 拆包（三）：Graph 和 Node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/07/2018-03-07-tfunpacking4/&quot;&gt;TensorFlow 拆包（四）：Device&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/09/2018-03-09-tfunpacking5/&quot;&gt;TensorFlow 拆包（五）：Distributed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇分析 TensorFlow 1.6.0 中的 RDMA 以及其他的传输优化的实现。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TensorFlow" scheme="https://jcf94.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 拆包（五）：Distributed</title>
    <link href="https://jcf94.com/2018/03/09/2018-03-09-tfunpacking5/"/>
    <id>https://jcf94.com/2018/03/09/2018-03-09-tfunpacking5/</id>
    <published>2018-03-09T12:40:43.000Z</published>
    <updated>2018-09-23T10:58:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>接上篇：</p><ul><li><a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a></li><li><a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a></li><li><a href="/2018/02/28/2018-02-28-tfunpacking3/">TensorFlow 拆包（三）：Graph 和 Node</a></li><li><a href="/2018/03/07/2018-03-07-tfunpacking4/">TensorFlow 拆包（四）：Device</a></li></ul><p>单节点的运行流程基本上已经有个大体印象了，接着就要来拆我所关注的重点所在——分布式运行时了。</p><a id="more"></a><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>进入代码部分之前，首先看一下<a href="https://www.tensorflow.org/extend/architecture" target="_blank" rel="noopener">官方文档</a>中，对整个 TensorFlow 结构的介绍。以下是一个典型的分布式 TensorFlow 的架构图：</p><p><img data-src="http://jcf94.com/download/2018-03-09-tfunpacking5-diag1.svg" alt=""></p><p>这里的 Master 和 Worker 服务只在分布式运行时中有，可以认为单节点的 Session 是包含了这两个服务的全部内容。</p><p>Client 指的是面向用户的前端编程接口，通常能用的就是 Python 和 C++ 了，client 完成运算图的构建，然后把图的定义通过 session 对象用<code>tf.GraphDef</code>这个 Protobuf 结构传给后面的 Master 服务来跑（即 Python 层定义好计算图，然后通过 Protobuf 的接口进入 C 部分的运行时）。</p><blockquote><p>所以源码中会有<code>/tensorflow/python/client</code>这个目录，其中的内容做的就是架构图中 client 这个概念的任务。</p></blockquote><p><img data-src="http://jcf94.com/download/2018-03-09-tfunpacking5-graph_master_cln.svg" alt=""></p><p>分布式环境中的 Master 有以下几个任务：</p><ol><li>精简并且优化计算图，根据当前次 client 提交运行的输入输出目标，提取出一个子图来</li><li>把子图划分到硬件上（graph -&gt; Partition -&gt; Device）</li><li>缓存前面那一步的结果，以便以后的 steps 能够重用而不用再把上面两步再执行一遍</li></ol><p>这些事情看上去好眼熟啊…没错！这就是<a href="/2018/01/13/2018-01-13-tfunpacking/#Executor">DirectSession 中 Executor 的任务啊</a>！</p><p>在 <a href="/2018/03/07/2018-03-07-tfunpacking4/#tensorflow-Partition"><code>tensorflow::Partition()</code></a>中，划分子图到硬件上，对于不在同一个设备上的边需要补充一对 send/recv 的 op 接口，例如上面那张目标图：</p><p><img data-src="http://jcf94.com/download/2018-03-09-tfunpacking5-graph_split2.svg" alt=""></p><p>Master 接下来再把任务分给具体的 Worker 服务来完成。每一个 Worker 服务都有自己对应的 tasks，ps 负责存储数据，worker 负责具体的计算。</p><blockquote><p>注意 Worker Service 和 Worker task 的区别，Worker Service 可以有 ps 和 worker 这两种 tasks。</p></blockquote><p><img data-src="http://jcf94.com/download/2018-03-09-tfunpacking5-graph_send_recv.svg" alt=""></p><hr><p>截两张实际运行中 Client、Master、Worker 的关系图。</p><p><img data-src="http://jcf94.com/download/2018-03-09-tfunpacking5-dist-graph.png" alt=""></p><p>上面一种做法是在 ps 和 worker1 中调用 <code>Server.join()</code>，建图等等的事情全部由 worker0 这边的一份代码完成，大概可以理解成某些代码中的 “Single Session” 的模式。</p><p>更常见的写法是下面这种，在 ps 上开 <code>Server.join()</code>，然后每个 worker 分别跑一遍完整的 Python 脚本，自己构建自己本地的计算图。</p><p><img data-src="http://jcf94.com/download/2018-03-09-tfunpacking5-dist-graph2.png" alt=""></p><p>那我们看到 Client（Python 脚本）运行时是通过本地的 Master Service 来管理整个计算过程，Master 相当于单节点环境的 Session 的角色，由它去分配任务给本地的 worker 或者远程的 worker。</p><p>Worker 服务有以下几个任务：</p><ol><li>处理 Master 交过来的请求</li><li>拿到自己的子图之后，调度其中的 op 完成具体的计算</li><li>处理与其他 task（即其他的 Worker 服务）之间的数据通信问题</li></ol><p>第 2 步的详细处理也是前面分析过的，即<a href="/2018/01/13/2018-01-13-tfunpacking/#RunAsync-amp-ScheduleReady">ExecutorState 的 RunAsync() 和 ScheduleReady()</a>部分处理的事情了。</p><p>关于 send/recv：</p><ul><li>CPU 和 GPU 之间通过 <code>cudaMemcpyAsync()</code> 来 overlap 计算和数据传输</li><li>两个本地 GPU 之间通过 DMA 直接传输</li><li>在 task 之间（不同的 Worker 服务、不同的计算节点之间）通过 gRPC 或者后来增加的 RDMA 来传输</li></ul><hr><p>Master 和 Worker 简单地想可以认为是把 DirectSession 中的 Executor 相关的结构功能给拆了出来。</p><p>接下来看看具体的代码实现。</p><h1 id="tf-train-Supervisor"><a href="#tf-train-Supervisor" class="headerlink" title="tf.train.Supervisor()"></a>tf.train.Supervisor()</h1><p>代码从单节点改造成分布式只需要替换掉几个固定的 API 即可，先从 Python 层 API 的 Supervisor 说起。</p><p>Supervisor 是一个对 Coordinator、Saver、SessionManager 等结构的封装类，用于管理运行的分布式 Session，在运行中建立检查点，并处理异常情况的恢复等等。</p><blockquote><p>1.6.0 版用这个 API 的时候会有警告说将在未来移除，建议换成 <code>tf.train.MonitoredTrainingSession</code>，但是改用这个新 API 实测性能会下降一截，可能是配置方式需要做一下改变，暂时先放下不作研究。</p></blockquote><p>Supervisor 的构造函数有一堆输入参数，挑几个比较重要的记一下：</p><ul><li>graph：运算图，<strong>不指定则使用默认图</strong>（这个跟单机版一致）</li><li>is_chief：分布式环境中可能存在多个 worker 节点，但是其中需要有一个作为 chief worker 节点。chief 需要负责初始化整个运行图，其他 worker 节点将从 chief 节点获取计算图的信息</li><li>init_op：图中用于初始化所有变量的 op</li><li>summary_op：用于收集整个运算过程中的有关信息的 op</li><li>saver：chief 将把有关的信息写到 log 中去</li><li>global_step：在分布式环境中全局共享的一个变量，标识当前跑到了第几次迭代</li><li>session_manager：用于管理执行具体运行的 session，也负责异常恢复等等，<strong>如果不指定则会创建一个</strong></li></ul><p>创建结束时，Supervisor 所关联的计算图将会被锁定，不再允许修改，因为这个图可能会被多个线程共享。</p><h1 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h1><p>分布式环境下的 C 运行时中存在 3 种 Session 结构，分别是 WorkerSession、MasterSession 以及 GrpcSession，基本上跟前面的 Architecture 是能对应起来的。下面从它们在代码中的调用顺序开始分析：</p><h2 id="WorkerSession"><a href="#WorkerSession" class="headerlink" title="WorkerSession"></a>WorkerSession</h2><p>WorkerSession 在创建 <code>tf.train.Server()</code>时就被构造出来。</p><p>C 层面的 Server 是一个用于管理当前进程中的 Master 和 Worker 服务的结构，通过<code>Start()</code>、<code>Stop()</code>、<code>Join()</code>构成了下图的状态机：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//                 Join()            Join()</span></span><br><span class="line"><span class="comment">//                  ___               ___</span></span><br><span class="line"><span class="comment">//      Start()     \ /    Stop()     \ /</span></span><br><span class="line"><span class="comment">// NEW ---------&gt; STARTED --------&gt; STOPPED</span></span><br><span class="line"><span class="comment">//   \                          /</span></span><br><span class="line"><span class="comment">//    \________________________/</span></span><br><span class="line"><span class="comment">//            Stop(), Join()</span></span><br></pre></td></tr></table></figure><p>GrpcServer 在被初始化时：</p><ul><li>检查当前可用的所有计算设备，构建 device 列表（与 DirectSession 中做的 <code>AddDevices()</code>一致）</li><li>创建了 RpcRendezvousMgr</li><li>检查传入的 cluster 信息中，其他 tasks 的端口等等的信息</li><li>注册一个 Grpc 的通信 server</li><li>创建 Master 以及 GrpcMasterService</li><li>创建 GrpcWorker 以及 GrpcWorkerService</li><li>启动 Grpc 的通信server</li><li>创建 WorkerCache</li><li>创建一个 SessionMgr，并随后在这个 SessionMgr 中创建 WorkerSession</li><li>这里没有马上创建 MasterSession，而是保存好创建 MasterSession 所需要的信息（大概是因为 ps 中不需要 Master？而 Worker 是所有节点都要有的）</li><li>创建 LocalMaster</li></ul><p>Work 类用于管理 WorkerSession、处理子图、运行子图、接收 Tensor 数据。GrpcWorker 继承了 Worker 类之后重载了其中的数据传输部分，添加的是一个额外的传输方法，用于在传输大数据时不经过 Protobuf 序列化而直接传（调用 send/recv op 的接口的话，应该是默认要序列化之后再传吧）。</p><p>GrpcWorkerService 重载的是 AsyncServiceInterface 这个类，AsyncServiceInterface 抽象的是一个异步等待服务，即创建一个新的线程，用 polling 循环来等待传入的 RPC 请求。</p><p>GrpcWorkerService 底层关联的是 WorkerService 这个通过 Protobuf 定义用于 RPC 的结构。</p><p>WorkerSession 相对而言反而是个比较简单的结构：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WorkerSession encapsulates all of the state relating to a given session.</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">WorkerSession</span> &#123;</span></span><br><span class="line">  <span class="comment">// The name of the session.</span></span><br><span class="line">  <span class="keyword">const</span> <span class="built_in">string</span> session_name;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The name of the worker. E.g., /job:mnist/replica:0/task:1.</span></span><br><span class="line">  <span class="keyword">const</span> <span class="built_in">string</span> worker_name;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Object from which WorkerInterface instances can be obtained.</span></span><br><span class="line">  <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;WorkerCacheInterface&gt; worker_cache;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Collection of local devices. These devices are typically RenamedDevices</span></span><br><span class="line">  <span class="comment">// in all except the SessionMgr.legacy_session_. legacy_session_.device_mgr</span></span><br><span class="line">  <span class="comment">// == worker_env_.device_mgr, which holds the true devices.</span></span><br><span class="line">  <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;DeviceMgr&gt; device_mgr;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// graph_mgr keeps track of the registered graphs of this session.</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// Note: graph_mgr must be deleted before rendezvous_mgr!</span></span><br><span class="line">  <span class="comment">// Note: graph_mgr must be deleted before device_mgr!</span></span><br><span class="line">  <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;GraphMgr&gt; graph_mgr;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;ClusterFunctionLibraryRuntime&gt; cluster_flr;</span><br><span class="line"></span><br><span class="line">  WorkerSession(<span class="keyword">const</span> <span class="built_in">string</span>&amp; session_name, <span class="keyword">const</span> <span class="built_in">string</span>&amp; worker_name,</span><br><span class="line">                <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;WorkerCacheInterface&gt; worker_cache,</span><br><span class="line">                <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;DeviceMgr&gt; device_mgr,</span><br><span class="line">                <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;GraphMgr&gt; graph_mgr);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>保存了名字啊、worker_cache啊、device_mgr啊、graph_mgr啊这样的内容。</p><h2 id="GrpcSession"><a href="#GrpcSession" class="headerlink" title="GrpcSession"></a>GrpcSession</h2><p>下一个断点首先是 GrpcSession 再被触发。</p><p>分布式环境下对应的 Session 结构为 Supervisor 中创建的<code>managed_session()</code>，对于 chief 节点，调用自己 SessionManager 中的 <code>_restore_checkpoint()</code> 来在 C 层面创建出 GrpcSession 结构，并且负责完成图的构建等等，之后检查本次运行是否有对应的检查点，有则把检查点的信息恢复出来。而非 chief 节点调用的是<code>wait_for_session()</code> ，创建 GrpcSession 之后等待 chief 节点完成图的构建。</p><p>GrpcSession 是从 Session 类继承出来的，其负责的任务跟单机版中的 DirectSession 很像，跟它是同一个层级的东西。</p><blockquote><p>或者说 Session 类在整个 TensorFlow 架构中更确切的应该叫它 Client Session，它们与 Python 层的 <code>sess = tf.Session()</code> 这种结构是直接对应的，是用户编程界面与 TF 运行时的入口。</p></blockquote><p>但 DirectSession 发挥功能的函数都是在本身中直接定义出来的，而这里的 GrpcSession 却可以说基本上是围绕 MasterService 的封装。通过 MasterInterface 来调用 MasterService 的功能来完成任务，可以说 GrpcSession 只是最上图中架构中 client 与 Master 服务之间的接口。</p><p>这里的 Master 接口有两种，LocalMaster 用于进程间的直接通信，GrpcMaster 用于 Grpc 通信，GrpcSession 在创建时会根据选项选择所需的 MasterInterface。通常情况下，由于 GrpcSession 都是是直接跟本地的 Master 进行交互，所以默认添加的是 LocalMaster。</p><h2 id="MasterSession"><a href="#MasterSession" class="headerlink" title="MasterSession"></a>MasterSession</h2><p>上面<code>managed_session()</code>在创建完 C 层面的 GrpcSession 返回之后，会很快执行一次 <code>sess.run()</code>，有检查点的情况是恢复检查点时的变量数据，没有检查点时是执行 init_op 来完成变量初始化。</p><p>这里执行的 <code>sess.run()</code>与单节点版本的行为相同，需要首先执行<code>_extend_graph()</code>，不同的是这里执行的是<code>tensorflow::GrpcSession::Extend()</code>，最终到<code>tensorflow::LocalMaster::CreateSession()</code>、<code>tensorflow::Master::CreateSession()</code>。</p><p>话说 TensorFlow 中跟 Master 这个概念相关的结构有一堆，一层套一层，而且功能上跟 Worker 又有很多区别的地方。类比起来，大概 MasterSession 也就是跟 Executor 比较像，每一次 Client Session 要 Run 一个子图时（<code>sess.run(...)</code>），启动一个 MasterSession。</p><p>MasterSession 追溯到最后是由 <code>GrpcSession.Extend()</code>、<code>GrpcSession.Create()</code>在构建运行图或者修改运行图的时候创建。调用栈大概是这个样子，层次看起来还是比较乱：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensorflow::GrpcSession::Create() -&gt;</span><br><span class="line">tensorflow::GrpcSession::CreateImpl(): master_-&gt;CreateSession() -&gt;</span><br><span class="line">tensorflow::LocalMaster::CreateSession(): master_impl_-&gt;CreateSession() -&gt;</span><br><span class="line">tensorflow::Master::CreateSession() -&gt; （在一个闭包中运行）</span><br><span class="line">tensorflow::MasterSession::Create()</span><br></pre></td></tr></table></figure><p>注释中对 MasterSession 的介绍是：</p><ol><li>负责分配 node 到 device</li><li>添加额外的边（例如 send/recv）</li><li>发射 commands 给 worker 来运行</li></ol><p>具体来看，还是从<code>sess.run()</code>入手：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensorflow::GrpcSession::Run() -&gt;</span><br><span class="line">tensorflow::GrpcSession::RunHelper() （开始准备 req 和 resp，用于异步请求和响应的结构）-&gt;</span><br><span class="line">tensorflow::GrpcSession::RunProto(): master_-&gt;RunStep() -&gt;</span><br><span class="line">tensorflow::LocalMaster::RunStep(): master_impl_-&gt;RunStep() -&gt;</span><br><span class="line">tensorflow::Master::RunStep() -&gt; （在一个闭包中运行）</span><br><span class="line">tensorflow::MasterSession::Run() -&gt;</span><br><span class="line">tensorflow::MasterSession::DoRunWithLocalExecution() -&gt;</span><br><span class="line">tensorflow::MasterSession::ReffedClientGraph::RunPartitions()</span><br></pre></td></tr></table></figure><p>最后的 ReffedClientGraph 是与计算图和 Worker 相关的内容了，具体的实现相当复杂，封装层次也是特别多，大致看了下<code>RunPartitions()</code>这里的注释：</p><ul><li>匹配 fed tensors 和它们在 req 中的 index</li><li>给每个 partition 准备一个将发给 worker 的 call</li><li>通过<code>tensorflow::MasterSession::ReffedClientGraph::Part::worker</code>（这是一个 WorkerInterface）的<code>RunGraphAsync()</code>方法，把运行的 call 提交给 worker 跑</li><li>等待 RunGraph 的 calls 返回结果</li><li>最后处理收到的运行结果</li></ul><hr><p>画张图稍微理一下上面这些结构的关系：</p><p><img data-src="http://jcf94.com/download/2018-03-09-tfunpacking5-master_worker.svg" alt=""></p><p>然后还有来自这里的一张图：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/26031658" target="_blank" rel="noopener">『深度长文』Tensorflow代码解析（五）</a></li></ul><p><img data-src="http://jcf94.com/download/2018-03-09-tfunpacking5-distributedsession.jpg" alt=""></p><h1 id="WorkerInterface"><a href="#WorkerInterface" class="headerlink" title="WorkerInterface"></a>WorkerInterface</h1><p>这两个类是作为 TensorFlow 运行时调用 gRPC 的接口基类。</p><p>从源码中可以看到，WorkerInterface 类定义了一堆诸如<code>GetStatusAsync()</code>、<code>CreateWorkerSessionAsync()</code>、<code>DeleteWorkerSessionAsync()</code>等等这样的虚函数接口，可以认为是跟 GrpcWorkerService 支持的 GrpcWorkerMethod 是一一对应的：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Names of worker methods.</span></span><br><span class="line"><span class="keyword">enum</span> <span class="class"><span class="keyword">class</span> <span class="title">GrpcWorkerMethod</span> &#123;</span></span><br><span class="line">  kGetStatus,</span><br><span class="line">  kCreateWorkerSession,</span><br><span class="line">  kDeleteWorkerSession,</span><br><span class="line">  kRegisterGraph,</span><br><span class="line">  kDeregisterGraph,</span><br><span class="line">  kRunGraph,</span><br><span class="line">  kCleanupGraph,</span><br><span class="line">  kCleanupAll,</span><br><span class="line">  kRecvTensor,</span><br><span class="line">  kLogging,</span><br><span class="line">  kTracing,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>当然这个同时也是要跟 Protobuf 的配置要一一对应。</p><p>具体的实现在它的两个继承类 Worker 和 GrpcRemoteWorker 里面。</p><p>从代码上来看，GrpcRemoteWorker 类中的每一个函数都是调用 <code>IssueRequest()</code> 发起一个异步的 gRPC 调用，远程的 GrpcWorkerService 作为守护进程处理传入的 gRPC 请求。</p><p>Worker 类中的对应实现则都是直接在本地做。</p><h2 id="Work-Flow"><a href="#Work-Flow" class="headerlink" title="Work Flow"></a>Work Flow</h2><p>最后回到前面的运行部分。</p><p>在<code>tensorflow::MasterSession::ReffedClientGraph::RunPartitions()</code>中，MasterSession 运行每一个已经划分好的 partitions 用的是 <code>part.worker-&gt;RunGraphAsync()</code> 调用。</p><p>part.worker 是每个 partitions 对应的 WorkerInterface 对象，很容易猜想到如果分配在远程对应的应该是 GrpcRemoteWorker 实例，否则对应的应该是 Worker 实例。</p><p>那再看数据收发部分的<code>send/recv</code>，之前已经知道了数据传输由<code>recv</code>部分发起，最终调的是<code>RpcRemoteRendezvous::RecvFromRemoteAsync()</code>：</p><p>继续往下看，检查各项参数，准备 RpcRecvTensorCall，之后启动 <code>call-&gt;Start()</code>，<code>Start()</code>里面调的是<code>StartRTCall()</code>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">StartRTCall</span><span class="params">(<span class="built_in">std</span>::function&lt;<span class="keyword">void</span>()&gt; recv_done)</span> </span>&#123;</span><br><span class="line">  resp_.InitAlloc(dst_device_, alloc_attrs_);</span><br><span class="line">  <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>::placeholders;</span><br><span class="line">  StatusCallback cb = <span class="built_in">std</span>::bind(</span><br><span class="line">      [<span class="keyword">this</span>](<span class="built_in">std</span>::function&lt;<span class="keyword">void</span>()&gt; recv_done,</span><br><span class="line">             <span class="comment">// Begin unbound arguments.</span></span><br><span class="line">             <span class="keyword">const</span> Status&amp; s) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!s.ok()) &#123;</span><br><span class="line">          mutex_lock l(mu_);</span><br><span class="line">          status_.Update(s);</span><br><span class="line">        &#125;</span><br><span class="line">        recv_done();</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="built_in">std</span>::move(recv_done), _1);</span><br><span class="line">  wi_-&gt;RecvTensorAsync(&amp;opts_, &amp;req_, &amp;resp_, <span class="built_in">std</span>::move(cb));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>wi_ 同样是一个 WorkerInterface 的结构。</p><p>这样就很清晰了，无论是 Master、Worker 相互之间的控制还是<code>send/recv</code>的数据传输都是通过 WorkerInterface 的派生类作为接口完成的，接口的另一头是底层的 gRPC 通信库。</p><p>那么再看到响应 gRPC 调用的那一边，在 GrpcWorkerService 创建时，守护进程<code>HandleRPCsLoop()</code>就启动了：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">HandleRPCsLoop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// TODO(ncteisen): This may require performance engineering. We can</span></span><br><span class="line">  <span class="comment">// change the number of threads, the number of handlers per thread,</span></span><br><span class="line">  <span class="comment">// or even decide to specialize certain threads to certain methods.</span></span><br><span class="line">  ENQUEUE_REQUEST(GetStatus, <span class="literal">false</span>);</span><br><span class="line">  ENQUEUE_REQUEST(CreateWorkerSession, <span class="literal">false</span>);</span><br><span class="line">  ENQUEUE_REQUEST(DeleteWorkerSession, <span class="literal">false</span>);</span><br><span class="line">  ENQUEUE_REQUEST(CleanupAll, <span class="literal">false</span>);</span><br><span class="line">  ENQUEUE_REQUEST(RegisterGraph, <span class="literal">false</span>);</span><br><span class="line">  ENQUEUE_REQUEST(DeregisterGraph, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// TODO(ncteisen): Determine a better policy for enqueuing the</span></span><br><span class="line">  <span class="comment">// appropriate number of each request type.</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; ++i) &#123;</span><br><span class="line">    EnqueueRecvTensorRequestRaw();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; ++i) &#123;</span><br><span class="line">    ENQUEUE_REQUEST(RunGraph, <span class="literal">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; ++i) &#123;</span><br><span class="line">    ENQUEUE_REQUEST(CleanupGraph, <span class="literal">false</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ENQUEUE_REQUEST(Logging, <span class="literal">false</span>);</span><br><span class="line">  ENQUEUE_REQUEST(Tracing, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">void</span>* tag;</span><br><span class="line">  <span class="keyword">bool</span> ok;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (cq_-&gt;Next(&amp;tag, &amp;ok)) &#123;</span><br><span class="line">    UntypedCall&lt;GrpcWorkerServiceThread&gt;::Tag* callback_tag =</span><br><span class="line">        <span class="keyword">static_cast</span>&lt;UntypedCall&lt;GrpcWorkerServiceThread&gt;::Tag*&gt;(tag);</span><br><span class="line">    CHECK(callback_tag);</span><br><span class="line">    callback_tag-&gt;OnCompleted(<span class="keyword">this</span>, ok);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先准备好一系列 gRPC 调用的等待队列，11 种调用请求与前面的 GrpcWorkerMethod 一一对应，插入完成之后就是 gRPC 部分的任务了。每个方法对应的处理过程的代码也都列在后面，随便挑一个举例：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GetStatusHandler</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    WorkerCall&lt;GetStatusRequest, GetStatusResponse&gt;* call)</span> </span>&#123;</span><br><span class="line">  Schedule([<span class="keyword">this</span>, call]() &#123;</span><br><span class="line">    Status s = worker_-&gt;GetStatus(&amp;call-&gt;request, &amp;call-&gt;response);</span><br><span class="line">    call-&gt;SendResponse(ToGrpcStatus(s));</span><br><span class="line">  &#125;);</span><br><span class="line">  ENQUEUE_REQUEST(GetStatus, <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>响应 gRPC 请求时这里把要做的任务都封装到线程池里面去执行，然后向队列中重新补充一个相同的等待调用。具体执行的是 worker_（其实是一个 GrpcWorker），完成后向调用方返回一个 gRPC 的 Response。</p><p>最后的一个 while 循环是读取 gRPC 完成队列中的内容，处理 gRPC 调用完成之后的收尾工作，<code>RequestReceived</code>、<code>ResponseSent</code>、<code>Cancelled</code>这三种状态。</p><blockquote><p>话说这种完成队列的方式跟 RDMA 的还是挺像的。</p></blockquote><h1 id="MasterInterface"><a href="#MasterInterface" class="headerlink" title="MasterInterface"></a>MasterInterface</h1><p>MasterInterface 的结构跟 WorkerInterface 基本类似，不过话说从代码上能看出来不像是一拨人做的啊（命名风格等等），很奇怪。</p><p>支持的一些调用：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span>* grpcMasterService_method_names[] = &#123;</span><br><span class="line">    <span class="string">"/tensorflow.MasterService/CreateSession"</span>,</span><br><span class="line">    <span class="string">"/tensorflow.MasterService/ExtendSession"</span>,</span><br><span class="line">    <span class="string">"/tensorflow.MasterService/PartialRunSetup"</span>,</span><br><span class="line">    <span class="string">"/tensorflow.MasterService/RunStep"</span>,</span><br><span class="line">    <span class="string">"/tensorflow.MasterService/CloseSession"</span>,</span><br><span class="line">    <span class="string">"/tensorflow.MasterService/ListDevices"</span>,</span><br><span class="line">    <span class="string">"/tensorflow.MasterService/Reset"</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>它所派生出来的两个类 GrpcRemoteMaster 和 LocalMaster 从名字上就能够看出来是分别针对远程和本地的调用接口。</p><blockquote><p>乍一看 GrpcRemoteWorker 和 GrpcRemoteMaster 实现远程调用的写法居然完全不一样，很尴尬。仔细往下分析会发现 GrpcRemoteWorker 的 IssueRequest 里面封装的 RPCState 里面的内容跟 GrpcRemoteMaster 的 Call 中的内容很类似。所以为什么不用统一的写法呢。。。</p><p>然后 LocalMaster 这个类竟然只是个壳你敢信？。。。里面真正实现本地功能的是 Master 类。</p><p>话说前面 Worker 这个类实现的是本地功能，但是 Worker 类是直接继承的 WorkerInterface，到了这里 Master 类跟 MasterInterface 类没有关系，继承 MasterInterface 的是 LocalMaster 类，但是你又发现这个 LocalMaster 类居然是 Master 类的壳。。。相当于跟 Worker 差不多的结构，但是中间多包了一层。</p></blockquote><p>再来看到 GrpcMasterService 的守护进程：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">HandleRPCsLoop</span><span class="params">()</span> override </span>&#123;</span><br><span class="line">  ENQUEUE_REQUEST(CreateSession, <span class="literal">true</span>);</span><br><span class="line">  ENQUEUE_REQUEST(ExtendSession, <span class="literal">false</span>);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; ++i) &#123;</span><br><span class="line">    ENQUEUE_REQUEST(PartialRunSetup, <span class="literal">false</span>);</span><br><span class="line">    ENQUEUE_REQUEST(RunStep, <span class="literal">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  ENQUEUE_REQUEST(CloseSession, <span class="literal">false</span>);</span><br><span class="line">  ENQUEUE_REQUEST(ListDevices, <span class="literal">false</span>);</span><br><span class="line">  ENQUEUE_REQUEST(Reset, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">void</span>* tag;</span><br><span class="line">  <span class="keyword">bool</span> ok;</span><br><span class="line">  <span class="keyword">while</span> (cq_-&gt;Next(&amp;tag, &amp;ok)) &#123;</span><br><span class="line">    UntypedCall&lt;GrpcMasterService&gt;::Tag* callback_tag =</span><br><span class="line">        <span class="keyword">static_cast</span>&lt;UntypedCall&lt;GrpcMasterService&gt;::Tag*&gt;(tag);</span><br><span class="line">    <span class="keyword">if</span> (callback_tag) &#123;</span><br><span class="line">      callback_tag-&gt;OnCompleted(<span class="keyword">this</span>, ok);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// NOTE(mrry): A null `callback_tag` indicates that this is</span></span><br><span class="line">      <span class="comment">// the shutdown alarm.</span></span><br><span class="line">      cq_-&gt;Shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基本的结构跟前面 Worker 是一致的。</p><hr><p>Worker 的远程调用实际发生在：</p><ul><li>本地 Master 处理好计算图的 partition 情况</li><li>根据 partition 是在本地还是远端，分别请求本地 Worker 或者 GrpcRemoteWorker 来执行</li><li>远程的 GrpcWorkerService 守护进程收到请求之后，调用自己本地的 Worker 进行处理，完成后将结果返回</li></ul><p>话说 GrpcRemoteMaster 我还没找到到底是在什么情况下用到的。</p><hr><p>后续：</p><ul><li><a href="/2018/03/12/2018-03-12-tfunpacking6">TensorFlow 拆包（六）：RDMA</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接上篇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/13/2018-01-13-tfunpacking/&quot;&gt;TensorFlow 拆包（一）：Session.Run()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/23/2018-01-23-tfunpacking2/&quot;&gt;TensorFlow 拆包（二）：TF 的数据流模型实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/02/28/2018-02-28-tfunpacking3/&quot;&gt;TensorFlow 拆包（三）：Graph 和 Node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/03/07/2018-03-07-tfunpacking4/&quot;&gt;TensorFlow 拆包（四）：Device&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;单节点的运行流程基本上已经有个大体印象了，接着就要来拆我所关注的重点所在——分布式运行时了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TensorFlow" scheme="https://jcf94.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 拆包（四）：Device</title>
    <link href="https://jcf94.com/2018/03/07/2018-03-07-tfunpacking4/"/>
    <id>https://jcf94.com/2018/03/07/2018-03-07-tfunpacking4/</id>
    <published>2018-03-07T08:19:48.000Z</published>
    <updated>2018-09-23T10:58:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>接上篇：</p><ul><li><a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a></li><li><a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a></li><li><a href="/2018/02/28/2018-02-28-tfunpacking3/">TensorFlow 拆包（三）：Graph 和 Node</a></li></ul><p>这篇要分析的是 TensorFlow 中跟计算设备相关的内容。</p><a id="more"></a><h1 id="DeviceFactory-AddDevices"><a href="#DeviceFactory-AddDevices" class="headerlink" title="DeviceFactory::AddDevices()"></a>DeviceFactory::AddDevices()</h1><p>TensorFlow 在创建 Session 时会首先扫描当前设备中存在哪些可用的计算资源（CPU、GPU）。</p><p>Python 层的 Session 构建时通过 <code>tf_session.TF_NewDeprecatedSession()</code> 这个接口调用 C 层的运行时来在 C 层新建 Session。</p><p>单节点下创建 DirectSession 由 <code>DirectSessionFactory::NewSession()</code>完成，其中又通过 <code>DeviceFactory::AddDevices()</code>获取当前可用的设备信息。</p><p>不同的计算设备分别注册各自的 DeviceFactory，<code>AddDevices()</code>调用不同设备的<code>CreateDevices()</code>，最终将所有的 device 信息传递回上层。</p><p>这里<code>device_factories()</code>中是一个静态的 <code>std::unordered_map&lt;string, FactoryItem&gt;</code>，比较奇怪的是我找了半天没找到这个东西是在哪赋值的，gdb 调试进去，明明第一次调用的时候应该是创建一个空的新对象，结果里面居然就有内容了 ：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">unordered_map</span> with <span class="number">2</span> elements = &#123;</span><br><span class="line">    [<span class="string">"CPU"</span>] = &#123;factory = <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;tensorflow::DeviceFactory&gt; containing <span class="number">0x15fcf00</span>, priority = <span class="number">70</span>&#125;,</span><br><span class="line">    [<span class="string">"GPU"</span>] = &#123;factory = <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;tensorflow::DeviceFactory&gt; containing <span class="number">0x15f2de0</span>, priority = <span class="number">210</span>&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>GPU 版编译的 TensorFlow 中，CPU 端对应的是<code>tensorflow::GPUCompatibleCPUDeviceFactory::CreateDevices</code>，GPU 端对应的是 <code>tensorflow::GPUDeviceFactory::CreateDevices</code>。</p><p>这两个 DeviceFactory 的定义在 <code>/tensorflow/core/common_runtime/gpu/gpu_device_factory.cc</code> 中，然后在这里面还比较惊喜地发现了两条与前面几篇中注册 Op 时很像的宏：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">REGISTER_LOCAL_DEVICE_FACTORY(<span class="string">"GPU"</span>, GPUDeviceFactory, <span class="number">210</span>);</span><br><span class="line">REGISTER_LOCAL_DEVICE_FACTORY(<span class="string">"CPU"</span>, GPUCompatibleCPUDeviceFactory, <span class="number">70</span>);</span><br></pre></td></tr></table></figure><p>这应该就是用于注册 Device 到前面的<code>std::unordered_map&lt;string, FactoryItem&gt;</code>结构中的内容了，但是关于注册这个过程是在什么时候运行的我还不是很明确，因为 gdb 在这里加了断点却没有进。</p><p>在整个 TensorFlow 的源码中，除了上面的两条以外，还可以找到：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">REGISTER_LOCAL_DEVICE_FACTORY(<span class="string">"CPU"</span>, ThreadPoolDeviceFactory, <span class="number">60</span>);</span><br><span class="line">REGISTER_LOCAL_DEVICE_FACTORY(<span class="string">"SYCL"</span>, SYCLDeviceFactory, <span class="number">200</span>);</span><br></pre></td></tr></table></figure><p>两种设备，<code>ThreadPoolDeviceFactory</code>应该是用于纯 CPU 版的 TensorFlow。事实上<code>GPUCompatibleCPUDeviceFactory</code>就是继承的<code>ThreadPoolDeviceFactory</code>，额外加了一些与 GPU 结合的选项。</p><hr><p>目前<code>ThreadPoolDeviceFactory</code>和<code>GPUDevice</code>最终都是用到的<code>LocalDevice</code>，配合 Eigen 使用。</p><p><code>AddDevices()</code>得到的 device 列表存在 DeviceMgr 中传入 DirectSession。</p><h2 id="Add-a-new-type-of-device-to-TF"><a href="#Add-a-new-type-of-device-to-TF" class="headerlink" title="Add a new type of device to TF"></a>Add a new type of device to TF</h2><p>官方在这块内容基本上没有给什么太详细的说明，不过从前面的分析也可以很容易看出来，要创建一个新类型的设备首先要继承一个新的 <code>tensorflow::Device</code>，以及其生成用的工厂模式<code>tensorflow::DeviceFactory</code>，中间涉及到一些必要的函数需要重载，然后用<code>REGISTER_LOCAL_DEVICE_FACTORY(...)</code>注册即可。</p><p><a href="https://github.com/knuedge/tensorflow/blob/36e0cdf04f294bfd51931d4f78e291590ed0d3ec/tensorflow/g3doc/hardware/adding_support/fake_device.md" target="_blank" rel="noopener">这里</a>有一个用 CPU 改个名字来虚拟新硬件的测试例子。</p><h1 id="Allocate-nodes-with-Devices"><a href="#Allocate-nodes-with-Devices" class="headerlink" title="Allocate nodes with Devices"></a>Allocate nodes with Devices</h1><h2 id="Placer-Run"><a href="#Placer-Run" class="headerlink" title="Placer::Run()"></a>Placer::Run()</h2><p><code>tensorflow::GraphExecutionState::Extend()</code>创建完整的运行图时，用了一个<code>tensorflow::Placer</code>结构来处理图中的运行节点与设备的关联问题。</p><p>这里的分配策略非常简单，用一个<strong>并查集（！！666！！）</strong>来维护所有节点的连通性，然后把连通在一起的节点分到同样的设备上。其中有手动指定的话就按手动指定的来，没有手动指定的则按 device 优先级来，默认 GPU 最高，然后是 OpenCL 的 SYCL，最后才是分配到 CPU 上。</p><p>这里有几个额外规则：</p><ol><li>source node 和 sink node 必须分到 CPU 上</li><li>对于没有输入，有一个输出的 GeneratorNode，分配到它的目标节点所在的设备上；</li><li>对于直接在原数据上进行操作（例如说 reshape）这样的 MetadataNode，分配到它的源节点所在的设备上。</li></ol><h2 id="tensorflow-Partition"><a href="#tensorflow-Partition" class="headerlink" title="tensorflow::Partition()"></a>tensorflow::Partition()</h2><p>上篇中，在<code>DirectSession::GetOrCreateExecutors()</code>的过程中，用<code>DirectSession::CreateGraphs()</code>根据当前的输入、输出等信息从完整图中创建出了一个当前运行所用的子图。</p><p>之后需要用到<code>tensorflow::Partition()</code>来完成需要运行的子图与设备的分配关联：</p><ul><li>为图中的每一个节点和边创建内存和设备类型信息</li><li>检查每一个节点的输入输出、以及其目标节点的输入信息等</li><li>给每一个节点上添加控制边等，如果数据传输不在同一个设备上就添加一对 send/recv 的 node</li></ul><h1 id="Devices-amp-Compute"><a href="#Devices-amp-Compute" class="headerlink" title="Devices &amp; Compute"></a>Devices &amp; Compute</h1><p>事实上，感觉 DirectSession 中创建的很多结构都像是有一一对应关系的，比如前面<code>Partition()</code>得到的子图、子图所关联的设备、以及子图所构建的 Executor。</p><p>在<code>tensorflow/core/common_runtime/device.h</code>中，可以找到 Device 类的计算函数的定义：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Performs the actual compute function.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Subclasses may override this function if they wish to perform</span></span><br><span class="line"><span class="comment">// some initialization before each compute.</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Compute</span><span class="params">(OpKernel* op_kernel, OpKernelContext* context)</span> </span>&#123;</span><br><span class="line">  op_kernel-&gt;Compute(context);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Asynchronous kernel's compute.</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">ComputeAsync</span><span class="params">(AsyncOpKernel* op_kernel, OpKernelContext* context,</span></span></span><br><span class="line"><span class="function"><span class="params">                          AsyncOpKernel::DoneCallback done)</span> </span>&#123;</span><br><span class="line">  op_kernel-&gt;ComputeAsync(context, <span class="built_in">std</span>::move(done));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实就是运行传入的 Op 所注册的 OpKernel 函数。</p><p>ThreadPoolDevice 类重载了这两个函数，加上一些额外需要记录的信息，核心部分还是运行 OpKernel。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>这里把前面几篇中的涉及到的内容稍微做一下总结：</p><p><img data-src="http://jcf94.com/download/2018-03-07-tfunpacking4-total.svg" alt=""></p><hr><p>后续：</p><ul><li><a href="/2018/03/09/2018-03-09-tfunpacking5/">TensorFlow 拆包（五）：Distributed</a> </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接上篇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/13/2018-01-13-tfunpacking/&quot;&gt;TensorFlow 拆包（一）：Session.Run()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/23/2018-01-23-tfunpacking2/&quot;&gt;TensorFlow 拆包（二）：TF 的数据流模型实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/02/28/2018-02-28-tfunpacking3/&quot;&gt;TensorFlow 拆包（三）：Graph 和 Node&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇要分析的是 TensorFlow 中跟计算设备相关的内容。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TensorFlow" scheme="https://jcf94.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 拆包（三）：Graph 和 Node</title>
    <link href="https://jcf94.com/2018/02/28/2018-02-28-tfunpacking3/"/>
    <id>https://jcf94.com/2018/02/28/2018-02-28-tfunpacking3/</id>
    <published>2018-02-28T12:50:26.000Z</published>
    <updated>2020-04-25T07:12:38.048Z</updated>
    
    <content type="html"><![CDATA[<p>接上篇：</p><ul><li><a href="http://jcf94.com/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a></li><li><a href="/2018/01/23/2018-01-23-tfunpacking2/">TensorFlow 拆包（二）：TF 的数据流模型实现</a></li></ul><p>先来拆一下第一篇里面 <code>DirectSession::Run</code> 里面跑的那个 graph 里面到底都是些什么内容。</p><a id="more"></a><hr><h1 id="DirectSession-GetOrCreateExecutors"><a href="#DirectSession-GetOrCreateExecutors" class="headerlink" title="DirectSession::GetOrCreateExecutors"></a>DirectSession::GetOrCreateExecutors</h1><p>前面分析到 <a href="/2018/01/13/2018-01-13-tfunpacking/#Executor">Executor</a> 的时候，中间看到 <code>DirectSession::GetOrCreateExecutors</code> 这个函数生成了一堆 Executor，其中 <code>CreateGraphs()</code> 做的就是根据输入的 op 名建图的过程。</p><p>函数调用在：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">direct_session.cc: <span class="number">1131</span></span><br><span class="line">  <span class="comment">// Nothing found, so create the executors and store in the cache.</span></span><br><span class="line">  BuildGraphOptions options;</span><br><span class="line">  options.feed_endpoints = inputs_sorted;</span><br><span class="line">  options.fetch_endpoints = outputs_sorted;</span><br><span class="line">  options.target_nodes = tn_sorted;</span><br><span class="line">  options.use_function_convention = !run_state_args-&gt;is_partial_run;</span><br><span class="line">  <span class="keyword">if</span> (!run_state_args-&gt;debug_options.debug_tensor_watch_opts().empty()) &#123;</span><br><span class="line">    options.debug_options = run_state_args-&gt;debug_options;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;ExecutorsAndKeys&gt; ek(<span class="keyword">new</span> ExecutorsAndKeys);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The executor_lock_ is intentionally released while executor is</span></span><br><span class="line">  <span class="comment">// being created.</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;Graph&gt;&gt; graphs;</span><br><span class="line">  TF_RETURN_IF_ERROR(CreateGraphs(options, &amp;graphs, &amp;ek-&gt;flib_def,</span><br><span class="line">                                  run_state_args, &amp;ek-&gt;input_types,</span><br><span class="line">                                  &amp;ek-&gt;output_types));</span><br></pre></td></tr></table></figure><p>这个调用很有意思，ek 和 graphs 这两个东西都是现场创建的，传地址进去其实用来作为函数的输出结果，所以实际的输入只有 options 和 run_state_args。</p><p>run_state_args 里面保存的是一些额外的运行信息，用于调试等等。options 的 feed_endpoints 和 fetch_endpoint 分别表示的就是当前运行中的输入点和输出点。</p><p>然后看一下 <code>CreateGraphs()</code> 的具体实现：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Status DirectSession::CreateGraphs(</span><br><span class="line">    <span class="keyword">const</span> BuildGraphOptions&amp; subgraph_options,</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;Graph&gt;&gt;* outputs,</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;FunctionLibraryDefinition&gt;* flib_def,</span><br><span class="line">    RunStateArgs* run_state_args, DataTypeVector* input_types,</span><br><span class="line">    DataTypeVector* output_types)</span><br></pre></td></tr></table></figure><ul><li><p>创建一个 <code>GraphExecutionState* execution_state</code> 用于保存当前次运行真正要用到的运行图。</p><p>DirectSession 对象中的 execution_state_ 成员保存的是环境中的完整的图信息。若当前次运行需要用精简的图，则从 execution_state_ 中提取出需要用到的一部分内容放进前面创建的 execution_state 中，如果不需要精简，则直接复制 executor_state_ 到 execution_state 中。</p><p>完成的图会输出到 client_graph 这个结构中。</p></li><li><p>检查输入输出的数量跟准备好的 client_graph 的输入输出是否对应</p></li><li><p>保存 Stateful placements（？？不知道是干嘛用的）</p></li><li><p>用<code>tensorflow::Partition()</code>把运行的图切分到当前可用的 device 上，返回的是一个 <code>std::unordered_map&lt;string, GraphDef&gt;</code>的结构，放在 partitions 这个变量中</p></li><li><p>对 partitions 中的每一组 GraphDef，用 <code>ConvertGraphDefToGraph()</code> 转化成 Graph，存入前面的 <code>std::unordered_map&lt;string, std::unique_ptr&lt;Graph&gt;&gt;</code> 结构，也就是 outputs 这个指针中</p></li><li><p>对图进行一定的优化，然后通过 outputs 指针返回到上一层去</p></li></ul><h2 id="Graph-amp-GraphDef"><a href="#Graph-amp-GraphDef" class="headerlink" title="Graph &amp; GraphDef"></a>Graph &amp; GraphDef</h2><blockquote><p>其实 Graph 本身实现的思路还是很容易接受的，但是加上 Protobuf 定义之后就变得…</p><p>贼复杂！！！</p><p>有的地方用 Graph，有的地方又是转成 GraphDef 然后重新提取信息用。</p></blockquote><p>GraphDef 是 TensorFlow 中对图的 Protobuf 定义结构，主要方便保存啊、传输啊等等，真正运行的时候要转成 Graph 这个结构用。</p><blockquote><p>我原本还奇怪为什么 TF 里面的很多东西都要用字符串来唯一标识，本来我觉得对象解析这种事情应该在比较高的层次上比如 Python 那层就做完，结果这里是到底层还要用字符串。</p><p>大概很大的原因就是为了方便 Protobuf 的序列化？</p></blockquote><p>下面这个链接中给出了 GraphDef 和 Graph 这两个结构的简单关系：</p><ul><li><a href="http://www.cnblogs.com/yao62995/p/5773070.html" target="_blank" rel="noopener">图解tensorflow源码] Graph 图模块 （UML视图）</a></li></ul><p>引用一下：</p><p><img data-src="http://jcf94.com/download/2018-02-28-tfunpacking3-graph_meta.jpg" alt=""></p><h2 id="Graph-in-C"><a href="#Graph-in-C" class="headerlink" title="Graph in C"></a>Graph in C</h2><p>有关 Graph 的定义，基本上都在 <code>tensorflow/core/graph/graph.h</code>这个头文件里面，几个类都分的比较清晰：</p><ul><li><p>Graph：表示计算图的一个大类，里面有整个图的完整结构，这里的图的定义是唯一起点和唯一终点，以及可用的计算设备表</p></li><li><p>Node：计算图中的节点，定义里面包含了当前节点的详细信息，以及输入输出的信息（输入节点、输出节点、输入边、输出边）</p><p>节点类型里面，switch、merge、enter、exit、next_iteration 这五个在上一篇里面讲了是 TF 的控制流部分，其他的也基本上是 TF 中的一些特殊用途的类型。</p><p>有关计算内容的定义似乎是要配合 Graph 中注册好的 Ops 表来完成的，这里还不是很明白这个过程具体是什么样的，猜测计算用的节点应该是属于 NC_OTHER 这种类型，具体的计算内容的定义写在 props_ 这个 NodeProperties 结构中。</p></li><li><p>Edge：计算图中的边</p></li><li><p>其他还有几个 iter，重载了运算符用来方便对 Graph 中的 Edge 和 Node 进行标识、对比什么的</p></li></ul><p>用 Graph 中定义的一些函数例如 AddNode、RemoveNode、AddEdge 等等就可以轻松地把整个表示出来了。</p><blockquote><p>这里的实现上很多地方是 Protobuf 的 Def 结构和非 Def 结构混用的，比如 AddNode 这个函数的输入参数是个 NodeDef，感觉很难受啊。</p></blockquote><p>剩下的实现倒是没什么特别的。</p><h2 id="Graph-amp-Op-in-Python"><a href="#Graph-amp-Op-in-Python" class="headerlink" title="Graph &amp; Op in Python"></a>Graph &amp; Op in Python</h2><p>Python 层的 Graph 定义在 <code>/tensorflow/python/framework/ops.py</code> 中，这个类的结构本身算是比较简单，主要就是一堆 Op 和 Tensor 的集合（<code>_nodes_by_id</code>和<code>_nodes_by_name</code> 两个<code>dict()</code> ， <code>_unfeedable_tensors</code>和<code>_unfetchable_ops</code>两个<code>set()</code>，还有几个关系标识）。往 Graph 中添加 Op 的函数<code>_add_op</code>即把 Op 或者 Tensor 加到<code>dict()</code>中。</p><p>TF 中的 Python Op 有两种定义方式，在 Python 层中直接定义的 Op 函数的核心部分是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> ops.name_scope(name, default_name, value) <span class="keyword">as</span> name:</span><br></pre></td></tr></table></figure><p>这个类封装。由它来找到 Op 的输入所在的 Graph，处理依赖关系以及把当前 Op 加入到 Graph 相应的列表中去。</p><p>……在代码里面搜<code>with ops.name_scope</code>这组关键词可以找到很多的 Op 定义。</p><p>另外一种 Op 建立方式是通过<code>load_library.load_op_library()</code>来载入编译好的 C 层的 Op 函数，然后包装成 Python 层的 Op。</p><h1 id="How-to-organize-the-Op-to-Graph"><a href="#How-to-organize-the-Op-to-Graph" class="headerlink" title="How to organize the Op to Graph"></a>How to organize the Op to Graph</h1><p>TF 官方有个创建自定义 Op 的教程：</p><ul><li><a href="https://www.tensorflow.org/extend/adding_an_op" target="_blank" rel="noopener">Adding a New Op</a></li></ul><p>先通过这个来了解一下 Op 的完整运行过程。</p><h2 id="Adding-a-New-Op"><a href="#Adding-a-New-Op" class="headerlink" title="Adding a New Op"></a>Adding a New Op</h2><p>教程中的示例是要创建一个输入一串 int32 的数组，把除了第一个数字以外的其他数字变成 0 后输出的 op。这里的创建从 C 层面开始，创建一个 zero_out.cc 文件：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/core/framework/op.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/core/framework/shape_inference.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> tensorflow;</span><br><span class="line"></span><br><span class="line">REGISTER_OP(<span class="string">"ZeroOut"</span>)</span><br><span class="line">    .Input(<span class="string">"to_zero: int32"</span>)</span><br><span class="line">    .Output(<span class="string">"zeroed: int32"</span>)</span><br><span class="line">    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) &#123;</span><br><span class="line">      c-&gt;set_output(<span class="number">0</span>, c-&gt;input(<span class="number">0</span>));</span><br><span class="line">      <span class="keyword">return</span> Status::OK();</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p><code>REGISTER_OP</code> 是一个宏，这套注册的过程是所有 op 首先要做的，打开 <code>tensorflow/core/ops/</code>目录下的每一个自带的 op 文件中也都是这些内容。</p><blockquote><p>这个宏注册的内容是给上层的 Python 层构建 Op 封装的时候用的。</p></blockquote><p><code>.SetShapeFn()</code>定义了输出的形状。</p><p>然后要写的是上面这个 Op 的 OpKernel，即 C 层实际运算的部分，从 OpKernel 继承出一个新的类，重写它的 Compute 函数，Compute 就是到时候扔到 TF 运行时里面跑的内容。从 OpKernelContext 里面可以获取到这个 OpKernel 在执行时的上下文信息：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/core/framework/op_kernel.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> tensorflow;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZeroOutOp</span> :</span> <span class="keyword">public</span> OpKernel &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">Compute</span><span class="params">(OpKernelContext* context)</span> override </span>&#123;</span><br><span class="line">    <span class="comment">// Grab the input tensor</span></span><br><span class="line">    <span class="keyword">const</span> Tensor&amp; input_tensor = context-&gt;input(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">auto</span> input = input_tensor.flat&lt;int32&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create an output tensor</span></span><br><span class="line">    Tensor* output_tensor = <span class="literal">NULL</span>;</span><br><span class="line">    OP_REQUIRES_OK(context, context-&gt;allocate_output(<span class="number">0</span>, input_tensor.shape(),</span><br><span class="line">                                                     &amp;output_tensor));</span><br><span class="line">    <span class="keyword">auto</span> output_flat = output_tensor-&gt;flat&lt;int32&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set all but the first element of the output tensor to 0.</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> N = input.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; N; i++) &#123;</span><br><span class="line">      output_flat(i) = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Preserve the first input value if possible.</span></span><br><span class="line">    <span class="keyword">if</span> (N &gt; <span class="number">0</span>) output_flat(<span class="number">0</span>) = input(<span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>之后再用一个宏，把这个注册好的 Op 和 OpKernel 关联在一起，C 部分的实现就完成了：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">REGISTER_KERNEL_BUILDER(Name(<span class="string">"ZeroOut"</span>).Device(DEVICE_CPU), ZeroOutOp);</span><br></pre></td></tr></table></figure><p>这个宏中 <code>Name()</code>里面是前面注册的 Op 名，<code>Device()</code>定义了当前这个 Kernel 函数的运算设备，最后是需要注册的 Kernel 函数名。</p><p><code>tensorflow/core/user_ops/fact.cc</code>中也是一个自定义 op 的示例。</p><p>把 C 实现编译成动态链接库之后，在 Python 中调用<code>tf.load_op_library()</code>方法，把前面注册好的 C 层面的 Op 以及它的 OpKernel 封装成一个 Python 层的 Op 对象。</p><p>之后这个 Op 就可以像 TensorFlow 中其他自带的 Op 一样使用了。</p><p>如果需要让这个 Op 支持自动求导，只需要在 Python 中注册好它的梯度函数即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@ops.RegisterGradient("ZeroOut")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_zero_out_grad</span><span class="params">(op, grad)</span>:</span></span><br><span class="line">    xxxxxxxxx</span><br></pre></td></tr></table></figure><hr><p>C 层还有另外两个名字很像的注册梯度函数的宏（……谁起的这名字！！！）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">REGISTER_OP_GRADIENT(<span class="string">"OpName"</span>, OpGradientDef);</span><br><span class="line">REGISTER_GRADIENT_OP(<span class="string">"OpName"</span>, OpGradientKernel);</span><br></pre></td></tr></table></figure><p>到这里为止，我们对 TensorFlow 中 Python 层与 C 层的 Op 结合过程有了一个大体的印象。</p><p><img data-src="http://jcf94.com/download/2018-02-28-tfunpacking3-dot_graph1.svg" alt=""></p><p>那么 C 层的 Graph 构建是什么时候发生的呢？回到前面创建 Executor 时的<code>CreateGraphs()</code>函数，可以看到此时DirectSession 对象中的 execution_state_ 成员已经保存了当前 Session 环境中的完整的图信息了，那么 execution_state_ 中的图是哪里来的？</p><h2 id="Back-to-TF-Run"><a href="#Back-to-TF-Run" class="headerlink" title="Back to TF_Run()"></a>Back to TF_Run()</h2><p>之前在<a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a>篇中已经对 TF_Run() 关于执行图的计算的部分进行了分析，现在需要把关注点放回到这里，看一下 Python 层中的 Graph 与 C 层中的 Graph 是如何联系在一起的。</p><p>以下是 Python 层的调用栈：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#8 File "dbg_mnist.py", line 62, in simple_dnn</span></span><br><span class="line">            train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#7 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 2213, in run</span></span><br><span class="line">    _run_using_default_session(self, feed_dict, self.graph, session)</span><br><span class="line"></span><br><span class="line"><span class="comment">#6 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 4790, in _run_using_default_session</span></span><br><span class="line">  session.run(operation, feed_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 895, in run</span></span><br><span class="line">      result = self._run(None, fetches, feed_dict, options_ptr,</span><br><span class="line">                         run_metadata_ptr)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1128, in _run</span></span><br><span class="line">      results = self._do_run(handle, final_targets, final_fetches,</span><br><span class="line">                             feed_dict_tensor, options, run_metadata)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run</span></span><br><span class="line">      <span class="built_in">return</span> self._do_call(_run_fn, self._session, feeds, fetches, targets,</span><br><span class="line">                           options, run_metadata)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1350, in _do_call</span></span><br><span class="line">      <span class="built_in">return</span> fn(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment">#1 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1329, in _run_fn</span></span><br><span class="line">          <span class="built_in">return</span> tf_session.TF_Run(session, options,</span><br><span class="line">                                   feed_dict, fetch_list, target_list,</span><br><span class="line">                                   status, run_metadata)</span><br><span class="line"></span><br><span class="line"><span class="comment">#0 &lt;built-in method TF_Run of module object at remote 0x7f7e938c6bd8&gt;</span></span><br></pre></td></tr></table></figure><p>从栈底开始逐步往内部看：</p><ul><li><p>#8、#7、#6 <code>Operation.run()</code>：通常我们的用法可能都是 <code>sess.run(Operation)</code> ，在设置好默认的 Session 之后，Operation 类中的 <code>run()</code> 方法就是调用默认 Session 的 <code>run()</code> 方法</p></li><li><p>#5 <code>BaseSession.run()</code>：fetches 是需要得到的输出目标，feed_dict 是喂进去的输入数据</p></li><li><p>#4 <code>BaseSession._run()</code>：检查 session，设置 feed_dict，创建一个 _FetchHandler，这个结构会根据 fetches 和 feed_dict 生成一个需要得到的 Tensor 列表和需要运行的 Op 列表（大概是遍历图？），final_fetches 中存放为了运行当前 Op 所需要得到的 Tensor，final_targets 中存放为了运行当前 Op 所需要运行的前置 Op</p></li><li><p>#3 <code>BaseSession._do_run()</code>：……贼多层 API 封装，<code>_run_fn()</code> 和<code>_prun_fn()</code> 是两种运行方式，跟参数一起传入下一层的函数</p></li><li><p>#2 <code>BaseSession._do_call()</code>：这层封装是用来处理异常的，其实要执行的是前面传进来的两个运行函数之一</p></li><li><p>#1 <code>BaseSession._run_fn()</code>： 准备进入 C 层的运行库，Python 层到这里结束。</p><p>在执行 <code>TF_Run()</code> 之前，这里还有一个<code>_extend_graph()</code> 的过程，初次执行时，C 部分的运行时会为 DirectSession 初始化一个 GraphExecutionState 结构，即前面所提的保存了环境中初始的图信息的 executor_state_ 。<strong>！！关键在这里！！</strong></p></li><li><p>#0 <code>tf_session.TF_Run()</code>：这就是<code>tensorflow/c/c_api.cc</code> 中 C 层运行时的入口函数了。</p></li></ul><p>整理一下上面的部分，Python 层的 API 要做的只是根据输入数据和输出目标找到整个图中的所有依赖项（包括 Tensor 和 Op），然后把这些内容传入 C 层。</p><p>那么最后再把前面的整个运行过程整理一遍：</p><ul><li>用 Python 层的接口构建出计算图</li><li>如果不定义新的 Graph 结构，则所有的 Op 都会放在默认图中</li><li>调用 <code>Session.run(...)</code> ，Python 层遍历计算图，整理出为了执行目标所需要提供的前置数据（Tensor）以及得到这些数据所需要执行的所有 Op 列表</li><li>首次运行 <code>_extend_graph()</code> 时，为 C 层的 DirectSession 对象初始化 GraphExecutionState 结构，这里面保存了 C 层的完整计算图定义</li><li>Python 层整理完的 feed_dice、fetch_list、target_list 通过 <code>TF_Run()</code> 接口传入 C 层</li><li>接下里是 <code>DirectSession::Run()</code> 中的内容，详细可见<a href="/2018/01/13/2018-01-13-tfunpacking/">TensorFlow 拆包（一）：Session.Run()</a>篇，为当前需要执行的部分创建 Executor、线程池等等，完成整个计算图的执行</li></ul><h1 id="Output-the-C-level-Graph"><a href="#Output-the-C-level-Graph" class="headerlink" title="Output the C level Graph!!!"></a>Output the C level Graph!!!</h1><p>在环境变量中加上<code>TF_CPP_MIN_VLOG_LEVEL</code>等于 2 以上的级别时，TensorFlow 运行时会输出比较详细的运行 log 来。其中就包含了 C 层面的建图相关的信息，于是用了几个 awk 脚本把这部分内容抓出来了：</p><blockquote><p>get_graph.sh</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"></span><br><span class="line">LOGFILE=<span class="variable">$1</span>.<span class="built_in">log</span></span><br><span class="line">ROUGH_LOGFILE=<span class="variable">$1_rough</span>.<span class="built_in">log</span></span><br><span class="line">FILTERED_LOGFILE=<span class="variable">$1_filterd</span>.<span class="built_in">log</span></span><br><span class="line">ROUGH_DOTFILE=<span class="variable">$1_rough</span>.dot</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="variable">$LOGFILE</span> ]; <span class="keyword">then</span></span><br><span class="line">    awk <span class="string">'match($0, /.*\|\|\s+(.*)/, out) &#123;print out[1]&#125;'</span> <span class="variable">$LOGFILE</span> &gt; <span class="variable">$ROUGH_LOGFILE</span></span><br><span class="line">    awk -f get_graph_filter.awk <span class="variable">$ROUGH_LOGFILE</span> &gt; <span class="variable">$FILTERED_LOGFILE</span></span><br><span class="line">    awk -f get_graph.awk <span class="variable">$FILTERED_LOGFILE</span> &gt; <span class="variable">$ROUGH_DOTFILE</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><blockquote><p>get_graph_filter.awk</p><p>第一步从 log 中抓出图部分的信息之后，用这个删掉其中的重复信息。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/awk</span></span><br><span class="line"></span><br><span class="line">BEGIN &#123;</span><br><span class="line">    RS = <span class="string">""</span>;</span><br><span class="line">    FS = <span class="string">"\n"</span>;</span><br><span class="line">    count = 0;</span><br><span class="line">    list[0] = <span class="string">""</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    list[count] = <span class="variable">$0</span>;</span><br><span class="line">    count ++;</span><br><span class="line">&#125;</span><br><span class="line">END &#123;</span><br><span class="line">    asort(list)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span> list[0];</span><br><span class="line">    <span class="keyword">for</span> (i=1;i&lt;count;i++)</span><br><span class="line">    <span class="keyword">if</span> (list[i] != list[i-1])</span><br><span class="line">        <span class="built_in">print</span> list[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>get_graph.awk</p><p>最后用这个脚本生成 GraphViz 的图。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/awk</span></span><br><span class="line"></span><br><span class="line">BEGIN &#123;</span><br><span class="line">    count = 0;</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"digraph newgraph &#123;\n"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (match(<span class="variable">$0</span>, /(\w+)\s=\s(\w+)\[(.*)\]\((.*)\)/, out))</span><br><span class="line">    &#123;</span><br><span class="line">        name = out[1];</span><br><span class="line">        newname = sprintf(<span class="string">"c%dn"</span>, count);</span><br><span class="line">        gsub(<span class="string">"n"</span>, newname, name);</span><br><span class="line">        content = out[3];</span><br><span class="line">        gsub(<span class="string">"\""</span>, <span class="string">"\\\""</span>, content)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"        %s[label=\"%s\", tooltip=\"%s\"];\n"</span>, name, out[2], content);</span><br><span class="line">        <span class="keyword">if</span> (out[4])</span><br><span class="line">        &#123;</span><br><span class="line">            inpt = out[4];</span><br><span class="line">            gsub(<span class="string">"n"</span>, newname, inpt);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"        %s -&gt; %s;\n"</span>, inpt, name);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">print</span> <span class="string">"#"</span>, <span class="variable">$0</span>;</span><br><span class="line">        <span class="keyword">if</span> (match(<span class="variable">$0</span>, /\(.*\&#123;/))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"    subgraph cluster_%d &#123;\n        label=\"c%d\";\n"</span>, count, count);</span><br><span class="line">            count ++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (match(<span class="variable">$0</span>, /\&#125;/))</span><br><span class="line">            <span class="built_in">print</span> <span class="string">"    &#125;"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">END &#123;</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"&#125;"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>稍微修正一下最终的输出图，我们就可以得到：</p><h2 id="Simple-DNN"><a href="#Simple-DNN" class="headerlink" title="Simple DNN"></a>Simple DNN</h2><p><img data-src="http://jcf94.com/download/2018-02-28-tfunpacking3-mnist_dnn.svg" alt=""></p><h2 id="Simple-DNN-Distributed"><a href="#Simple-DNN-Distributed" class="headerlink" title="Simple DNN Distributed"></a>Simple DNN Distributed</h2><p><img data-src="http://jcf94.com/download/2018-02-28-tfunpacking3-mnist_dnn_distributed.svg" alt=""></p><h2 id="Simple-CNN-Distributed"><a href="#Simple-CNN-Distributed" class="headerlink" title="Simple CNN Distributed"></a>Simple CNN Distributed</h2><p><img data-src="http://jcf94.com/download/2018-02-28-tfunpacking3-cnn_distributed.svg" alt=""></p><p>为了比较好的视觉效果，上面输出来的图中或多或少被我删掉一点不重要的内容，有的在相同变量上也还没做整合。<code>ApplyGradientDescent</code>、<code>ApplyAdam</code>、<code>Assign</code> 这些有多出来的虚线我加的也不一定对，<strong>暂时先批判地看待上面这几张图吧</strong>。</p><p>C 层面的图结构比 Tensorboard 里面的 Python 层要稍微多点东西（比如跨设备的 send/recv 等），然后有的地方信息又不太全（比如上图中最右侧的部分，对照 Tensorboard 才知道是 adam 中两个值的平方，从 C 层面这些 node 本身的信息上体现不出来），不过大致上还是一致的。</p><h2 id="Assign、Identity"><a href="#Assign、Identity" class="headerlink" title="Assign、Identity"></a>Assign、Identity</h2><p>关于图中的 Assign 和 Identity 这两个 op，可以见<a href="https://www.jianshu.com/p/bebcdfb74fb1" target="_blank" rel="noopener">这里的一些介绍</a>。</p><p>简单来说，Variable 持有一个内存中的 Tensor 实例，Assign 是对这块内存中的数据进行修改的操作。</p><p><a href="https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for" target="_blank" rel="noopener">Stack Overflow 上对 Identity 有个讨论</a>，然而我感觉高票答案的 <code>tf.control_dependencies()</code> 的例子其实引的不好，根本说明不清楚问题。</p><p>从官方文档里面只能看出来是做了一个别名引用，下面做一个简单的测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable(<span class="number">0</span>)</span><br><span class="line">a_i = tf.identity(a)</span><br><span class="line"></span><br><span class="line">b = tf.assign_add(a, <span class="number">1</span>)</span><br><span class="line">b_i = tf.identity(b)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">print(<span class="string">'a:'</span>, sess.run(a))</span><br><span class="line">print(<span class="string">'a_i:'</span>, sess.run(a_i))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'b:'</span>, sess.run(b))</span><br><span class="line">print(<span class="string">'a:'</span>, sess.run(a))</span><br><span class="line">print(<span class="string">'a_i:'</span>, sess.run(a_i))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'b_i:'</span>, sess.run(b_i))</span><br><span class="line">print(<span class="string">'a:'</span>, sess.run(a))</span><br><span class="line">print(<span class="string">'a_i:'</span>, sess.run(a_i))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'b:'</span>, sess.run(tf.assign_add(b, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'a:'</span>, sess.run(a))</span><br><span class="line">print(<span class="string">'a_i:'</span>, sess.run(a_i))</span><br></pre></td></tr></table></figure><p>得到的输出结果是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a: 0</span><br><span class="line">a_i: 0</span><br><span class="line">b: 1</span><br><span class="line">a: 1</span><br><span class="line">a_i: 1</span><br><span class="line">b_i: 2</span><br><span class="line">a: 2</span><br><span class="line">a_i: 2</span><br><span class="line">b: 4</span><br><span class="line">a: 4</span><br><span class="line">a_i: 4</span><br></pre></td></tr></table></figure><p>这里 a_i 和 b_i 分别是对 a 和 b 的 <code>tf.identity()</code> 操作。</p><p>首次输出的 a 和 a_i 都是 a 的初始值 0，a_i 在这里就是对 a 的直接引用。</p><p>接下来，输出 b 之后，再次输出 a 和 a_i，得到的结果与前面相同，都是 a 执行加一之后的 1，可见<code>tf.assign_add()</code> 是直接对 a 所代表的 Tensor 数据本身进行的操作。</p><p>然后再测试 b_i，结果与前面运行 b 一致。</p><hr><p>后续：</p><ul><li><a href="/2018/03/07/2018-03-07-tfunpacking4/">TensorFlow 拆包（四）：Device</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接上篇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://jcf94.com/2018/01/13/2018-01-13-tfunpacking/&quot;&gt;TensorFlow 拆包（一）：Session.Run()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/2018/01/23/2018-01-23-tfunpacking2/&quot;&gt;TensorFlow 拆包（二）：TF 的数据流模型实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先来拆一下第一篇里面 &lt;code&gt;DirectSession::Run&lt;/code&gt; 里面跑的那个 graph 里面到底都是些什么内容。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Project" scheme="https://jcf94.com/categories/Project/"/>
    
    
      <category term="TensorFlow" scheme="https://jcf94.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Intel 处理器架构演进</title>
    <link href="https://jcf94.com/2018/02/13/2018-02-13-intel/"/>
    <id>https://jcf94.com/2018/02/13/2018-02-13-intel/</id>
    <published>2018-02-13T13:00:36.000Z</published>
    <updated>2018-09-23T10:58:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>刚刚把《硬/软件接口》重新过完了一遍，觉得对微处理器中间的结构有点<strong>意犹未尽</strong>，真的是很有趣啊，然鹅翻开《量化分析方法》的目录看了看，又吓得我把书扔回去了……内容略多，留着慢慢看吧。</p><p>其实 Intel 历年处理器架构演变这事老早我就很好奇了，尤其在 SC17 上今年我们摸过的 Xeon Platinum 8176 那一代 CPU 性能比上代 E5-269x 暴涨了一大截，更是让人好奇这里面有些什么变化。</p><p>所以准备来理一理 Intel 处理器架构的演进史。</p><p><img data-src="http://jcf94.com/download/2018-02-13-intel-Intel-logo.svg" alt=""></p><a id="more"></a><h1 id="Intel-Micro-architecture"><a href="#Intel-Micro-architecture" class="headerlink" title="Intel Micro-architecture"></a>Intel Micro-architecture</h1><p>下面这个表是从<a href="https://www.wikiwand.com/en/Tick%E2%80%93tock_model" target="_blank" rel="noopener">维基百科</a>里面扒的：</p><div class="table-area" style="overflow:auto"><table class="wikitable" style="margin:0.5em auto; width:auto;"><tbody><tr><th scope="col" rowspan="2">Archi­tectural change</th><th scope="col" rowspan="2">Fabri­cation process</th><th scope="col" rowspan="2">Micro-architecture</th><th scope="col" rowspan="2">Code names</th><th scope="col" rowspan="2">Release date</th><th scope="col" colspan="5">Processors</th></tr><tr><th scope="col">8P/4P Server</th><th scope="col">4P/2P Server/ Workstation</th><th scope="col">Enthusiast/ Workstation</th><th scope="col">Desktop</th><th scope="col">Mobile</th></tr><tr><td>Tick (new fabrication process)</td><td rowspan="2">65 nm</td><td>P6, NetBurst</td><td>Presle, Cedar Mill, Yonah</td><td>2006-01-05</td><td></td><td></td><td>Presler</td><td>Cedar Mill</td><td>Yonah</td></tr><tr><td>Tock (new micro-architecture)</td><td rowspan="2">Core</td><td>Merom</td><td>2006-07-27</td><td>Tigerton</td><td>Woodcrest Clovertown</td><td>Kentsfield</td><td>Conroe</td><td>Merom</td></tr><tr><td>Tick</td><td rowspan="2">45 nm</td><td>Penryn</td><td>2007-11-11</td><td>Dunnington</td><td>Harpertown</td><td>Yorkfield</td><td>Wolfdale</td><td>Penryn</td></tr><tr><td>Tock</td><td rowspan="2">Nehalem</td><td>Nehalem</td><td>2008-11-17</td><td>Beckton</td><td>Gainestown</td><td>Bloomfield</td><td>Lynnfield</td><td>Clarksfield</td></tr><tr><td>Tick</td><td rowspan="2">32 nm</td><td>Westmere</td><td>2010-01-04</td><td>Westmere-EX</td><td>Westmere-EP</td><td>Gulftown</td><td>Clarkdale</td><td>Arrandale</td></tr><tr><td>Tock</td><td rowspan="2">Sandy Bridge</td><td>Sandy Bridge</td><td>2011-01-09</td><td>(Skipped)</td><td>Sandy Bridge-EP</td><td>Sandy Bridge-E</td><td>Sandy Bridge</td><td>Sandy Bridge-M</td></tr><tr><td>Tick</td><td rowspan="3">22 nm</td><td>Ivy Bridge</td><td>2012-04-29</td><td>Ivy Bridge-EX</td><td>Ivy Bridge-EP</td><td>Ivy Bridge-E</td><td>Ivy Bridge</td><td>Ivy Bridge-M</td></tr><tr><td>Tock</td><td rowspan="3">Haswell</td><td>Haswell</td><td>2013-06-02</td><td>Haswell-EX</td><td>Haswell-EP</td><td>Haswell-E</td><td>Haswell-DT</td><td>Haswell-MB (notebooks)<br>Haswell-LP (ultrabooks)</td></tr><tr><td rowspan="1">Refresh</td><td rowspan="1">Haswell Refresh, Devil's Canyon</td><td rowspan="1">2014-05-11, 2014-06-02</td><td rowspan="1"></td><td rowspan="1"></td><td rowspan="1"></td><td rowspan="1"></td><td rowspan="1"></td></tr><tr><td>Tick</td><td rowspan="5">14 nm</td><td>Broadwell</td><td>2014-09-05</td><td>Broadwell-EX</td><td>Broadwell-EP</td><td>Broadwell-E</td><td></td><td></td></tr><tr><td>Tock</td><td rowspan="5">Skylake</td><td>Skylake</td><td>2015-08-05</td><td>Skylake-SP</td><td>Skylake-SP</td><td>Skylake-X</td><td>Skylake</td><td></td></tr><tr><td rowspan="3">Optimizations (refreshes)</td><td>Kaby Lake</td><td>2017-01-03</td><td></td><td></td><td>Kabylake-X</td><td>Kabylake</td><td></td></tr><tr><td>Kaby Lake R</td><td>2017-08-21</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Coffee Lake</td><td>2017-10-05</td><td></td><td></td><td></td><td>Coffee Lake</td><td></td></tr><tr><td>Process</td><td rowspan="3">10 nm</td><td>Cannon Lake</td><td>2018</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Architecture</td><td rowspan="3">Ice Lake</td><td>Ice Lake</td><td>2018/ 2019?</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Optimization</td><td>Tiger Lake</td><td>2019?</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Process</td><td rowspan="3">7 nm</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Architecture</td><td rowspan="3"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Optimization</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Process</td><td rowspan="3">5 nm</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Architecture</td><td rowspan="2"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Optimization</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><hr><h2 id="P6"><a href="#P6" class="headerlink" title="P6"></a>P6</h2><p><a href="https://www.wikiwand.com/en/P6_(microarchitecture)" target="_blank" rel="noopener">P6</a> 是 Intel 的第六代微架构，最早用于 1995 年的 <a href="https://www.wikiwand.com/en/Pentium_Pro" target="_blank" rel="noopener">Pentium Pro</a> 处理器，后面 2000 的 <a href="https://www.wikiwand.com/en/NetBurst_(microarchitecture)" target="_blank" rel="noopener">NetBurst</a> 感觉应该也算是包含在 P6 这个大系列里面，一直到 2006 年的 <a href="https://www.wikiwand.com/en/Core_(microarchitecture)" target="_blank" rel="noopener">Core</a> 为止。</p><p>这个横跨了将近 10 年的架构系列最早是 600nm 的工艺，一直到最后达到了 65nm，算是不断摸索完善出来的，也是 Intel 走上比较规则的架构发展之路的一个起点。</p><p>P6 相对于之前的架构加入了很多新的技术：</p><ul><li><strong>预测执行（Speculation）</strong>和<strong>乱序执行</strong>！！！</li><li>14级流水线，第一代奔腾的流水线只有 5 级，P6 的 14 级在当时是最深的</li><li><strong>片内的 L2 cache</strong>！！！</li><li>物理地址扩展，达到最大36位，理论上这个位宽最大可以支持到 64G 的内存（虽然制程的地址空间还是只能用到 4G）</li><li><strong>寄存器重命名</strong>！！！</li><li>MMX 和 SSE 指令集扩展，开始 SIMD 的思路了</li></ul><p>以上这些都是现代处理器中非常重要的设计。</p><p>更重要的是从这里开始，奠定了 Intel 沿着摩尔定律发展的 Tick-Tock 架构演进道路：</p><ul><li>Tick 改进制程工艺，微架构基本不做大改，重点在把晶体管的工艺水平往上提升</li><li>Tock 改进微架构设计，保持工艺水平不变，重点在用更复杂、更高级的架构设计</li></ul><p>然后就是一代 Tick 再一代 Tock，交替演进。</p><p>P6 的末尾阶段，首次出现了双核，当时的双核还是基本上像是把两个单核用胶水粘在一起的感觉。</p><h2 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h2><p><img data-src="http://jcf94.com/download/2018-02-13-intel-Intel_Core2_arch.svg" alt="Core 架构"></p><p>最早的名字里面带 Core 这个牌子的处理器是 Core Duo，它的架构代号是 Yonah，其实还是算是个 NetBurst 的改版，只是跟后期的 NetBurst 走向了不同的发展道路，虽然名字上有 Core 但不是 Core 架构。主要的设计目标是面向移动平台，因此很多设计都是偏向低功耗，<strong>高能效</strong>的。</p><p>再后来的 Core 2 Duo 才是采用 Core 架构的新一代处理器，全线 65nm，然后微架构在 Yonah 之上做了比较大的改动。</p><p>Core 架构把 NetBurst 做深了的流水线级数又砍下来了，主频虽然降下来了（而且即使后来工艺提升到 45nm 之后也没有超过 NetBurst 的水平），但是却提高了整个流水线中的资源利用率，所以性能还是提升了；把奔腾4上曾经用过的超线程也砍掉了；对各个部分进行了强化，双核共享 L2 cache 等等。</p><p>从 Core 架构开始是真的走向多核了，就不再是以前“胶水粘的”伪双核了，这时候已经有最高 4 核的处理器设计了。</p><h2 id="Nehalem"><a href="#Nehalem" class="headerlink" title="Nehalem"></a>Nehalem</h2><p><img data-src="http://jcf94.com/download/2018-02-13-intel-Intel_Nehalem_arch.svg" alt="Nehalem 架构"></p><p>Core 从 65nm 改到 45nm 之后，基于 45nm 又推出了新一代架构叫 Nehalem，这一代的提升引入了相当多的新技术，算是个非常重要的里程碑。</p><p>Core 这个名字变成了桌面 PC 以及笔记本处理器的系列名，后面架构继续更新，然而 Core（酷睿） 这个名字就留下来了，然后系列开始细分，这个架构推出了<strong>第一代的 i7</strong>。</p><p>相对上一代的主要改进：</p><ul><li>引入了<strong>片内 4-12 MB 的 L3 cache</strong>！！！</li><li>重新加入<strong>超线程</strong>（奔腾4时代有，后来砍掉了，这一代开始重新引入）</li><li><strong>Intel Turbo Boost 1.0</strong>！！！</li><li><strong>分支预测器分级</strong>！！！</li><li>二级的 TLB</li><li>每个核上有 3 个整数 ALU, 2 个向量 ALU and 2 个 AGU</li><li><strong>采用 Intel QPI 来代替原来的前端总线</strong>！！！</li><li><strong>PCIE 和 DMI 控制器直接做到片内了</strong>，不再需要北桥</li><li><strong>IMC（集成内存控制器）</strong>，内存控制也从北桥移到了片内</li><li>第二代的 Intel 虚拟化技术</li><li>流水线加到 20 到 24 级</li><li>其他指令扩展升级等等</li></ul><p>相对上一代的性能：</p><ul><li>同等功耗下，10-25% 的单线程性能提升，<strong>20-100% 的多线程性能提升</strong>！！！</li><li>同等性能下功耗降低 30%</li><li>15-20% 的 clock-to-clock（不知道这个词应该怎么翻译） 性能提升</li></ul><p>工艺提升到 32nm 的 Westmere 后，推出了<strong>第一代的 i5 和 i3</strong>。</p><p>Xeon 系列也从 Westmere 开始推出了第一代 E 命名的 E7-x8xx 系列。</p><h2 id="Sandy-Bridge"><a href="#Sandy-Bridge" class="headerlink" title="Sandy Bridge"></a>Sandy Bridge</h2><p><img data-src="http://jcf94.com/download/2018-02-13-intel-sandybridge-uarch.png" alt=""></p><p>32nm 的下一代 Tock 是 Sandy Bridge，<strong>二代 Core i 系列</strong>以及<strong>第一代 Xeon E3、E5 系列</strong>也基于这个架构：</p><ul><li>Intel Turbo Boost 2.0</li><li>增大了 L1 和 L2 cache</li><li>共享的 L3 cache 也同时支持片上的<strong>核芯显卡</strong></li><li>IMC 强化成了 GMCH（integrated graphics and memory controller），片上显卡共用主存作为它的显存</li><li>每个核上的运算部件增强</li><li>分支预测增强</li><li><strong>微操作译码部分新增了一个 cache（uop cache）</strong></li><li>14 到 19 级<strong>指令流水线</strong>！！！（长度区别基于上面那个 uop cache 是否命中）</li><li>多个核间、核芯显卡、cache 间用了<strong>环状总线（ring bus）</strong></li><li>Intel Quick Sync Video，<strong>支持视频的硬解码</strong></li><li>其他指令扩展升级等等</li></ul><p>Ring Bus:</p><p><img data-src="http://jcf94.com/download/2018-02-13-intel-ring.jpg" alt=""></p><p>真是令人惊叹的操作啊。</p><blockquote><p>这个故事教育我们，cache 这个思路很多地方都能用到！！！<br>这个简单的想法能起到的效果可不简单~~</p></blockquote><p>相对上一代的性能：</p><ul><li>11.3% 的 clock-to-clock 性能提升</li><li>2 倍的显示性能提升（…这个不用想都知道会很多…）</li></ul><p>Tick 到 22nm 的下一代架构叫 Ivy Bridge，<strong>三代 Core i 系列</strong>和<strong>二代 Xeon E 系列</strong>：</p><ul><li>16 位浮点指令</li><li>片内硬件随机数生成器</li><li><strong>PCIE 3.0</strong></li><li>其他各个部分都做了很多提升</li></ul><h2 id="Haswell"><a href="#Haswell" class="headerlink" title="Haswell"></a>Haswell</h2><p><img data-src="http://jcf94.com/download/2018-02-13-intel-haswell_block_diagram.svg" alt=""></p><p>22nm 的 Tock 到了 Haswell，<strong>四代 Core i 系列</strong>和<strong>三代 Xeon E 系列</strong>：</p><ul><li>每个核内的部分进一步升级，更多的 ALU、各种带宽增加等等</li><li>支持 DDR4 内存</li><li>提供部分雷电接口（Thunderbolt）支持</li><li><strong>完整集成电压调节器（FIVR）</strong>，把主板上的一部分电源控制做到了片内</li><li>更高级的功耗控制系统，增加了 L6 和 L7 两级 CPU 睡眠状态</li><li>其他指令扩展升级等等</li></ul><p>相对上一代的性能：</p><ul><li>8% 的向量运算能力提升</li><li>5% 的单线程性能和 6% 的多线程性能</li></ul><blockquote><p>好像提的不是很多，Intel 开始挤牙膏了</p></blockquote><p>14nm 的 Tick 到了 Broadwell，<strong>五代 Core i 系列</strong>和<strong>四代 Xeon E 系列</strong>。各种指令集升级、支持了很多新功能特性。</p><h2 id="Skylake"><a href="#Skylake" class="headerlink" title="Skylake"></a>Skylake</h2><p>14nm 的 Tock 到了 Skylake，进入 XXlake 时代，<strong>六代 Core i 系列</strong>。</p><p>一系列指令集升级、新功能特性等等。上一代加入的 FIVR 这里又拿掉了，其他包括雷电 3.0 等等好多升级什么的。</p><blockquote><p>从比较粗粒度的架构图来看，Skylake 的架构基本上跟前面那张 Haswell 的没什么差别，大概就是寄存器什么的数字上往上涨了一些，所以图这里就不贴了。<br>（当然细节上肯定还是有挺多升级的）<br>挤牙膏啊挤牙膏，疯狂挤牙膏</p></blockquote><p>这个阶段的微架构除了升级指令、加上更多扩展功能以外，不像 Nehalem 和 Sandy Bridge 那时候能有更多革新的设计了，而且由于制程已经达到了很小的程度，再往下可能很快就要碰到工艺极限了，所以摩尔定律开始放缓，性能很难有特别大的提升了。</p><p>所以 Intel 开始从 Tick-Tock 两步升级战略转变到 Process-Architecture-Optimization 的三步升级战略，分别是提升工艺制程，升级微架构，然后再在微架构基础上进行优化。</p><p>其实这个三步战略从上面 Haswell 时代就已经开始了，Broadwell 前面还有个 refresh 的 Haswell 升级版，i3/i5/i7 4x20 系列。</p><p>Skylake 优化版的下一代是 Kaby Lake，即<strong>七代 Core i 系列</strong>。相比 Skylake 提升了主频，频率切换更快，提升了显示核心等等。</p><p>Kaby Lake 继续优化到了 Coffee Lake，<strong>八代 Core i 系列</strong>。这个系列的 i3 提到了 4 核，i5、i7 都从 6 核开始起步，然后继续提升主频，各种优化等等。</p><h3 id="What’s-new"><a href="#What’s-new" class="headerlink" title="What’s new!"></a>What’s new!</h3><p>话说 Kaby Lake 和 Coffee Lake 这个时代，Intel 又推出了新的 Core i 系列，命名为 <strong>Core i9</strong>，第一代的桌面版 Core i9 是 Skylake 架构（Skylake-X），第一代笔记本版 i9 是 Coffee Lake 架构。</p><p>那么本该在这个时候推出的<strong>第五代 Xeon</strong>，也就是 E3/E5/E7 的 xxxx v5 版呢？</p><p>Skylake 的第五代 Xeon 摆脱了原本的系列名，而是重新改成了 <strong>Bronze</strong>、<strong>Silver</strong>、<strong>Gold</strong>、<strong>Platinum</strong> 4 个系列（一股浓浓的网络游戏装备风，说不定再下次改名就可能改名叫稀有、史诗、传说什么的，→_→）。青铜和白银系列支持双路（原本的 E5-24xx、E7-28xx 系列），黄金系列支持四路（原本的 E5-46xx、E7-48xx 系列），白金系列支持八路（原本的 E7-88xx 系列）。</p><p>这里还有个重要变动，Intel 沿用了很多年的 Ring Bus 片内总线从 Skylake-X 开始改掉了！前面说 Sandy Bridge 开始，微架构设计上已经全面采用了 Ring Bus，其实最早到 Nehalem 时代的 Xeon 系列就已经开始用上 Ring Bus了，主要用于解决多核（非常非常多的核）之间的共享、扩展等等的问题。</p><p>然而随着 CPU 的发展，核越来越多，所以一个 CPU 片内还可能有多个 Ring Bus，就像下面这样。这会有什么问题呢？</p><p>以前我们考虑多路服务器里面的 CPU 亲和性的时候，只考虑过 socket 之间的 NUMA 关系，两片 CPU 核之间会有亲和性问题。。。。。。谁想过在下面这种结构的单片 CPU 里面其实也已经是个 NUMA 结构了！！！</p><p><img data-src="http://jcf94.com/download/2018-02-13-intel-v4_24coreshcc.png" alt="E5 v4 的 Ring Bus 架构"></p><p>但是当核的数量持续增长，Ring Bus 的延迟也会越来越高，终究不是个办法，Intel 在 KNL 上已经试过 2D Mesh 的总线结构了，大概是效果比较好，于是从 Skylake-X 开始，之后的系列开始全面改用 Mesh 结构。</p><p><img data-src="http://jcf94.com/download/2018-02-13-intel-mesh.jpeg" alt="Mesh 架构"></p><ul><li><a href="https://zhuanlan.zhihu.com/p/32216294" target="_blank" rel="noopener">破茧化蝶，从Ring Bus到Mesh网络，CPU片内总线的进化之路</a></li></ul><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>emmm……所以 Platinum 8176 疯狂吊打 E5-269x v4 是妥妥的事情。毕竟微架构差了一代，而且白金版原本对应的是八路的 E7 系列，再加上 Mesh 相比 Ring Bus 解决了很多问题。</p><blockquote><p>然后我就有个疑问了：话说……为啥以前我们不用 E7 呢</p></blockquote><hr><p>更新一下两个资料参考网站：</p><ul><li><p><a href="https://en.wikichip.org/wiki/WikiChip" target="_blank" rel="noopener">WikiChip</a></p></li><li><p><a href="http://www.moepc.net/" target="_blank" rel="noopener">MOEPC</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;刚刚把《硬/软件接口》重新过完了一遍，觉得对微处理器中间的结构有点&lt;strong&gt;意犹未尽&lt;/strong&gt;，真的是很有趣啊，然鹅翻开《量化分析方法》的目录看了看，又吓得我把书扔回去了……内容略多，留着慢慢看吧。&lt;/p&gt;
&lt;p&gt;其实 Intel 历年处理器架构演变这事老早我就很好奇了，尤其在 SC17 上今年我们摸过的 Xeon Platinum 8176 那一代 CPU 性能比上代 E5-269x 暴涨了一大截，更是让人好奇这里面有些什么变化。&lt;/p&gt;
&lt;p&gt;所以准备来理一理 Intel 处理器架构的演进史。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://jcf94.com/download/2018-02-13-intel-Intel-logo.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Computer Architecture" scheme="https://jcf94.com/categories/Computer-Architecture/"/>
    
    
      <category term="Intel" scheme="https://jcf94.com/tags/Intel/"/>
    
      <category term="Microarchitecture" scheme="https://jcf94.com/tags/Microarchitecture/"/>
    
  </entry>
  
</feed>
